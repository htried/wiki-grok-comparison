{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce302a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwapi\n",
    "from mwapi.errors import APIError\n",
    "\n",
    "# This script gets all English Wikipedia pages in Category:Wikipedia controversial topics\n",
    "\n",
    "def query():\n",
    "    pages = []\n",
    "    session = mwapi.Session(\n",
    "        'https://en.wikipedia.org',\n",
    "        user_agent='mwapi sync demo'\n",
    "    )\n",
    "    try:\n",
    "        # Use continuation to fetch all pages in the category\n",
    "        for portion in session.get(\n",
    "            formatversion=2,\n",
    "            action='query',\n",
    "            generator='categorymembers',\n",
    "            gcmtitle='Category:Wikipedia controversial topics',\n",
    "            gcmlimit=100,  # 100 results per request\n",
    "            continuation=True\n",
    "        ):\n",
    "            if 'query' in portion and 'pages' in portion['query']:\n",
    "                for page in portion['query']['pages']:\n",
    "                    pages.append(page['title'])\n",
    "            else:\n",
    "                print(\"MediaWiki returned empty result batch.\")\n",
    "    except APIError as error:\n",
    "        raise ValueError(\n",
    "            \"MediaWiki returned an error:\", str(error)\n",
    "        )\n",
    "    print(\"Fetched {} pages\".format(len(pages)))\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c4819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "controversial_pages = query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66074a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "controversial_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b51324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "controversial_page_set = set()\n",
    "\n",
    "for p in controversial_pages:\n",
    "    if 'File talk:' in p or 'User talk:' in p or 'Template:' in p or 'Template talk:' in p or 'Wikipedia:' in p or 'Wikipedia talk:' in p:\n",
    "        continue\n",
    "\n",
    "    p = p.replace('Talk:', '')\n",
    "    if re.search(r'Archive \\d+', p):\n",
    "        p = re.sub(r'Archive \\d+', '', p)\n",
    "\n",
    "    controversial_page_set.add(p.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2375f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(controversial_page_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "grok_idx = pkl.load(open('../results/cached_grok_idx.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b7b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "controversial_pages_in_grokipedia = controversial_page_set.intersection(grok_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b58098",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/controversial_pages_in_grokipedia.txt', 'w') as f:\n",
    "    for page in controversial_pages_in_grokipedia:\n",
    "        f.write(page + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638c758b",
   "metadata": {},
   "source": [
    "## Citation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f825fa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84b39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_df = pd.read_csv(f'../supplemental_data/perennial_sources_enwiki/enwiki_perennial_list.csv')\n",
    "reliability_df['domain'] = reliability_df['source']\n",
    "reliability_df = reliability_df[['domain', 'status']]\n",
    "similarities = pd.read_parquet('../results/embeddings_similarities_pairwise_top1_alignments.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reliability_shift_maximal_controversial(\n",
    "    reliability_df, \n",
    "    controversial_path='../results/controversial_pages_in_grokipedia.txt',\n",
    "    result_dir='../results', \n",
    "    fsuffix='_domains.json'\n",
    "):\n",
    "    \"\"\"\n",
    "    Find articles (limited to controversial articles) where\n",
    "    (WP_reliable - WP_unreliable) - (grok_unreliable - grok_reliable) is maximal.\n",
    "\n",
    "    Args:\n",
    "        reliability_df: DataFrame with 'domain' and 'status' columns\n",
    "        controversial_path: Path to file with list of controversial articles (one per line, name already normalized)\n",
    "        result_dir: Directory containing wp_domains.json and grok_domains.json\n",
    "\n",
    "    Returns:\n",
    "        List of tuples with article shift data, sorted by the maximal function\n",
    "    \"\"\"\n",
    "\n",
    "    # Load controversial articles set (assume already normalized, as written before)\n",
    "    with open(controversial_path, 'r', encoding='utf-8') as f:\n",
    "        controversial_set = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "    def get_article_reliability_counts(json_file, reliability_df):\n",
    "        \"\"\"Load JSON and count citations by reliability status per article.\"\"\"\n",
    "        # Create lookup dict for reliability status by normalized domain\n",
    "        reliability_lookup = {}\n",
    "        for _, row in reliability_df.iterrows():\n",
    "            domain = row['domain']\n",
    "            if domain:\n",
    "                reliability_lookup[domain] = row.get('status', None)\n",
    "        \n",
    "        article_stats = {}\n",
    "        \n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for article_dict in data:\n",
    "            for article_title, domains in article_dict.items():\n",
    "                if article_title.lower().replace('_', ' ') not in controversial_set:\n",
    "                    continue\n",
    "                if isinstance(domains, dict):\n",
    "                    rel_count = 0\n",
    "                    unrel_count = 0\n",
    "                    blacklist_count = 0\n",
    "                    no_consensus_count = 0\n",
    "                    deprecated_count = 0\n",
    "                    other_count = 0\n",
    "                    total_count = 0\n",
    "                    \n",
    "                    for domain, count in domains.items():\n",
    "                        total_count += count\n",
    "                        if domain and domain in reliability_lookup:\n",
    "                            status = reliability_lookup[domain]\n",
    "                            if status == 'Generally reliable':\n",
    "                                rel_count += count\n",
    "                            elif status == 'Generally unreliable':\n",
    "                                unrel_count += count\n",
    "                            elif status == 'Deprecated':\n",
    "                                deprecated_count += count\n",
    "                            elif status == 'No consensus':\n",
    "                                no_consensus_count += count\n",
    "                            elif status == 'Blacklisted':\n",
    "                                blacklist_count += count\n",
    "                            else:\n",
    "                                other_count += count\n",
    "                        else:\n",
    "                            other_count += count\n",
    "                    \n",
    "                    article_stats[article_title] = (\n",
    "                        article_title, rel_count, unrel_count, blacklist_count, no_consensus_count, deprecated_count, other_count, total_count)\n",
    "        return article_stats\n",
    "    \n",
    "    # Get per-article reliability counts, but only for controversial articles\n",
    "    wp_dict = get_article_reliability_counts(f'{result_dir}/wp{fsuffix}', reliability_df)\n",
    "    grok_dict = get_article_reliability_counts(f'{result_dir}/grok{fsuffix}', reliability_df)\n",
    "    \n",
    "    print(f\"Loaded {len(wp_dict)} controversial articles from Wikipedia\")\n",
    "    print(f\"Loaded {len(grok_dict)} controversial articles from Grokipedia\\n\")\n",
    "    \n",
    "    # All controversial articles found in either dataset\n",
    "    all_articles = set(wp_dict.keys()) | set(grok_dict.keys())\n",
    "\n",
    "    # Calculate the maximal function for each controversial article\n",
    "    article_maximals = []\n",
    "    for article in all_articles:\n",
    "        wp_data = wp_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        grok_data = grok_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        \n",
    "        _, wp_rel, wp_unrel, wp_blacklist, wp_no_consensus, wp_deprecated, wp_other, wp_total = wp_data\n",
    "        _, grok_rel, grok_unrel, grok_blacklist, grok_no_consensus, grok_deprecated, grok_other, grok_total = grok_data\n",
    "        \n",
    "        # Calculate: (WP_reliable - WP_unreliable) - (grok_unreliable - grok_reliable)\n",
    "        maximal_value = (wp_rel - wp_unrel) - (grok_unrel - grok_rel)\n",
    "        \n",
    "        article_maximals.append((\n",
    "            article,\n",
    "            wp_rel, wp_unrel, wp_blacklist, wp_no_consensus, wp_deprecated, wp_other, wp_total,\n",
    "            grok_rel, grok_unrel, grok_blacklist, grok_no_consensus, grok_deprecated, grok_other, grok_total,\n",
    "            maximal_value\n",
    "        ))\n",
    "    \n",
    "    return article_maximals\n",
    "\n",
    "columns = [\n",
    "        'title', 'wp_reliable', 'wp_unreliable', 'wp_blacklist', 'wp_no_consensus', 'wp_deprecated', 'wp_other',\n",
    "        'wp_total', 'grok_reliable', 'grok_unreliable', 'grok_blacklist', 'grok_no_consensus', 'grok_deprecated',\n",
    "        'grok_other', 'grok_total', 'maximal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4411214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_maximals = find_reliability_shift_maximal_controversial(reliability_df)\n",
    "max_reliability_shift_df = pd.DataFrame(article_maximals, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f40932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_charts(filtered_df, fsuffix='', show=True, title=None):\n",
    "    \"\"\"\n",
    "    Plots stacked bar + diagonal comparison charts using the given DataFrame:\n",
    "    1. Stacked bar and overlay: Proportion of sources in each reliability category for Wikipedia and Grokipedia, with diagonal fills illustrating change.\n",
    "    2. Bar chart: Percentage of articles containing at least one source of each type (not 'other') for Wikipedia and Grokipedia.\n",
    "\n",
    "    Parameters:\n",
    "        filtered_df (pd.DataFrame): DataFrame, typically filtered on articles of interest.\n",
    "        show (bool): If True, calls plt.show() at end.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Setup categories, labels, colors ---\n",
    "    column_order = [\n",
    "        'reliable', 'unreliable', 'blacklist', 'no_consensus', 'deprecated', 'other'\n",
    "    ]\n",
    "    display_names = {\n",
    "        'reliable': 'Generally reliable',\n",
    "        'unreliable': 'Generally unreliable',\n",
    "        'blacklist': 'Blacklisted',\n",
    "        'no_consensus': 'No consensus',\n",
    "        'deprecated': 'Deprecated',\n",
    "        'other': 'Other'\n",
    "    }\n",
    "    color_map = {\n",
    "        'reliable': 'green',\n",
    "        'unreliable': 'red',\n",
    "        'blacklist': 'black',\n",
    "        'no_consensus': 'yellow',\n",
    "        'deprecated': 'orange',\n",
    "        'other': 'grey'\n",
    "    }\n",
    "\n",
    "    # --- Aggregate counts as \"wp\" and \"grok\" \"status\" table ---\n",
    "    agg = {\n",
    "        'wp_reliable': filtered_df['wp_reliable'].sum(),\n",
    "        'wp_unreliable': filtered_df['wp_unreliable'].sum(),\n",
    "        'wp_blacklist': filtered_df['wp_blacklist'].sum(),\n",
    "        'wp_no_consensus': filtered_df['wp_no_consensus'].sum(),\n",
    "        'wp_deprecated': filtered_df['wp_deprecated'].sum(),\n",
    "        'wp_other': filtered_df['wp_other'].sum(),\n",
    "        'grok_reliable': filtered_df['grok_reliable'].sum(),\n",
    "        'grok_unreliable': filtered_df['grok_unreliable'].sum(),\n",
    "        'grok_blacklist': filtered_df['grok_blacklist'].sum(),\n",
    "        'grok_no_consensus': filtered_df['grok_no_consensus'].sum(),\n",
    "        'grok_deprecated': filtered_df['grok_deprecated'].sum(),\n",
    "        'grok_other': filtered_df['grok_other'].sum(),\n",
    "    }\n",
    "\n",
    "    # Make DF of shape: index=['Wikipedia', 'Grokipedia'], columns=column_order\n",
    "    wp_row = [agg[f'wp_{k}'] for k in column_order]\n",
    "    grok_row = [agg[f'grok_{k}'] for k in column_order]\n",
    "    prop_df = pd.DataFrame(\n",
    "        [wp_row, grok_row],\n",
    "        columns=column_order,\n",
    "        index=['Wikipedia', 'Grokipedia']\n",
    "    )\n",
    "    prop_df_norm = prop_df.div(prop_df.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    # --- Plotting stacked bars with diagonal overlays, make first plot narrower and better aligned ---\n",
    "    labels = ['Wikipedia', 'Grokipedia']\n",
    "    x = np.arange(len(labels))\n",
    "    bar_sep = 0.09  # reduce gap between bars\n",
    "    width = 0.18    # make bars narrower\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 7), gridspec_kw={'width_ratios': [1, 1.7]})\n",
    "    \n",
    "    # Set figure title (for whole figure)\n",
    "    plot_title = title if title is not None else \"Source Reliability Category Proportion: Wikipedia vs Grokipedia\"\n",
    "    fig.suptitle(plot_title, fontsize=18, y=0.98)\n",
    "    ax = axs[0]\n",
    "\n",
    "    # Set up stacking\n",
    "    bottoms = [0, 0]\n",
    "    bars_wp = []\n",
    "    bars_grok = []\n",
    "\n",
    "    # For synchronized stacking, process in column order:\n",
    "    for j, col in enumerate(column_order):\n",
    "        color = color_map.get(col, 'grey')\n",
    "        # WP bar proportions\n",
    "        wp_prop = prop_df_norm.loc['Wikipedia', col]\n",
    "        grok_prop = prop_df_norm.loc['Grokipedia', col]\n",
    "        bar_wp = ax.bar(x[0] - width/2 - bar_sep/2, wp_prop, width=width,\n",
    "                        bottom=bottoms[0], color=color, edgecolor='none', zorder=2, alpha=0.8)\n",
    "        bar_grok = ax.bar(x[1] + width/2 + bar_sep/2, grok_prop, width=width,\n",
    "                          bottom=bottoms[1], color=color, edgecolor='none', zorder=2, alpha=0.8)\n",
    "\n",
    "        # Diagonal change fill\n",
    "        wp_top = bottoms[0] + wp_prop\n",
    "        grok_top = bottoms[1] + grok_prop\n",
    "        ax.fill_between(\n",
    "            [x[0] - width/2, x[1] + width/2],\n",
    "            [wp_top, grok_top],\n",
    "            [bottoms[0], bottoms[1]],\n",
    "            color=color, alpha=0.25, zorder=1, linewidth=0\n",
    "        )\n",
    "        bars_wp.append(bar_wp)\n",
    "        bars_grok.append(bar_grok)\n",
    "        bottoms[0] += wp_prop\n",
    "        bottoms[1] += grok_prop\n",
    "\n",
    "    # Set axis ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, fontsize=16)\n",
    "    ax.set_ylabel(\"Proportion of Citations\", fontsize=16)\n",
    "    ax.set_title(\"Source Status Proportion: Wikipedia vs Grokipedia\", fontsize=16)\n",
    "\n",
    "    # Make axis tight with bars, remove excess white space\n",
    "    # Bars are at x[0]=0 and x[1]=1, with width=0.18 and bar_sep=0.09\n",
    "    # Left bar spans: -0.18 to 0, right bar spans: 1 to 1.18\n",
    "    # Add small padding: 0.05 on each side\n",
    "    ax.set_xlim(-0.23, 1.23)\n",
    "    ax.set_ylim(bottom=0, top=1.01)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    # Custom legend patch (color by status) - move inside plot to reduce whitespace\n",
    "    legend_elements = [Patch(facecolor=color_map.get(col, 'grey'), label=display_names.get(col, col), alpha=0.8) for col in column_order]\n",
    "    ax.legend(handles=legend_elements, title='Source Status', loc='upper center', framealpha=0.9)\n",
    "\n",
    "    # Tighten subplot spacing to reduce whitespace\n",
    "    fig.subplots_adjust(wspace=0.15, left=0.05, right=0.97, top=0.92, bottom=0.1)\n",
    "\n",
    "    # ---- New plot: % of articles containing at least 1 in each source type (not 'other') ----\n",
    "\n",
    "    ax2 = axs[1]\n",
    "    main_types = ['reliable', 'unreliable', 'blacklist', 'no_consensus', 'deprecated']\n",
    "    type_labels = [display_names[t] for t in main_types]\n",
    "    bar_x = np.arange(len(main_types))\n",
    "    bar_width = 0.36\n",
    "\n",
    "    n_articles = len(filtered_df)\n",
    "    percentages = {'Wikipedia': [], 'Grokipedia': []}\n",
    "    for source_type in main_types:\n",
    "        wp_col = f'wp_{source_type}'\n",
    "        grok_col = f'grok_{source_type}'\n",
    "        wp_count = (filtered_df[wp_col] > 0).sum()\n",
    "        grok_count = (filtered_df[grok_col] > 0).sum()\n",
    "        percentages['Wikipedia'].append(wp_count / n_articles * 100)\n",
    "        percentages['Grokipedia'].append(grok_count / n_articles * 100)\n",
    "\n",
    "    # Plot as side-by-side bars\n",
    "    ax2.bar(bar_x - bar_width/2, percentages['Wikipedia'], bar_width,\n",
    "           label='Wikipedia', color=\"#4977bc\", edgecolor='black', alpha=0.7)\n",
    "    ax2.bar(bar_x + bar_width/2, percentages['Grokipedia'], bar_width,\n",
    "           label='Grokipedia', color=\"#e86b54\", edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for i, (wp, gk) in enumerate(zip(percentages['Wikipedia'], percentages['Grokipedia'])):\n",
    "        ax2.text(i - bar_width/2, wp + 1, f\"{wp:.1f}%\", ha='center', va='bottom', fontsize=10, color=\"#10426b\")\n",
    "        ax2.text(i + bar_width/2, gk + 1, f\"{gk:.1f}%\", ha='center', va='bottom', fontsize=10, color=\"#7a230c\")\n",
    "\n",
    "    ax2.set_xticks(bar_x)\n",
    "    ax2.set_xticklabels(type_labels, rotation=14, fontsize=16)\n",
    "    ax2.set_ylim(0, 105)\n",
    "    ax2.set_ylabel(\"Percent of Articles\", fontsize=16)\n",
    "    ax2.set_title(\"% of Articles Citing Any Source in Category\", fontsize=16)\n",
    "    ax2.legend(loc='upper right', fontsize=16)\n",
    "    ax2.grid(axis='y', linestyle=':', alpha=0.4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'../graphics/overall_grok_wp_cite_composition_{fsuffix}.pdf')\n",
    "\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f9170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_charts(max_reliability_shift_df, fsuffix='controversial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9acb096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reliability_shift_by_buckets_controversial(\n",
    "    lin_reliability, \n",
    "    controversial_path='../results/controversial_pages_in_grokipedia.txt',\n",
    "    result_dir='../results', \n",
    "    fsuffix='_domains.json'\n",
    "):\n",
    "    \"\"\"\n",
    "    Find reliability shifts in controversial articles, grouped by reliability score buckets (0.2-sized).\n",
    "    Uses lin_reliability DataFrame with reliability_score or pc1, but filters to controversial articles only.\n",
    "\n",
    "    Args:\n",
    "        lin_reliability: DataFrame with 'domain' and 'reliability_score' (or 'pc1') columns\n",
    "        controversial_path: Path to file with list of controversial articles (already normalized, one per line)\n",
    "        result_dir: Directory containing wp_domains.json and grok_domains.json\n",
    "\n",
    "    Returns:\n",
    "        List of tuples with article shift data for controversial articles grouped by reliability score buckets\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    # Load controversial articles set (assume already normalized, as written before)\n",
    "    with open(controversial_path, 'r', encoding='utf-8') as f:\n",
    "        controversial_set = set(line.strip() for line in f if line.strip())\n",
    "\n",
    "    # Define bucket edges and labels (same as general function)\n",
    "    bucket_size = 0.2\n",
    "    buckets = [(i * bucket_size, (i + 1) * bucket_size) for i in range(5)]\n",
    "    bucket_labels = [f\"{i * bucket_size:.1f}-{(i + 1) * bucket_size:.1f}\" for i in range(5)]\n",
    "\n",
    "    def get_bucket(score):\n",
    "        if pd.isna(score):\n",
    "            return None\n",
    "        for i, (low, high) in enumerate(buckets):\n",
    "            if low <= score < high:\n",
    "                return i\n",
    "        # Handle edge case: score == 1.0\n",
    "        if score == 1.0:\n",
    "            return 4\n",
    "        return None\n",
    "\n",
    "    def get_article_reliability_bucket_counts(json_file, lin_reliability):\n",
    "        \"\"\"Load JSON and count citations by reliability score bucket per controversial article.\"\"\"\n",
    "        # Create lookup dict for reliability score by domain\n",
    "        score_col = 'reliability_score' if 'reliability_score' in lin_reliability.columns else 'pc1'\n",
    "        reliability_lookup = dict()\n",
    "        for _, row in lin_reliability.iterrows():\n",
    "            domain = row['domain']\n",
    "            if domain and pd.notna(row.get(score_col)):\n",
    "                reliability_lookup[domain] = row[score_col]\n",
    "        \n",
    "        article_stats = []\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for article_dict in data:\n",
    "            for article_title, domains in article_dict.items():\n",
    "                # Only process if article is in the controversial set\n",
    "                if article_title.lower().replace('_', ' ') not in controversial_set:\n",
    "                    continue\n",
    "                if isinstance(domains, dict):\n",
    "                    bucket_counts = {i: 0 for i in range(5)}\n",
    "                    other_count = 0\n",
    "                    total_count = 0\n",
    "                    for domain, count in domains.items():\n",
    "                        total_count += count\n",
    "                        if domain and domain in reliability_lookup:\n",
    "                            score = reliability_lookup[domain]\n",
    "                            bucket_idx = get_bucket(score)\n",
    "                            if bucket_idx is not None:\n",
    "                                bucket_counts[bucket_idx] += count\n",
    "                            else:\n",
    "                                other_count += count\n",
    "                        else:\n",
    "                            other_count += count\n",
    "                    article_stats.append((\n",
    "                        article_title,\n",
    "                        bucket_counts[0], bucket_counts[1], bucket_counts[2],\n",
    "                        bucket_counts[3], bucket_counts[4], other_count, total_count\n",
    "                    ))\n",
    "        return article_stats\n",
    "\n",
    "    # Get per-controversial-article reliability bucket counts for each source\n",
    "    wp_articles = get_article_reliability_bucket_counts(f'{result_dir}/wp{fsuffix}', lin_reliability)\n",
    "    grok_articles = get_article_reliability_bucket_counts(f'{result_dir}/grok{fsuffix}', lin_reliability)\n",
    "    print(f\"Loaded {len(wp_articles)} controversial articles from Wikipedia\")\n",
    "    print(f\"Loaded {len(grok_articles)} controversial articles from Grokipedia\\n\")\n",
    "\n",
    "    # Lookup by article\n",
    "    wp_dict = {art[0]: art for art in wp_articles}\n",
    "    grok_dict = {art[0]: art for art in grok_articles}\n",
    "    all_articles = set(wp_dict.keys()) | set(grok_dict.keys())\n",
    "\n",
    "    # Calculate reliability bucket shifts for controversial subset\n",
    "    article_bucket_shifts = []\n",
    "    for article in all_articles:\n",
    "        wp_data = wp_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        grok_data = grok_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "\n",
    "        _, wp_b0, wp_b1, wp_b2, wp_b3, wp_b4, wp_other, wp_total = wp_data\n",
    "        _, grok_b0, grok_b1, grok_b2, grok_b3, grok_b4, grok_other, grok_total = grok_data\n",
    "\n",
    "        # Calculate shift for each bucket: WP_count - Grok_count (positive=WP has more, negative=Grok has more)\n",
    "        shifts = [\n",
    "            wp_b0 - grok_b0,\n",
    "            wp_b1 - grok_b1,\n",
    "            wp_b2 - grok_b2,\n",
    "            wp_b3 - grok_b3,\n",
    "            wp_b4 - grok_b4\n",
    "        ]\n",
    "        # Weighted sum: higher buckets weighted more\n",
    "        weighted_shift = sum(shifts[i] * (i + 1) for i in range(5))\n",
    "\n",
    "        article_bucket_shifts.append((\n",
    "            article,\n",
    "            wp_b0, wp_b1, wp_b2, wp_b3, wp_b4, wp_other, wp_total,\n",
    "            grok_b0, grok_b1, grok_b2, grok_b3, grok_b4, grok_other, grok_total,\n",
    "            shifts[0], shifts[1], shifts[2], shifts[3], shifts[4],\n",
    "            weighted_shift\n",
    "        ))\n",
    "\n",
    "    return article_bucket_shifts\n",
    "\n",
    "\n",
    "bucket_columns_controversial = [\n",
    "    'title',\n",
    "    'wp_bucket_0_0.2', 'wp_bucket_0.2_0.4', 'wp_bucket_0.4_0.6', 'wp_bucket_0.6_0.8', 'wp_bucket_0.8_1.0', 'wp_other', 'wp_total',\n",
    "    'grok_bucket_0_0.2', 'grok_bucket_0.2_0.4', 'grok_bucket_0.4_0.6', 'grok_bucket_0.6_0.8', 'grok_bucket_0.8_1.0', 'grok_other', 'grok_total',\n",
    "    'shift_bucket_0_0.2', 'shift_bucket_0.2_0.4', 'shift_bucket_0.4_0.6', 'shift_bucket_0.6_0.8', 'shift_bucket_0.8_1.0',\n",
    "    'weighted_shift'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9f80db",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reliability = pd.read_csv('../supplemental_data/news_reliability/LinRating_Join.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458d62b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_maximals = find_reliability_shift_by_buckets_controversial(lin_reliability)\n",
    "max_reliability_shift_df = pd.DataFrame(article_maximals, columns=bucket_columns_controversial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3ccda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_bucket_charts(filtered_df, fsuffix='', show=True, title=None):\n",
    "    \"\"\"\n",
    "    Plots stacked bar + diagonal comparison charts using reliability score buckets:\n",
    "    1. Stacked bar and overlay: Proportion of sources in each reliability bucket for Wikipedia and Grokipedia, with diagonal fills illustrating change.\n",
    "    2. Bar chart: Percentage of articles containing at least one source in each bucket for Wikipedia and Grokipedia.\n",
    "\n",
    "    Parameters:\n",
    "        filtered_df (pd.DataFrame): DataFrame with bucket columns (from find_reliability_shift_by_buckets)\n",
    "        fsuffix (str): Suffix for output filename\n",
    "        show (bool): If True, calls plt.show() at end.\n",
    "        title (str): Optional custom title for the whole figure. If None, uses default title.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.patches import Patch\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    # --- Setup buckets, labels, colors ---\n",
    "    bucket_labels = ['0.0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0', 'other']\n",
    "    display_names = {\n",
    "        '0.0-0.2': '0.0-0.2',\n",
    "        '0.2-0.4': '0.2-0.4',\n",
    "        '0.4-0.6': '0.4-0.6',\n",
    "        '0.6-0.8': '0.6-0.8',\n",
    "        '0.8-1.0': '0.8-1.0',\n",
    "        'other': 'No score'\n",
    "    }\n",
    "    \n",
    "    # Create color map: green (for 1.0), yellow (for 0.5), red (for 0.0), gray for 'other'\n",
    "    from matplotlib.colors import to_hex, LinearSegmentedColormap\n",
    "    \n",
    "    # Create a green-yellow-red colormap, where 1.0 is green, 0.5 is yellow, 0.0 is red\n",
    "    spect_cmap = LinearSegmentedColormap.from_list(\n",
    "        \"green_yellow_red\", [(0.0, \"#D73027\"), (0.5, \"#FEE08B\"), (1.0, \"#1A9850\")]  # red, yellow, green\n",
    "    )\n",
    "    \n",
    "    # Map bucket labels to their midpoint values for colormap\n",
    "    bucket_midpoints = {\n",
    "        '0.0-0.2': 0.1,  # Red end\n",
    "        '0.2-0.4': 0.3,  # Red-yellow transition\n",
    "        '0.4-0.6': 0.5,  # Yellow (middle)\n",
    "        '0.6-0.8': 0.7,  # Yellow-green transition\n",
    "        '0.8-1.0': 0.9,  # Green end\n",
    "    }\n",
    "    \n",
    "    # Generate colors for each bucket using the colormap\n",
    "    color_map = {}\n",
    "    for bucket_label in bucket_labels:\n",
    "        if bucket_label == 'other':\n",
    "            color_map[bucket_label] = 'grey'\n",
    "        else:\n",
    "            midpoint = bucket_midpoints[bucket_label]\n",
    "            color_map[bucket_label] = to_hex(spect_cmap(midpoint))\n",
    "\n",
    "    # --- Aggregate counts as \"wp\" and \"grok\" bucket table ---\n",
    "    # Map bucket labels to actual column name suffixes\n",
    "    bucket_to_col = {\n",
    "        '0.0-0.2': '0_0.2',\n",
    "        '0.2-0.4': '0.2_0.4',\n",
    "        '0.4-0.6': '0.4_0.6',\n",
    "        '0.6-0.8': '0.6_0.8',\n",
    "        '0.8-1.0': '0.8_1.0'\n",
    "    }\n",
    "    \n",
    "    agg = {}\n",
    "    for bucket_label in bucket_labels:\n",
    "        if bucket_label == 'other':\n",
    "            agg[f'wp_{bucket_label}'] = filtered_df['wp_other'].sum()\n",
    "            agg[f'grok_{bucket_label}'] = filtered_df['grok_other'].sum()\n",
    "        else:\n",
    "            # Use the correct column name format\n",
    "            col_suffix = bucket_to_col[bucket_label]\n",
    "            agg[f'wp_{bucket_label}'] = filtered_df[f'wp_bucket_{col_suffix}'].sum()\n",
    "            agg[f'grok_{bucket_label}'] = filtered_df[f'grok_bucket_{col_suffix}'].sum()\n",
    "\n",
    "    # Make DF of shape: index=['Wikipedia', 'Grokipedia'], columns=bucket_labels\n",
    "    wp_row = [agg[f'wp_{k}'] for k in bucket_labels]\n",
    "    grok_row = [agg[f'grok_{k}'] for k in bucket_labels]\n",
    "    prop_df = pd.DataFrame(\n",
    "        [wp_row, grok_row],\n",
    "        columns=bucket_labels,\n",
    "        index=['Wikipedia', 'Grokipedia']\n",
    "    )\n",
    "    prop_df_norm = prop_df.div(prop_df.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    # --- Plotting stacked bars with diagonal overlays ---\n",
    "    labels = ['Wikipedia', 'Grokipedia']\n",
    "    x = np.arange(len(labels))\n",
    "    bar_sep = 0.09\n",
    "    width = 0.18\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 7), gridspec_kw={'width_ratios': [1, 1.7]})\n",
    "    ax = axs[0]\n",
    "\n",
    "    # Set up stacking\n",
    "    bottoms = [0, 0]\n",
    "    bars_wp = []\n",
    "    bars_grok = []\n",
    "\n",
    "    # For synchronized stacking, process in bucket order:\n",
    "    for j, bucket_label in enumerate(bucket_labels):\n",
    "        color = color_map.get(bucket_label, 'grey')\n",
    "        # WP bar proportions\n",
    "        wp_prop = prop_df_norm.loc['Wikipedia', bucket_label]\n",
    "        grok_prop = prop_df_norm.loc['Grokipedia', bucket_label]\n",
    "        bar_wp = ax.bar(x[0] - width/2 - bar_sep/2, wp_prop, width=width,\n",
    "                        bottom=bottoms[0], color=color, edgecolor='none', zorder=2)\n",
    "        bar_grok = ax.bar(x[1] + width/2 + bar_sep/2, grok_prop, width=width,\n",
    "                          bottom=bottoms[1], color=color, edgecolor='none', zorder=2)\n",
    "\n",
    "        # Diagonal change fill\n",
    "        wp_top = bottoms[0] + wp_prop\n",
    "        grok_top = bottoms[1] + grok_prop\n",
    "        ax.fill_between(\n",
    "            [x[0] - width/2, x[1] + width/2],\n",
    "            [wp_top, grok_top],\n",
    "            [bottoms[0], bottoms[1]],\n",
    "            color=color, alpha=0.25, zorder=1, linewidth=0\n",
    "        )\n",
    "        bars_wp.append(bar_wp)\n",
    "        bars_grok.append(bar_grok)\n",
    "        bottoms[0] += wp_prop\n",
    "        bottoms[1] += grok_prop\n",
    "\n",
    "    # Set axis ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, fontsize=16)\n",
    "    ax.set_ylabel(\"Proportion of Citations\", fontsize=16)\n",
    "    ax.set_title(\"Source Reliability Score Proportion: Wikipedia vs Grokipedia\", fontsize=16)\n",
    "\n",
    "    # Make axis tight with bars\n",
    "    ax.set_xlim(-0.23, 1.23)\n",
    "    ax.set_ylim(bottom=0, top=1.01)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    # Custom legend patch (color by bucket) - reversed order - move inside plot to reduce whitespace\n",
    "    legend_elements = [Patch(facecolor=color_map.get(bucket_label, 'grey'), label=display_names.get(bucket_label, bucket_label), alpha=0.55) for bucket_label in reversed(bucket_labels)]\n",
    "    ax.legend(handles=legend_elements, title='Reliability Score', loc='upper center', framealpha=0.9)\n",
    "\n",
    "    # Set figure title (for whole figure)\n",
    "    plot_title = title if title is not None else \"Source Reliability Score Proportion: Wikipedia vs Grokipedia\"\n",
    "    fig.suptitle(plot_title, fontsize=18, y=0.98)\n",
    "    \n",
    "    # Tighten subplot spacing to reduce whitespace (leave room for suptitle)\n",
    "    fig.subplots_adjust(wspace=0.15, left=0.05, right=0.97, top=0.88, bottom=0.1)\n",
    "\n",
    "    # ---- New plot: % of articles containing at least 1 in each bucket ----\n",
    "    ax2 = axs[1]\n",
    "    main_buckets = ['0.0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
    "    bucket_display_labels = [display_names[b] for b in main_buckets]\n",
    "    bar_x = np.arange(len(main_buckets))\n",
    "    bar_width = 0.36\n",
    "\n",
    "    n_articles = len(filtered_df)\n",
    "    percentages = {'Wikipedia': [], 'Grokipedia': []}\n",
    "    for bucket_label in main_buckets:\n",
    "        # Use the correct column name format\n",
    "        col_suffix = bucket_to_col[bucket_label]\n",
    "        wp_col = f'wp_bucket_{col_suffix}'\n",
    "        grok_col = f'grok_bucket_{col_suffix}'\n",
    "        wp_count = (filtered_df[wp_col] > 0).sum()\n",
    "        grok_count = (filtered_df[grok_col] > 0).sum()\n",
    "        percentages['Wikipedia'].append(wp_count / n_articles * 100)\n",
    "        percentages['Grokipedia'].append(grok_count / n_articles * 100)\n",
    "\n",
    "    # Plot as side-by-side bars with blue/red scheme (same as reliability chart)\n",
    "    wp_color = \"#4977bc\"  # Blue for Wikipedia\n",
    "    grok_color = \"#e86b54\"  # Red for Grokipedia\n",
    "    ax2.bar(bar_x - bar_width/2, percentages['Wikipedia'], bar_width,\n",
    "           label='Wikipedia', color=wp_color, edgecolor='black', alpha=0.7)\n",
    "    ax2.bar(bar_x + bar_width/2, percentages['Grokipedia'], bar_width,\n",
    "           label='Grokipedia', color=grok_color, edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for i, (wp, gk) in enumerate(zip(percentages['Wikipedia'], percentages['Grokipedia'])):\n",
    "        ax2.text(i - bar_width/2, wp + 1, f\"{wp:.1f}%\", ha='center', va='bottom', fontsize=10, color=\"#10426b\")\n",
    "        ax2.text(i + bar_width/2, gk + 1, f\"{gk:.1f}%\", ha='center', va='bottom', fontsize=10, color=\"#7a230c\")\n",
    "\n",
    "    ax2.set_xticks(bar_x)\n",
    "    ax2.set_xticklabels(bucket_display_labels, rotation=14, fontsize=16)\n",
    "    ax2.set_ylim(0, 105)\n",
    "    ax2.set_ylabel(\"Percent of Articles\", fontsize=16)\n",
    "    ax2.set_title(\"% of Articles Citing Any Source in Reliability Bucket\", fontsize=16)\n",
    "    ax2.legend(loc='upper left', fontsize=16)\n",
    "    ax2.grid(axis='y', linestyle=':', alpha=0.4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'../graphics/overall_grok_wp_cite_composition_lin_{fsuffix}.pdf')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f399a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_bucket_charts(max_reliability_shift_df, fsuffix='controversial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6193d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare embedding similarities between Wikipedia and Grokipedia for controversial vs. non-controversial articles\n",
    "\n",
    "# Read controversial article titles from file\n",
    "controversial_titles = set()\n",
    "with open(\"../results/controversial_pages_in_grokipedia.txt\") as f:\n",
    "    for line in f:\n",
    "        title = line.strip()\n",
    "        if title:\n",
    "            controversial_titles.add(title.lower())\n",
    "\n",
    "def normalize_title(s):\n",
    "    return s.lower().replace(\"_\", \" \").strip()\n",
    "\n",
    "# Normalize the article titles\n",
    "similarities['normalized_article'] = similarities['title'].apply(normalize_title)\n",
    "\n",
    "# Split into controversial/non-controversial sets\n",
    "cont_sim_df = similarities[similarities['normalized_article'].isin(controversial_titles)].copy()\n",
    "noncont_sim_df = similarities[~similarities['normalized_article'].isin(controversial_titles)].copy()\n",
    "\n",
    "print(f\"Number of controversial articles in similarity df: {len(cont_sim_df)}\")\n",
    "print(f\"Number of non-controversial articles in similarity df: {len(noncont_sim_df)}\")\n",
    "\n",
    "cont_mean = cont_sim_df[\"similarity\"].mean()\n",
    "noncont_mean = noncont_sim_df[\"similarity\"].mean()\n",
    "\n",
    "# Plot histogram overlays, y axis as percentage (not density)\n",
    "plt.figure(figsize=(10, 6))\n",
    "bins = 100\n",
    "\n",
    "# Get bin counts (not density), then scale to percent\n",
    "cont_counts, bin_edges = np.histogram(cont_sim_df[\"similarity\"], bins=bins, range=(0,1))\n",
    "noncont_counts, _ = np.histogram(noncont_sim_df[\"similarity\"], bins=bin_edges)\n",
    "\n",
    "cont_percents = cont_counts / cont_sim_df.shape[0] * 100 if cont_sim_df.shape[0] > 0 else np.zeros_like(cont_counts)\n",
    "noncont_percents = noncont_counts / noncont_sim_df.shape[0] * 100 if noncont_sim_df.shape[0] > 0 else np.zeros_like(noncont_counts)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "plt.bar(bin_centers, cont_percents, width=(bin_edges[1] - bin_edges[0]), \n",
    "        color=\"tab:orange\", alpha=0.75, label=\"Controversial Articles\", edgecolor=None)\n",
    "plt.bar(bin_centers, noncont_percents, width=(bin_edges[1] - bin_edges[0]), \n",
    "        color=\"tab:blue\", alpha=0.55, label=\"Non-controversial Articles\", edgecolor=None)\n",
    "\n",
    "plt.axvline(x=cont_mean, color=\"black\", linestyle=\":\", linewidth=2, label=f\"Controversial Mean={cont_mean:.2f}\")\n",
    "plt.axvline(x=noncont_mean, color=\"black\", linestyle=\"--\", linewidth=2, label=f\"Non-controversial Mean={noncont_mean:.2f}\")\n",
    "\n",
    "plt.xlabel(\"Similarity\", fontsize=16)\n",
    "plt.ylabel(\"Percent of Articles\", fontsize=16)\n",
    "plt.title(\"Embedding Similarity for Controversial vs. Non-controversial Articles\", fontsize=18)\n",
    "plt.legend(fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../graphics/similarity_distribution_controversial_vs_noncontroversial_percent.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "controversial_pages = set()\n",
    "\n",
    "with open('../results/controversial_pages_in_grokipedia.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        title = line.strip()\n",
    "        if title:\n",
    "            controversial_pages.add(title.lower())\n",
    "\n",
    "licensed_pages = set()\n",
    "\n",
    "with open('../results/grokipedia_w_license.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        title = line.strip()\n",
    "        if title:\n",
    "            licensed_pages.add(title.lower())\n",
    "\n",
    "non_licensed_pages = set()\n",
    "\n",
    "with open('../results/grokipedia_wo_license.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        title = line.strip()\n",
    "        if title:\n",
    "            non_licensed_pages.add(title.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd38d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(controversial_pages.intersection(licensed_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b854ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(controversial_pages.intersection(non_licensed_pages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7034ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(controversial_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb55783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
