{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20b211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "from urllib.parse import quote\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaceda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_batch_gaps(urls, scraped_data_dir=\"../scraped_data\", start_index=None, end_index=None):\n",
    "    \"\"\"\n",
    "    Analyze scraped batches to find (a) skipped pages and (b) failed/errored pages\n",
    "    \n",
    "    Args:\n",
    "        urls: List of urls that should have been scraped (in order)\n",
    "        scraped_data_dir: Directory containing scraped batch files\n",
    "        start_index: Start index in discovered_titles (None = start from beginning)\n",
    "        end_index: End index in discovered_titles (None = end at last)\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (skipped_pages, failed_pages)\n",
    "        - skipped_pages: List of titles that were never scraped\n",
    "        - failed_pages: List of titles that failed with errors\n",
    "    \"\"\"\n",
    "    # Get the range of titles to analyze\n",
    "    if start_index is None:\n",
    "        start_index = 0\n",
    "    if end_index is None:\n",
    "        end_index = len(urls)\n",
    "    \n",
    "    urls_to_analyze = urls[start_index:end_index]\n",
    "    \n",
    "    # Load all scraped data\n",
    "    scraped_titles = set()\n",
    "    failed_titles = []\n",
    "    \n",
    "    batch_files = sorted(Path(scraped_data_dir).glob('batch_*.jsonl'))\n",
    "    \n",
    "    print(f\"Analyzing batches from index {start_index} to {end_index}\")\n",
    "    print(f\"Total urls to analyze: {len(urls_to_analyze)}\")\n",
    "    \n",
    "    for batch_file in batch_files:\n",
    "        # Extract batch range from filename\n",
    "        match = re.search(r'batch_(\\d+)_(\\d+)\\.jsonl', str(batch_file))\n",
    "        if not match:\n",
    "            continue\n",
    "        \n",
    "        batch_start_idx = int(match.group(1))\n",
    "        batch_end_idx = int(match.group(2))\n",
    "        \n",
    "        # Check if this batch is in our range of interest\n",
    "        if batch_end_idx < start_index or batch_start_idx > end_index:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing {batch_file.name} (range: {batch_start_idx}-{batch_end_idx})\")\n",
    "        \n",
    "        with open(batch_file, 'r') as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                scraped_titles.add(data['title'])\n",
    "    \n",
    "    # Load failed pages\n",
    "    failed_file = Path('../scraping_failed.jsonl')\n",
    "    if failed_file.exists():\n",
    "        with open(failed_file, 'r') as f:\n",
    "            for line in f:\n",
    "                failed_data = json.loads(line)\n",
    "                failed_titles.append({\n",
    "                    'title': failed_data.get('title', ''),\n",
    "                    'url': failed_data.get('url', ''),\n",
    "                    'error': failed_data.get('error', 'unknown'),\n",
    "                    'failed_at': failed_data.get('failed_at', '')\n",
    "                })\n",
    "    \n",
    "    # Find skipped pages (expected but not scraped)\n",
    "    skipped_pages = []\n",
    "    for url in urls_to_analyze:\n",
    "        title = url.split('/page/')[-1]\n",
    "        if title not in scraped_titles:\n",
    "            skipped_pages.append(title)\n",
    "    \n",
    "    print(f\"\\nAnalysis Results:\")\n",
    "    print(f\"  Scraped successfully: {len(scraped_titles)}\")\n",
    "    print(f\"  Skipped (not scraped): {len(skipped_pages)}\")\n",
    "    print(f\"  Failed with errors: {len(failed_titles)}\")\n",
    "    \n",
    "    return skipped_pages, failed_titles\n",
    "\n",
    "\n",
    "def save_retry_lists(skipped_pages, failed_pages, output_dir=\"../retry\"):\n",
    "    \"\"\"\n",
    "    Save skipped and failed pages to files for retry\n",
    "    \n",
    "    Args:\n",
    "        skipped_pages: List of titles that were skipped\n",
    "        failed_pages: List of dicts with failed page info\n",
    "        output_dir: Directory to save retry lists\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save skipped pages\n",
    "    if skipped_pages:\n",
    "        skipped_file = Path(output_dir) / 'skipped_pages.jsonl'\n",
    "        with open(skipped_file, 'w') as f:\n",
    "            for title in skipped_pages:\n",
    "                f.write(json.dumps({'title': title, 'url': f\"https://grokipedia.com/page/{quote(title)}\"}) + '\\n')\n",
    "        print(f\"Saved {len(skipped_pages)} skipped pages to {skipped_file}\")\n",
    "    \n",
    "    # Save failed pages\n",
    "    if failed_pages:\n",
    "        failed_file = Path(output_dir) / 'failed_pages.jsonl'\n",
    "        with open(failed_file, 'w') as f:\n",
    "            for item in failed_pages:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        print(f\"Saved {len(failed_pages)} failed pages to {failed_file}\")\n",
    "    \n",
    "    # Save combined retry list\n",
    "    all_retry = []\n",
    "    \n",
    "    # Add skipped pages\n",
    "    for title in skipped_pages:\n",
    "        all_retry.append({\n",
    "            'title': title,\n",
    "            'url': f\"https://grokipedia.com/page/{quote(title)}\",\n",
    "            'reason': 'skipped'\n",
    "        })\n",
    "    \n",
    "    # Add failed pages\n",
    "    for item in failed_pages:\n",
    "        all_retry.append({\n",
    "            'title': item.get('title', ''),\n",
    "            'url': item.get('url', ''),\n",
    "            'reason': item.get('error', 'unknown'),\n",
    "            'previous_error': item.get('error', 'unknown')\n",
    "        })\n",
    "    \n",
    "    if all_retry:\n",
    "        retry_file = Path(output_dir) / 'retry_list.jsonl'\n",
    "        with open(retry_file, 'w') as f:\n",
    "            for item in all_retry:\n",
    "                f.write(json.dumps(item) + '\\n')\n",
    "        print(f\"Saved {len(all_retry)} total pages to retry list: {retry_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a405f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "# Load discovered titles (replace with your actual method)\n",
    "df_urls = pd.read_json(\"hf://datasets/stefan-it/grokipedia-urls/urls.jsonl\", lines=True)\n",
    "urls = df_urls['url'].tolist()\n",
    "\n",
    "# Analyze batch gap for indices 166500 to 167352\n",
    "skipped, failed = analyze_batch_gaps(\n",
    "    urls, \n",
    "    start_index=0, \n",
    "    end_index=176500\n",
    ")\n",
    "\n",
    "# Save results for retry\n",
    "save_retry_lists(skipped, failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfca1d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
