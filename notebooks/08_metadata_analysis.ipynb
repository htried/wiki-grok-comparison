{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c95ac3e0",
   "metadata": {},
   "source": [
    "## pageview data\n",
    "\n",
    "from Wikipedia dumps + pageviews.wmcloud.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690ce6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bz2\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from typing import Dict, Optional\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import pickle as pkl\n",
    "import geopandas as gpd\n",
    "import pycountry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb23170",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_idx = pkl.load(open('../results/cached_grok_idx.pkl', \"rb\"))\n",
    "# sep_2025_enwiki_view = 6_916_125_844 # from public API\n",
    "oct_2025_enwiki_view = 7_153_203_240 # from public API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baab79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pageview_file(filepath, grok_idx):\n",
    "    total_views = 0\n",
    "    i = 0\n",
    "    page_ids = set()\n",
    "    \n",
    "    # Debug: Check grok_idx structure\n",
    "    print(f\"grok_idx type: {type(grok_idx)}\")\n",
    "    if isinstance(grok_idx, dict):\n",
    "        grok_keys = set(grok_idx.keys())\n",
    "        print(f\"grok_idx is dict with {len(grok_keys)} keys\")\n",
    "        sample_keys = list(grok_keys)[:5]\n",
    "        print(f\"Sample grok_idx keys: {sample_keys}\")\n",
    "    else:\n",
    "        grok_keys = grok_idx if isinstance(grok_idx, set) else set(grok_idx)\n",
    "        print(f\"grok_idx is {type(grok_idx)} with {len(grok_keys)} items\")\n",
    "        sample_keys = list(grok_keys)[:5]\n",
    "        print(f\"Sample grok_idx items: {sample_keys}\")\n",
    "    \n",
    "    sample_pageview_titles = []\n",
    "    sample_normalized = []\n",
    "    matches_found = []\n",
    "    lines_checked = 0\n",
    "    en_wiki_count = 0\n",
    "    other_projects = {}\n",
    "    sample_raw_lines = []\n",
    "    \n",
    "    with bz2.open(filepath, mode='rt', encoding='utf-8', errors='replace') as f:\n",
    "        for j, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Collect first few raw lines for debugging\n",
    "            if len(sample_raw_lines) < 5:\n",
    "                sample_raw_lines.append(line[:100])  # First 100 chars\n",
    "            \n",
    "            try:\n",
    "                # Format can be either:\n",
    "                # 1. project_code page_title page_id daily_total hourly_counts...\n",
    "                # 2. project_code page_title page_id access_type daily_total hourly_counts...\n",
    "                parts = line.split(' ')\n",
    "                if len(parts) < 4:\n",
    "                    continue\n",
    "                \n",
    "                lines_checked += 1\n",
    "                project = parts[0]\n",
    "                \n",
    "                # Track project types\n",
    "                if project != 'en.wikipedia':\n",
    "                    if project not in other_projects:\n",
    "                        other_projects[project] = 0\n",
    "                    other_projects[project] += 1\n",
    "                    continue\n",
    "                \n",
    "                en_wiki_count += 1\n",
    "                \n",
    "                # Try to determine format - check if parts[3] looks like an access type\n",
    "                # Access types are usually: desktop, mobile-web, mobile-app, etc.\n",
    "                access_types = {'desktop', 'mobile-web', 'mobile-app', 'all-sites'}\n",
    "                has_access_type = parts[3] in access_types if len(parts) > 3 else False\n",
    "                \n",
    "                if has_access_type:\n",
    "                    # Format: project page_title page_id access_type daily_total ...\n",
    "                    original_title = parts[1]\n",
    "                    page_id = parts[2]\n",
    "                    daily_total = int(parts[4]) if len(parts) > 4 else 0\n",
    "                else:\n",
    "                    # Format: project page_title page_id daily_total ...\n",
    "                    original_title = parts[1]\n",
    "                    page_id = parts[2]\n",
    "                    daily_total = int(parts[3]) if len(parts) > 3 else 0\n",
    "\n",
    "                # Convert Wikipedia title (title case with underscores) to lowercase with spaces\n",
    "                # e.g., \"Barack_Obama\" -> \"barack obama\"\n",
    "                page_title = original_title.replace('_', ' ').lower()\n",
    "                \n",
    "                # Skip entries with special titles like \"-\" or \"null\"\n",
    "                if original_title in ['-', 'null'] or not original_title:\n",
    "                    continue\n",
    "                \n",
    "                # Collect samples for debugging\n",
    "                if len(sample_pageview_titles) < 10:\n",
    "                    sample_pageview_titles.append(original_title)\n",
    "                    sample_normalized.append(page_title)\n",
    "                    # Check if this normalized title is in grok_keys\n",
    "                    if page_title in grok_keys:\n",
    "                        matches_found.append((original_title, page_title))\n",
    "                        print(f\"MATCH FOUND: '{original_title}' -> '{page_title}'\")\n",
    "                \n",
    "                # Check if normalized title is in grok_keys\n",
    "                if page_title in grok_keys:\n",
    "                    # Only add page_id if it matches grok_keys\n",
    "                    if page_id not in page_ids:\n",
    "                        page_ids.add(page_id)\n",
    "                    i += 1\n",
    "                    if i % 10000 == 0:\n",
    "                        print(f\"Matches so far: {i}, total views: {total_views:,}\")\n",
    "                    total_views += daily_total\n",
    "            except Exception as e:\n",
    "                if j < 100:  # Only print first few errors\n",
    "                    print(f\"Error on line {j}: {e}, line content: {line[:100]}\")\n",
    "                continue\n",
    "    \n",
    "    # Print debugging summary\n",
    "    print(f\"\\n=== Debugging Summary ===\")\n",
    "    print(f\"Total lines read: {j+1}\")\n",
    "    print(f\"Lines with valid format: {lines_checked}\")\n",
    "    print(f\"en.wikipedia lines found: {en_wiki_count}\")\n",
    "    if en_wiki_count == 0:\n",
    "        print(f\"\\n⚠️  WARNING: No 'en.wikipedia' entries found in this file!\")\n",
    "        print(f\"   The file appears to contain only: {list(other_projects.keys())[:10]}\")\n",
    "        print(f\"   You may need to use a different pageview file that contains English Wikipedia data.\")\n",
    "    print(f\"Other projects seen: {dict(list(other_projects.items())[:10])}\")\n",
    "    print(f\"Total matches found: {i}\")\n",
    "    print(f\"Total views: {total_views:,}\")\n",
    "    print(f\"\\nSample raw lines (first 100 chars): {sample_raw_lines[:3]}\")\n",
    "    print(f\"\\nSample pageview titles (original): {sample_pageview_titles[:10]}\")\n",
    "    print(f\"Sample normalized titles: {sample_normalized[:10]}\")\n",
    "    \n",
    "    return total_views, page_ids\n",
    "\n",
    "# grok_sep_2025_views, grok_sep_2025_page_ids = process_pageview_file(os.path.expanduser('~/Downloads/pageviews-202509-user.bz2'), grok_idx)\n",
    "grok_oct_2025_views, grok_oct_2025_page_ids = process_pageview_file(os.path.expanduser('~/Downloads/pageviews-202510-user-en-wikipedia-only.bz2'), grok_idx)\n",
    "\n",
    "# print(\"September 2025 matched grok_idx views:\", grok_sep_2025_views)\n",
    "print(\"October 2025 matched grok_idx views:\", grok_oct_2025_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4ceafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(grok_oct_2025_page_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58880cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of enwiki views in october 2025 that grokipedia pages capture\n",
    "grok_oct_2025_views / oct_2025_enwiki_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94abbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grok_oct_2025_page_ids = {\n",
    "    int(page_id)\n",
    "    for page_id in grok_oct_2025_page_ids\n",
    "    if page_id is not None and page_id != \"null\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb026b",
   "metadata": {},
   "source": [
    "### geo pv analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aec674",
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_csv('../supplemental_data/pageview_data/pageviews_0.csv')\n",
    "df1 = pd.read_csv('../supplemental_data/pageview_data/pageviews_1.csv')\n",
    "df = pd.concat([df0, df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc8cf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['project'] == 'en.wikipedia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db4a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.page_id.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd7245e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['page_id'].isin(grok_oct_2025_page_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a142497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# World map visualization of filtered noisy_views by country\n",
    "\n",
    "# Filter the dataframe\n",
    "filtered_df = df[df['page_id'].isin(grok_oct_2025_page_ids)]\n",
    "\n",
    "# Aggregate noisy_views by country_code\n",
    "filtered_agg = filtered_df.groupby('country_code')['noisy_views'].sum().reset_index()\n",
    "world = gpd.read_file(\"https://raw.githubusercontent.com/holtzy/D3-graph-gallery/master/DATA/world.geojson\")\n",
    "\n",
    "# Convert ISO 2 to ISO 3 for merging\n",
    "def iso2_to_iso3(iso2_code):\n",
    "    try:\n",
    "        country = pycountry.countries.get(alpha_2=iso2_code)\n",
    "        return country.alpha_3 if country else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "filtered_agg['country_code_iso3'] = filtered_agg['country_code'].apply(iso2_to_iso3)\n",
    "\n",
    "# Merge with world map\n",
    "filtered_world = world.merge(filtered_agg, left_on='id', right_on='country_code_iso3', how='left')\n",
    "\n",
    "# Replace NaN with 0 for countries with no data\n",
    "filtered_world['noisy_views'] = filtered_world['noisy_views'].fillna(0)\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "# Find min/max for color scale (excluding zeros)\n",
    "non_zero_values = filtered_world[filtered_world['noisy_views'] > 0]['noisy_views'].values\n",
    "if len(non_zero_values) > 0:\n",
    "    vmin = non_zero_values.min()\n",
    "    vmax = non_zero_values.max()\n",
    "else:\n",
    "    vmin = 1\n",
    "    vmax = 1\n",
    "\n",
    "# Plot filtered dataset\n",
    "filtered_world.plot(\n",
    "    column='noisy_views',\n",
    "    ax=ax,\n",
    "    cmap='YlOrRd',\n",
    "    missing_kwds={'color': 'lightgray'},\n",
    "    legend=True,\n",
    "    norm=LogNorm(vmin=max(vmin, 1), vmax=vmax),\n",
    "    legend_kwds={'shrink': 0.6, 'aspect': 20},\n",
    "    edgecolor='black',\n",
    "    linewidth=0.1\n",
    ")\n",
    "ax.set_title('Grokipedia Pageviews by Country (October 2025, Log Scale)', fontsize=16)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../graphics/pageview_world_map_grok.pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"Filtered dataset: {len(filtered_agg)} countries, {filtered_agg['noisy_views'].sum():,} total views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927dc087",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_world['noisy_views'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed907c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_world.noisy_views.sort_values()[:5].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ee410",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_world.sort_values(by='noisy_views', ascending=False)[:10].noisy_views.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d28472",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in filtered_world[['country_code', 'noisy_views']].sort_values(by='noisy_views', ascending=False)[:10]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be47bc",
   "metadata": {},
   "source": [
    "## 30k sample for topic and quality modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f68cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article_metadata(N: int, input_file: str = '../grokipedia_wikipedia_articles.ndjson'):\n",
    "    \"\"\"\n",
    "    Fetch quality and topic metadata for N articles from the ndjson file.\n",
    "    \n",
    "    Args:\n",
    "        N: Number of articles to process\n",
    "        input_file: Path to the ndjson file containing articles\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with article titles and their metadata\n",
    "    \"\"\"    \n",
    "    # Single pass: record byte offsets of each line start\n",
    "    line_offsets = []\n",
    "    with open(input_file, 'rb') as f:\n",
    "        line_offsets.append(f.tell())  # First line starts at position 0\n",
    "        while f.readline():\n",
    "            line_offsets.append(f.tell())\n",
    "    \n",
    "    file_line_count = len(line_offsets) - 1  # Last offset is EOF\n",
    "    \n",
    "    if N > file_line_count:\n",
    "        N = file_line_count\n",
    "    \n",
    "    # Select N unique random line numbers (0-indexed)\n",
    "    chosen_line_numbers = sorted(random.sample(range(file_line_count), N))\n",
    "    \n",
    "    # Seek to selected lines and read them\n",
    "    articles = []\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line_num in chosen_line_numbers:\n",
    "            f.seek(line_offsets[line_num])\n",
    "            line = f.readline()\n",
    "            try:\n",
    "                article_data = json.loads(line.strip())\n",
    "                title = article_data.get('name', '')\n",
    "                if title:\n",
    "                    # Replace spaces with underscores\n",
    "                    title_underscore = title.replace(' ', '_')\n",
    "                    articles.append({\n",
    "                        'original_title': title,\n",
    "                        'title': title_underscore\n",
    "                    })\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing random-selected line {line_num+1}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Randomly selected and loaded {len(articles)} articles\")\n",
    "\n",
    "    # Setup session with user agent\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'htriedman grokipedia'\n",
    "    })\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, article in enumerate(articles):\n",
    "        title = article['title']\n",
    "        print(f\"Processing {idx+1}/{len(articles)}: {title}\")\n",
    "        \n",
    "        result = {\n",
    "            'title': title,\n",
    "            'original_title': article['original_title'],\n",
    "            'quality_data': None,\n",
    "            'topic_data': None,\n",
    "            'errors': []\n",
    "        }\n",
    "        \n",
    "        # Fetch quality data\n",
    "        quality_url = f\"https://misalignment.wmcloud.org/api/v1/quality-article-features?lang=en&title={title}\"\n",
    "        quality_data = fetch_with_retry(session, quality_url, method='GET')\n",
    "        if quality_data:\n",
    "            result['quality_data'] = quality_data\n",
    "        else:\n",
    "            result['errors'].append('quality_fetch_failed')\n",
    "        \n",
    "        # Small delay between requests\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        # Fetch topic data\n",
    "        topic_url = \"https://api.wikimedia.org/service/lw/inference/v1/models/outlink-topic-model:predict\"\n",
    "        topic_payload = {\n",
    "            \"page_title\": title,\n",
    "            \"lang\": \"en\",\n",
    "            \"threshold\": 0.1\n",
    "        }\n",
    "        topic_data = fetch_with_retry(session, topic_url, method='POST', json_data=topic_payload)\n",
    "        if topic_data:\n",
    "            result['topic_data'] = topic_data\n",
    "        else:\n",
    "            result['errors'].append('topic_fetch_failed')\n",
    "        \n",
    "        # Delay between articles to avoid rate limiting\n",
    "        time.sleep(0.05)\n",
    "        \n",
    "        results.append(result)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def fetch_with_retry(session: requests.Session, url: str, method: str = 'GET', \n",
    "                     json_data: Optional[Dict] = None, max_retries: int = 3, \n",
    "                     base_delay: float = 2.0) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch data from an API with retry logic and rate limit handling.\n",
    "    \n",
    "    Args:\n",
    "        session: Requests session object\n",
    "        url: URL to fetch\n",
    "        method: HTTP method ('GET' or 'POST')\n",
    "        json_data: JSON payload for POST requests\n",
    "        max_retries: Maximum number of retries\n",
    "        base_delay: Base delay in seconds for exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        JSON response as dict, or None if all retries failed\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if method == 'GET':\n",
    "                response = session.get(url, timeout=30)\n",
    "            elif method == 'POST':\n",
    "                response = session.post(url, json=json_data, timeout=30)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported method: {method}\")\n",
    "            \n",
    "            # Check for rate limiting (429) or server errors (5xx)\n",
    "            if response.status_code == 429:\n",
    "                retry_after = int(response.headers.get('Retry-After', base_delay * (2 ** attempt)))\n",
    "                print(f\"Rate limited. Waiting {retry_after} seconds...\")\n",
    "                time.sleep(retry_after)\n",
    "                continue\n",
    "            \n",
    "            if response.status_code == 503:\n",
    "                wait_time = base_delay * (2 ** attempt)\n",
    "                print(f\"Service unavailable. Waiting {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            # Check for other errors\n",
    "            if response.status_code >= 500:\n",
    "                wait_time = base_delay * (2 ** attempt)\n",
    "                print(f\"Server error {response.status_code}. Waiting {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            \n",
    "            # Success\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                print(f\"Unexpected status code {response.status_code}: {response.text[:200]}\")\n",
    "                return None\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            wait_time = base_delay * (2 ** attempt)\n",
    "            print(f\"Timeout on attempt {attempt+1}. Waiting {wait_time} seconds...\")\n",
    "            time.sleep(wait_time)\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error on attempt {attempt+1}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = base_delay * (2 ** attempt)\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return None\n",
    "    \n",
    "    print(f\"Failed to fetch {url} after {max_retries} attempts\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd82d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fetch_article_metadata(N=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../results/article_metadata_results1.jsonl', 'w') as f:\n",
    "#     for item in results:\n",
    "#         f.write(json.dumps(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7183231c",
   "metadata": {},
   "source": [
    "## Similarities by topic and article class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de492de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../results/article_metadata_results.jsonl', lines=True)\n",
    "similarities = pd.read_parquet('../results/embeddings_similarities_pairwise_top1_alignments.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92f7d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = '../results'\n",
    "\n",
    "with open(f\"{RESULT_DIR}/grokipedia_wo_license.txt\", encoding=\"utf-8\") as f:\n",
    "    grokipedia_wo_license_df = pd.DataFrame({\"title\": [line.rstrip('\\n').lower() for line in f]})\n",
    "\n",
    "with open(f\"{RESULT_DIR}/grokipedia_w_license.txt\", encoding=\"utf-8\") as f:\n",
    "    grokipedia_w_license_df = pd.DataFrame({\"title\": [line.rstrip('\\n').lower() for line in f]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfcf32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, similarities, on='title', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics_from_topic_data(topic_data):\n",
    "    if topic_data is None:\n",
    "        return None\n",
    "    prediction = topic_data.get('prediction')\n",
    "    if not prediction:\n",
    "        return None\n",
    "    results = prediction.get('results', [])\n",
    "    if not results:\n",
    "        return None\n",
    "    # Only return topics with score > 0.5\n",
    "    filtered_topics = [topic for topic in results if topic.get('score', 0) > 0.5]\n",
    "    if not filtered_topics:\n",
    "        return None\n",
    "    return filtered_topics\n",
    "\n",
    "def get_article_class(quality_data):\n",
    "    if quality_data is None:\n",
    "        return None\n",
    "    class_ = quality_data.get('class', '')\n",
    "\n",
    "    if not class_:\n",
    "        return None\n",
    "    return class_\n",
    "\n",
    "def get_article_quality(quality_data):\n",
    "    if quality_data is None:\n",
    "        return None\n",
    "    quality = quality_data.get('quality', None)\n",
    "\n",
    "    if not quality:\n",
    "        return None\n",
    "    return quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc34df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['topics'] = df['topic_data'].apply(get_topics_from_topic_data)\n",
    "df = df.explode('topics')\n",
    "df['topic'] = df['topics'].apply(lambda x: x.get('topic') if isinstance(x, dict) else None)\n",
    "\n",
    "# Extract first-level topic prefix before the first dot\n",
    "df['topic_prefix'] = df['topic'].apply(lambda t: t if not (isinstance(t, str) and '.' in t) else t.split('.', 1)[0])\n",
    "df['2nd_level_topic'] = df['topic'].apply(lambda t: t if not (isinstance(t, str) and '.' in t) else '.'.join(t.split('.')[:2]))\n",
    "df['region_topic'] = df['topic'].apply(lambda t: t if (isinstance(t, str) and 'Region' in t) else None)\n",
    "\n",
    "df['class'] = df['quality_data'].apply(get_article_class)\n",
    "df['quality'] = df['quality_data'].apply(get_article_quality)\n",
    "top_prefixes = df['topic_prefix'].value_counts().index.tolist()\n",
    "second_level_topics = df['2nd_level_topic'].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_level_topics = [t for t in sorted(second_level_topics) if 'Region' not in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cef4615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['continent'] = df['region_topic'].apply(lambda t: t.split('.')[2] if isinstance(t, str) and '.' in t and len(t.split('.')) > 2 else None)\n",
    "\n",
    "df['subcontinent'] = df['topic'].apply(\n",
    "    lambda t: t.split('.')[3] if (isinstance(t, str) and 'Region' in t and len(t.split('.')) > 3) else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa757e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['region_topic'] = df['topic'].apply(lambda t: t if (isinstance(t, str) and 'Region' in t) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772fb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity distributions by license status for each top-level topic group\n",
    "# and for each second-level topic group.\n",
    "# License lists are in grokipedia_w_license_df and grokipedia_wo_license_df (lowercase, with spaces)\n",
    "\n",
    "# ---- PLOT: TOPIC PREFIX (1st level) ----\n",
    "n_prefixes = len(top_prefixes)\n",
    "ncols = 2\n",
    "nrows = math.ceil(n_prefixes / ncols)\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(12, 5 * nrows), squeeze=False)\n",
    "\n",
    "for i, prefix in enumerate(top_prefixes):\n",
    "    row = i // ncols\n",
    "    col = i % ncols\n",
    "    ax = axes[row][col]\n",
    "\n",
    "    # Select rows for this topic prefix\n",
    "    prefix_df = df[df['topic_prefix'] == prefix].copy()\n",
    "    # Prepare for join: lowercase & replace underscores with spaces to match license df\n",
    "    prefix_df[\"title_lc\"] = prefix_df[\"title\"].str.replace('_', ' ').str.lower()\n",
    "\n",
    "    # Join with license/wo_license dfs on normalized title\n",
    "    prefix_w_license = pd.merge(\n",
    "        grokipedia_w_license_df, prefix_df, left_on=\"title\", right_on=\"title_lc\"\n",
    "    )\n",
    "    prefix_wo_license = pd.merge(\n",
    "        grokipedia_wo_license_df, prefix_df, left_on=\"title\", right_on=\"title_lc\"\n",
    "    )\n",
    "\n",
    "    ax.hist(\n",
    "        [prefix_w_license[\"similarity\"].dropna(), prefix_wo_license[\"similarity\"].dropna()],\n",
    "        bins=100,\n",
    "        color=[\"tab:blue\", \"tab:orange\"],\n",
    "        label=[\"With License\", \"Without License\"],\n",
    "        alpha=0.7,\n",
    "        histtype=\"stepfilled\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Similarity\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"'{prefix}'\")\n",
    "    ax.legend()\n",
    "\n",
    "# Remove empty subplots, if any\n",
    "for j in range(i + 1, nrows * ncols):\n",
    "    fig.delaxes(axes[j // ncols][j % ncols])\n",
    "\n",
    "plt.suptitle(\"Embedding Similarity Distributions by Topic Prefix and License Status\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.savefig(f\"../graphics/embedding_similarity_by_topic_prefix_subplots_license_split.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e632cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PLOT: TOP 20 2ND LEVEL TOPICS BY CHUNK COUNT ----\n",
    "\n",
    "# Count number of chunks per 2nd level topic\n",
    "top_20_2nd_level_topics = (\n",
    "    df['2nd_level_topic']\n",
    "    .value_counts()\n",
    "    .head(20)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "top_20_2nd_level_topics = sorted(top_20_2nd_level_topics)\n",
    "\n",
    "n_second_levels = len(top_20_2nd_level_topics)\n",
    "ncols2 = 5\n",
    "nrows2 = math.ceil(n_second_levels / ncols2)\n",
    "fig2, axes2 = plt.subplots(\n",
    "    nrows2,\n",
    "    ncols2,\n",
    "    figsize=(4 * ncols2, 4 * nrows2),  # square & smaller subplots\n",
    "    squeeze=False\n",
    ")\n",
    "\n",
    "for i, second_level in enumerate(top_20_2nd_level_topics):\n",
    "    row = i // ncols2\n",
    "    col = i % ncols2\n",
    "    ax = axes2[row][col]\n",
    "\n",
    "    # Select rows for this second-level topic\n",
    "    second_level_df = df[df['2nd_level_topic'] == second_level].copy()\n",
    "    # Prepare for join: normalize as before\n",
    "    second_level_df[\"title_lc\"] = second_level_df[\"title\"].str.replace('_', ' ').str.lower()\n",
    "\n",
    "    second_level_w_license = pd.merge(\n",
    "        grokipedia_w_license_df, second_level_df, left_on=\"title\", right_on=\"title_lc\"\n",
    "    )\n",
    "    second_level_wo_license = pd.merge(\n",
    "        grokipedia_wo_license_df, second_level_df, left_on=\"title\", right_on=\"title_lc\"\n",
    "    )\n",
    "\n",
    "    h = ax.hist(\n",
    "        [second_level_w_license[\"similarity\"].dropna(), second_level_wo_license[\"similarity\"].dropna()],\n",
    "        bins=100,\n",
    "        color=[\"tab:blue\", \"tab:orange\"],\n",
    "        label=[\"With License\", \"Without License\"],\n",
    "        alpha=0.7,\n",
    "        histtype=\"stepfilled\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Similarity\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"'{second_level}'\")\n",
    "\n",
    "# Add legend to only the first axis\n",
    "axes2[0][0].legend()\n",
    "\n",
    "# Remove empty subplots, if any\n",
    "for j in range(i + 1, nrows2 * ncols2):\n",
    "    fig2.delaxes(axes2[j // ncols2][j % ncols2])\n",
    "\n",
    "plt.suptitle(\"Embedding Similarity Distributions by Top 20 2nd-Level Topics (by chunk count) and License Status\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.savefig(f\"../graphics/embedding_similarity_by_second_level_topic_subplots_license_split.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b10129",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_csv('../results/reliability_citation_diff.csv')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "import math\n",
    "\n",
    "# --- Setup categories, labels, colors ---\n",
    "column_order = [\n",
    "    'reliable', 'unreliable', 'blacklist', 'no_consensus', 'deprecated', 'other'\n",
    "]\n",
    "display_names = {\n",
    "    'reliable': 'Generally reliable',\n",
    "    'unreliable': 'Generally unreliable',\n",
    "    'blacklist': 'Blacklisted',\n",
    "    'no_consensus': 'No consensus',\n",
    "    'deprecated': 'Deprecated',\n",
    "    'other': 'Other'\n",
    "}\n",
    "color_map = {\n",
    "    'reliable': 'green',\n",
    "    'unreliable': 'red',\n",
    "    'blacklist': 'black',\n",
    "    'no_consensus': 'yellow',\n",
    "    'deprecated': 'orange',\n",
    "    'other': 'grey'\n",
    "}\n",
    "\n",
    "# Add missing columns if they don't exist (set to 0)\n",
    "for col in ['wp_blacklist', 'wp_no_consensus', 'wp_deprecated', \n",
    "            'grok_blacklist', 'grok_no_consensus', 'grok_deprecated']:\n",
    "    if col not in filtered_df.columns:\n",
    "        filtered_df[col] = 0\n",
    "\n",
    "# --- Join with topic data ---\n",
    "# Get unique titles with their 2nd level topics from df\n",
    "# Note: df may have multiple rows per title due to explode, so we keep all topic assignments\n",
    "df_with_topics = df[['title', '2nd_level_topic']].copy()\n",
    "\n",
    "# Merge citation data with topic data - join directly on title (both use underscores)\n",
    "# This will create multiple rows per article if an article has multiple topics\n",
    "citation_with_topics = pd.merge(\n",
    "    filtered_df, df_with_topics, on='title', how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Total articles in filtered_df: {len(filtered_df)}\")\n",
    "print(f\"Total articles after join: {len(citation_with_topics)}\")\n",
    "print(f\"Articles with topics: {citation_with_topics['2nd_level_topic'].notna().sum()}\")\n",
    "print(f\"Sample citation counts - reliable: {citation_with_topics['wp_reliable'].sum()}, unreliable: {citation_with_topics['wp_unreliable'].sum()}, other: {citation_with_topics['wp_other'].sum()}\")\n",
    "\n",
    "# Get top 20 2nd level topics by citation count\n",
    "top_20_2nd_level_topics = (\n",
    "    citation_with_topics['2nd_level_topic']\n",
    "    .value_counts()\n",
    "    .head(20)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "top_20_2nd_level_topics = sorted([t for t in top_20_2nd_level_topics if t is not None])\n",
    "\n",
    "# --- Create subplots ---\n",
    "n_second_levels = len(top_20_2nd_level_topics)\n",
    "ncols2 = 5\n",
    "nrows2 = math.ceil(n_second_levels / ncols2)\n",
    "fig2, axes2 = plt.subplots(\n",
    "    nrows2,\n",
    "    ncols2,\n",
    "    figsize=(4 * ncols2, 4 * nrows2),\n",
    "    squeeze=False\n",
    ")\n",
    "\n",
    "for i, second_level in enumerate(top_20_2nd_level_topics):\n",
    "    row = i // ncols2\n",
    "    col = i % ncols2\n",
    "    ax = axes2[row][col]\n",
    "\n",
    "    # Select rows for this second-level topic\n",
    "    second_level_df = citation_with_topics[citation_with_topics['2nd_level_topic'] == second_level].copy()\n",
    "    \n",
    "    # Aggregate counts for this topic\n",
    "    agg = {\n",
    "        'wp_reliable': second_level_df['wp_reliable'].sum(),\n",
    "        'wp_unreliable': second_level_df['wp_unreliable'].sum(),\n",
    "        'wp_blacklist': second_level_df['wp_blacklist'].sum(),\n",
    "        'wp_no_consensus': second_level_df['wp_no_consensus'].sum(),\n",
    "        'wp_deprecated': second_level_df['wp_deprecated'].sum(),\n",
    "        'wp_other': second_level_df['wp_other'].sum(),\n",
    "        'grok_reliable': second_level_df['grok_reliable'].sum(),\n",
    "        'grok_unreliable': second_level_df['grok_unreliable'].sum(),\n",
    "        'grok_blacklist': second_level_df['grok_blacklist'].sum(),\n",
    "        'grok_no_consensus': second_level_df['grok_no_consensus'].sum(),\n",
    "        'grok_deprecated': second_level_df['grok_deprecated'].sum(),\n",
    "        'grok_other': second_level_df['grok_other'].sum(),\n",
    "    }\n",
    "\n",
    "    # Make DF of shape: index=['Wikipedia', 'Grokipedia'], columns=column_order\n",
    "    wp_row = [agg[f'wp_{k}'] for k in column_order]\n",
    "    grok_row = [agg[f'grok_{k}'] for k in column_order]\n",
    "    prop_df = pd.DataFrame(\n",
    "        [wp_row, grok_row],\n",
    "        columns=column_order,\n",
    "        index=['Wikipedia', 'Grokipedia']\n",
    "    )\n",
    "    prop_df_norm = prop_df.div(prop_df.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    # --- Plotting stacked bars ---\n",
    "    labels = ['Wikipedia', 'Grokipedia']\n",
    "    x = np.arange(len(labels))\n",
    "    bar_sep = 0.09\n",
    "    width = 0.18\n",
    "\n",
    "    # Set up stacking\n",
    "    bottoms = [0, 0]\n",
    "\n",
    "    # For synchronized stacking, process in column order:\n",
    "    for j, col_name in enumerate(column_order):\n",
    "        color = color_map.get(col_name, 'grey')\n",
    "        # WP bar proportions\n",
    "        wp_prop = prop_df_norm.loc['Wikipedia', col_name]\n",
    "        grok_prop = prop_df_norm.loc['Grokipedia', col_name]\n",
    "        ax.bar(x[0] - width/2 - bar_sep/2, wp_prop, width=width,\n",
    "                bottom=bottoms[0], color=color, edgecolor='none', zorder=2, alpha=0.5)\n",
    "        ax.bar(x[1] + width/2 + bar_sep/2, grok_prop, width=width,\n",
    "                bottom=bottoms[1], color=color, edgecolor='none', zorder=2, alpha=0.5)\n",
    "\n",
    "        # Diagonal change fill\n",
    "        wp_top = bottoms[0] + wp_prop\n",
    "        grok_top = bottoms[1] + grok_prop\n",
    "        ax.fill_between(\n",
    "            [x[0] - width/2, x[1] + width/2],\n",
    "            [wp_top, grok_top],\n",
    "            [bottoms[0], bottoms[1]],\n",
    "            color=color, alpha=0.14, zorder=1, linewidth=0\n",
    "        )\n",
    "        bottoms[0] += wp_prop\n",
    "        bottoms[1] += grok_prop\n",
    "\n",
    "    # Set axis ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.set_ylabel(\"Proportion of Citations\")\n",
    "    ax.set_title(f\"'{second_level}'\")\n",
    "    \n",
    "    # Make axis tight with bars\n",
    "    ax.set_xlim(-0.23, 1.23)\n",
    "    ax.set_ylim(bottom=0, top=1.01)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "# Add legend to only the first axis\n",
    "legend_elements = [Patch(facecolor=color_map.get(col, 'grey'), label=display_names.get(col, col), alpha=0.55) for col in column_order]\n",
    "axes2[0][0].legend(handles=legend_elements, title='Source Status', loc='upper center', framealpha=0.9)\n",
    "\n",
    "# Remove empty subplots, if any\n",
    "for j in range(i + 1, nrows2 * ncols2):\n",
    "    fig2.delaxes(axes2[j // ncols2][j % ncols2])\n",
    "\n",
    "plt.suptitle(\"Citation Status Proportion by Top 20 2nd-Level Topics (by citation count): Wikipedia vs Grokipedia\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.savefig(f\"../graphics/citation_composition_by_second_level_topic_subplots.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2473a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PLOT: REGION TOPIC on GEOSPATIAL 6x? GRID ----\n",
    "\n",
    "# Define canonical region and continent values from arrays (ignore continent* and region None)\n",
    "region_values = [\n",
    "    'South_Asia', 'Asia*', 'Europe*', 'East_Asia', 'Western_Europe', 'North_America', 'Southern_Europe',\n",
    "    'Western_Africa', 'Africa*', 'West_Asia', 'Eastern_Europe', 'North_Asia', 'Northern_Europe', 'Southeast_Asia',\n",
    "    'Central_Asia', 'South_America', 'Central_America', 'Northern_Africa', 'Central_Africa', 'Southern_Africa',\n",
    "    'Eastern_Africa'\n",
    "]\n",
    "continent_values = ['Asia', 'Europe', 'Americas', 'Africa', 'Oceania']\n",
    "\n",
    "# Lay out regions approximately geospatially (None for blank grid cells)\n",
    "# (Manual arrangement for visual resemblance and coverage)\n",
    "region_grid_labels = [\n",
    "#    0               1                2               3                 4                  5\n",
    "    ['North_America',  None,        None,         'Northern_Europe',    None,           'North_Asia', None],\n",
    "    ['Central_America',None, 'Western_Europe',  'Southern_Europe', 'Eastern_Europe',  'Central_Asia', 'East_Asia'],\n",
    "    [None,             'South_America',        None,        'Northern_Africa',     None,            'West_Asia', None],\n",
    "    [None,       None, 'Western_Africa', 'Central_Africa',  'Eastern_Africa', 'South_Asia', 'Southeast_Asia'],\n",
    "    [None,        None,        None,       'Southern_Africa',     None,        None, 'Oceania'],\n",
    "]\n",
    "\n",
    "print(\"== GEOSPATIAL REGION 6x? GRID ==\")\n",
    "for row in region_grid_labels:\n",
    "    print([r if r is not None else \"---\" for r in row])\n",
    "\n",
    "# All region_topics in the data (ignore None)\n",
    "input_region_topics = set(df['region_topic'].dropna().unique())\n",
    "\n",
    "# Standardize region naming for mapping: e.g., 'South_Asia', 'North_America'\n",
    "def standardize_region_key(r):\n",
    "    # Handles e.g. 'Geography.Regions.Americas.North_America' -> 'north_america', 'Asia*' -> 'asia*'\n",
    "    if '.' in r:\n",
    "        last = r.split('.')[-1]\n",
    "    else:\n",
    "        last = r\n",
    "    return last.strip().replace(' ', '_').replace('-', '_').lower()\n",
    "\n",
    "# Map from lower-snakecase region key to region value in the data\n",
    "input_region_map = {standardize_region_key(r): r for r in input_region_topics}\n",
    "\n",
    "# Build (label, canonical-region) grid for plotting\n",
    "region_plot_grid = []\n",
    "for row in region_grid_labels:\n",
    "    label_row = []\n",
    "    for label in row:\n",
    "        if label is None:\n",
    "            label_row.append(None)\n",
    "            continue\n",
    "        std_key = label.replace('-', '_').lower()\n",
    "        canonical = input_region_map.get(std_key)\n",
    "        if canonical is None:\n",
    "            # Try looser containment fallback (for \"*\", e.g. \"asia*\" matches \"asia*\")\n",
    "            canonical = next((orig for std, orig in input_region_map.items() if std_key in std), None)\n",
    "        label_row.append((label, canonical))\n",
    "    region_plot_grid.append(label_row)\n",
    "\n",
    "nrows = len(region_plot_grid)\n",
    "ncols = max(len(row) for row in region_plot_grid)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows, ncols,\n",
    "    figsize=(4 * ncols, 3.5 * nrows),\n",
    "    squeeze=False\n",
    ")\n",
    "\n",
    "for row_idx, row in enumerate(region_plot_grid):\n",
    "    for col_idx, cell in enumerate(row):\n",
    "        ax = axes[row_idx][col_idx]\n",
    "        if cell is None or cell[1] is None:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        label, region = cell\n",
    "        # Only show if region in our filtered region_values list\n",
    "        r_label = label.replace('-', '_')\n",
    "        # Consider region_values is all upper snakecase; do lower for compare\n",
    "        if r_label.lower() not in [rv.lower() for rv in region_values]:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        region_df = df[df['region_topic'] == region].copy()\n",
    "        if region_df.empty:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        region_df[\"title_lc\"] = region_df[\"title\"].str.replace('_', ' ').str.lower()\n",
    "\n",
    "        region_w_license = pd.merge(\n",
    "            grokipedia_w_license_df, region_df, left_on=\"title\", right_on=\"title_lc\"\n",
    "        )\n",
    "        region_wo_license = pd.merge(\n",
    "            grokipedia_wo_license_df, region_df, left_on=\"title\", right_on=\"title_lc\"\n",
    "        )\n",
    "\n",
    "        h = ax.hist(\n",
    "            [region_w_license[\"similarity\"].dropna(), region_wo_license[\"similarity\"].dropna()],\n",
    "            bins=100,\n",
    "            color=[\"tab:blue\", \"tab:orange\"],\n",
    "            label=[\"With License\", \"Without License\"],\n",
    "            alpha=0.7,\n",
    "            histtype=\"stepfilled\",\n",
    "        )\n",
    "        ax.set_xlabel(\"Similarity\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        pretty_label = label.replace('_', ' ').replace('-', ' ').replace('*', ' (all)').title()\n",
    "        ax.set_title(pretty_label)\n",
    "\n",
    "# Add legend to only the first visible axis\n",
    "legend_added = False\n",
    "for r in range(nrows):\n",
    "    for c in range(ncols):\n",
    "        if c >= len(region_plot_grid[r]):\n",
    "            continue  # safety for non-rectangular grid\n",
    "        cell = region_plot_grid[r][c]\n",
    "        ax = axes[r][c]\n",
    "        if not legend_added and cell is not None and cell[1] is not None:\n",
    "            ax.legend()\n",
    "            legend_added = True\n",
    "\n",
    "plt.suptitle(\"Embedding Similarity Distributions by Geospatial Region and License Status\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.savefig(f\"../graphics/embedding_similarity_by_spatial_region_6col_grid_license_split.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a25ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PLOT: CITATION COMPOSITION BY SUBCONTINENT on GEOSPATIAL GRID ----\n",
    "\n",
    "filtered_df = pd.read_csv('../results/reliability_citation_diff.csv')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "import math\n",
    "\n",
    "# --- Setup categories, labels, colors ---\n",
    "column_order = [\n",
    "    'reliable', 'unreliable', 'blacklist', 'no_consensus', 'deprecated', 'other'\n",
    "]\n",
    "display_names = {\n",
    "    'reliable': 'Generally reliable',\n",
    "    'unreliable': 'Generally unreliable',\n",
    "    'blacklist': 'Blacklisted',\n",
    "    'no_consensus': 'No consensus',\n",
    "    'deprecated': 'Deprecated',\n",
    "    'other': 'Other'\n",
    "}\n",
    "color_map = {\n",
    "    'reliable': 'green',\n",
    "    'unreliable': 'red',\n",
    "    'blacklist': 'black',\n",
    "    'no_consensus': 'yellow',\n",
    "    'deprecated': 'orange',\n",
    "    'other': 'grey'\n",
    "}\n",
    "\n",
    "# Add missing columns if they don't exist (set to 0)\n",
    "for col in ['wp_blacklist', 'wp_no_consensus', 'wp_deprecated', \n",
    "            'grok_blacklist', 'grok_no_consensus', 'grok_deprecated']:\n",
    "    if col not in filtered_df.columns:\n",
    "        filtered_df[col] = 0\n",
    "\n",
    "# --- Join with subcontinent data ---\n",
    "# Get unique titles with their subcontinents from df\n",
    "df_with_subcontinents = df[['title', 'subcontinent']].dropna().copy()\n",
    "\n",
    "# Merge citation data with subcontinent data - join directly on title\n",
    "citation_with_subcontinents = pd.merge(\n",
    "    filtered_df, df_with_subcontinents, on='title', how='inner'\n",
    ")\n",
    "\n",
    "# Define subcontinent grid layout (similar to region grid)\n",
    "subcontinent_grid_labels = [\n",
    "    ['North_America', None, None, 'Northern_Europe', None, 'North_Asia', None],\n",
    "    ['Central_America', None, 'Western_Europe', 'Southern_Europe', 'Eastern_Europe', 'Central_Asia', 'East_Asia'],\n",
    "    [None, 'South_America', None, 'Northern_Africa', None, 'West_Asia', None],\n",
    "    [None, None, 'Western_Africa', 'Central_Africa', 'Eastern_Africa', 'South_Asia', 'Southeast_Asia'],\n",
    "    [None, None, None, 'Southern_Africa', None, None, 'Oceania'],\n",
    "]\n",
    "\n",
    "print(\"== GEOSPATIAL SUBCONTINENT GRID ==\")\n",
    "for row in subcontinent_grid_labels:\n",
    "    print([r if r is not None else \"---\" for r in row])\n",
    "\n",
    "# All subcontinents in the data\n",
    "input_subcontinents = set(citation_with_subcontinents['subcontinent'].dropna().unique())\n",
    "\n",
    "# Standardize subcontinent naming for mapping\n",
    "def standardize_subcontinent_key(s):\n",
    "    # Handles e.g. 'Geography.Regions.Americas.North_America' -> 'north_america'\n",
    "    if '.' in s:\n",
    "        last = s.split('.')[-1]\n",
    "    else:\n",
    "        last = s\n",
    "    return last.strip().replace(' ', '_').replace('-', '_').lower()\n",
    "\n",
    "# Map from lower-snakecase subcontinent key to subcontinent value in the data\n",
    "input_subcontinent_map = {standardize_subcontinent_key(s): s for s in input_subcontinents}\n",
    "\n",
    "# Build (label, canonical-subcontinent) grid for plotting\n",
    "subcontinent_plot_grid = []\n",
    "for row in subcontinent_grid_labels:\n",
    "    label_row = []\n",
    "    for label in row:\n",
    "        if label is None:\n",
    "            label_row.append(None)\n",
    "            continue\n",
    "        std_key = label.replace('-', '_').lower()\n",
    "        canonical = input_subcontinent_map.get(std_key)\n",
    "        if canonical is None:\n",
    "            # Try looser containment fallback\n",
    "            canonical = next((orig for std, orig in input_subcontinent_map.items() if std_key in std), None)\n",
    "        label_row.append((label, canonical))\n",
    "    subcontinent_plot_grid.append(label_row)\n",
    "\n",
    "nrows = len(subcontinent_plot_grid)\n",
    "ncols = max(len(row) for row in subcontinent_plot_grid)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows, ncols,\n",
    "    figsize=(4 * ncols, 3.5 * nrows),\n",
    "    squeeze=False\n",
    ")\n",
    "\n",
    "for row_idx, row in enumerate(subcontinent_plot_grid):\n",
    "    for col_idx, cell in enumerate(row):\n",
    "        ax = axes[row_idx][col_idx]\n",
    "        if cell is None or cell[1] is None:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        label, subcontinent = cell\n",
    "        \n",
    "        # Select rows for this subcontinent\n",
    "        subcontinent_df = citation_with_subcontinents[citation_with_subcontinents['subcontinent'] == subcontinent].copy()\n",
    "        if subcontinent_df.empty:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        # Aggregate counts for this subcontinent\n",
    "        agg = {\n",
    "            'wp_reliable': subcontinent_df['wp_reliable'].sum(),\n",
    "            'wp_unreliable': subcontinent_df['wp_unreliable'].sum(),\n",
    "            'wp_blacklist': subcontinent_df['wp_blacklist'].sum(),\n",
    "            'wp_no_consensus': subcontinent_df['wp_no_consensus'].sum(),\n",
    "            'wp_deprecated': subcontinent_df['wp_deprecated'].sum(),\n",
    "            'wp_other': subcontinent_df['wp_other'].sum(),\n",
    "            'grok_reliable': subcontinent_df['grok_reliable'].sum(),\n",
    "            'grok_unreliable': subcontinent_df['grok_unreliable'].sum(),\n",
    "            'grok_blacklist': subcontinent_df['grok_blacklist'].sum(),\n",
    "            'grok_no_consensus': subcontinent_df['grok_no_consensus'].sum(),\n",
    "            'grok_deprecated': subcontinent_df['grok_deprecated'].sum(),\n",
    "            'grok_other': subcontinent_df['grok_other'].sum(),\n",
    "        }\n",
    "\n",
    "        # Make DF of shape: index=['Wikipedia', 'Grokipedia'], columns=column_order\n",
    "        wp_row = [agg[f'wp_{k}'] for k in column_order]\n",
    "        grok_row = [agg[f'grok_{k}'] for k in column_order]\n",
    "        prop_df = pd.DataFrame(\n",
    "            [wp_row, grok_row],\n",
    "            columns=column_order,\n",
    "            index=['Wikipedia', 'Grokipedia']\n",
    "        )\n",
    "        prop_df_norm = prop_df.div(prop_df.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "        # --- Plotting stacked bars ---\n",
    "        labels = ['Wikipedia', 'Grokipedia']\n",
    "        x = np.arange(len(labels))\n",
    "        bar_sep = 0.09\n",
    "        width = 0.18\n",
    "\n",
    "        # Set up stacking\n",
    "        bottoms = [0, 0]\n",
    "\n",
    "        # For synchronized stacking, process in column order:\n",
    "        for j, col_name in enumerate(column_order):\n",
    "            color = color_map.get(col_name, 'grey')\n",
    "            # WP bar proportions\n",
    "            wp_prop = prop_df_norm.loc['Wikipedia', col_name]\n",
    "            grok_prop = prop_df_norm.loc['Grokipedia', col_name]\n",
    "            ax.bar(x[0] - width/2 - bar_sep/2, wp_prop, width=width,\n",
    "                    bottom=bottoms[0], color=color, edgecolor='none', zorder=2, alpha=0.5)\n",
    "            ax.bar(x[1] + width/2 + bar_sep/2, grok_prop, width=width,\n",
    "                    bottom=bottoms[1], color=color, edgecolor='none', zorder=2, alpha=0.5)\n",
    "\n",
    "            # Diagonal change fill\n",
    "            wp_top = bottoms[0] + wp_prop\n",
    "            grok_top = bottoms[1] + grok_prop\n",
    "            ax.fill_between(\n",
    "                [x[0] - width/2, x[1] + width/2],\n",
    "                [wp_top, grok_top],\n",
    "                [bottoms[0], bottoms[1]],\n",
    "                color=color, alpha=0.14, zorder=1, linewidth=0\n",
    "            )\n",
    "            bottoms[0] += wp_prop\n",
    "            bottoms[1] += grok_prop\n",
    "\n",
    "        # Set axis ticks and labels\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_ylabel(\"Proportion of Citations\")\n",
    "        pretty_label = label.replace('_', ' ').replace('-', ' ').replace('*', ' (all)').title()\n",
    "        ax.set_title(pretty_label)\n",
    "        \n",
    "        # Make axis tight with bars\n",
    "        ax.set_xlim(-0.23, 1.23)\n",
    "        ax.set_ylim(bottom=0, top=1.01)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "# Add legend to only the first visible axis\n",
    "legend_added = False\n",
    "for r in range(nrows):\n",
    "    for c in range(ncols):\n",
    "        if c >= len(subcontinent_plot_grid[r]):\n",
    "            continue  # safety for non-rectangular grid\n",
    "        cell = subcontinent_plot_grid[r][c]\n",
    "        ax = axes[r][c]\n",
    "        if not legend_added and cell is not None and cell[1] is not None:\n",
    "            legend_elements = [Patch(facecolor=color_map.get(col, 'grey'), label=display_names.get(col, col), alpha=0.55) for col in column_order]\n",
    "            ax.legend(handles=legend_elements, title='Source Status', loc='upper center', framealpha=0.9)\n",
    "            legend_added = True\n",
    "\n",
    "plt.suptitle(\"Citation Status Proportion by Subcontinent: Wikipedia vs Grokipedia\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.savefig(f\"../graphics/citation_composition_by_subcontinent_grid.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d47cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c859fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PLOT: CONTINENT TOPIC on GEOSPATIAL 6x? GRID ----\n",
    "\n",
    "# Fix for \"Americas\" not showing up: \n",
    "# We'll reconstruct a continent mapping from region_topic codes.\n",
    "# Most region_topic values have format: \"Geography.Regions.<Continent>[.<...>]\"\n",
    "# We'll extract the continent for each row and use that to plot.\n",
    "\n",
    "continent_values = ['Asia', 'Europe', 'Americas', 'Africa', 'Oceania']\n",
    "region_grid_labels = [\n",
    "    ['Americas', 'Europe', 'Asia'],\n",
    "    [None, 'Africa', 'Oceania'],\n",
    "]\n",
    "\n",
    "print(\"== GEOSPATIAL CONTINENTS GRID TO PLOT ==\")\n",
    "for row in region_grid_labels:\n",
    "    print([r if r is not None else \"---\" for r in row])\n",
    "\n",
    "def extract_continent(region_topic):\n",
    "    \"\"\"Extracts continent from region_topic string, e.g.: \n",
    "       'Geography.Regions.Americas.North_America' => 'Americas'\n",
    "       'Geography.Regions.Europe.Europe*' => 'Europe'\n",
    "    \"\"\"\n",
    "    if not isinstance(region_topic, str):\n",
    "        return None\n",
    "    parts = region_topic.split('.')\n",
    "    # Look for \"Geography.Regions.<Continent>\"\n",
    "    try:\n",
    "        r_idx = parts.index('Regions') + 1\n",
    "        if r_idx < len(parts):\n",
    "            cont = parts[r_idx]\n",
    "            # Canonicalize for special cases ('Americas', 'Asia', etc)\n",
    "            for v in continent_values:\n",
    "                if cont.lower().startswith(v.lower()):\n",
    "                    return v\n",
    "    except (ValueError, IndexError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Add a column to df for continent (avoid mutating original)\n",
    "df_cont = df.copy()\n",
    "df_cont['continent'] = df_cont['region_topic'].apply(extract_continent)\n",
    "\n",
    "# If we want \"Asia*\" etc to count as \"Asia\", this function above will handle that.\n",
    "\n",
    "nrows = len(region_grid_labels)\n",
    "ncols = max(len(row) for row in region_grid_labels)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows, ncols,\n",
    "    figsize=(4 * ncols, 3.5 * nrows),\n",
    "    squeeze=False\n",
    ")\n",
    "\n",
    "for row_idx, row in enumerate(region_grid_labels):\n",
    "    for col_idx, label in enumerate(row):\n",
    "        ax = axes[row_idx][col_idx]\n",
    "        if label is None:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        # Only show if label is a known continent\n",
    "        if label not in continent_values:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        region_df = df_cont[df_cont['continent'] == label].copy()\n",
    "        if region_df.empty:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        region_df[\"title_lc\"] = region_df[\"title\"].str.replace('_', ' ').str.lower()\n",
    "        region_w_license = pd.merge(\n",
    "            grokipedia_w_license_df, region_df, left_on=\"title\", right_on=\"title_lc\"\n",
    "        )\n",
    "        region_wo_license = pd.merge(\n",
    "            grokipedia_wo_license_df, region_df, left_on=\"title\", right_on=\"title_lc\"\n",
    "        )\n",
    "        h = ax.hist(\n",
    "            [region_w_license[\"similarity\"].dropna(), region_wo_license[\"similarity\"].dropna()],\n",
    "            bins=100,\n",
    "            color=[\"tab:blue\", \"tab:orange\"],\n",
    "            label=[\"With License\", \"Without License\"],\n",
    "            alpha=0.7,\n",
    "            histtype=\"stepfilled\",\n",
    "        )\n",
    "        ax.set_xlabel(\"Similarity\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        pretty_label = label.replace('_', ' ').replace('-', ' ').replace('*', ' (all)').title()\n",
    "        ax.set_title(pretty_label)\n",
    "\n",
    "# Add legend to only the first visible axis\n",
    "legend_added = False\n",
    "for r in range(nrows):\n",
    "    for c in range(ncols):\n",
    "        if c >= len(region_grid_labels[r]):\n",
    "            continue\n",
    "        ax = axes[r][c]\n",
    "        cell_label = region_grid_labels[r][c]\n",
    "        if not legend_added and cell_label is not None:\n",
    "            ax.legend()\n",
    "            legend_added = True\n",
    "\n",
    "plt.suptitle(\"Embedding Similarity Distributions by Continent and License Status\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.savefig(f\"../graphics/embedding_similarity_by_continents_grid_license_split.pdf\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae538217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- PLOT: CITATION COMPOSITION BY CONTINENT on GEOSPATIAL GRID ----\n",
    "\n",
    "filtered_df = pd.read_csv('../results/reliability_citation_diff.csv')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "import math\n",
    "\n",
    "# --- Setup categories, labels, colors ---\n",
    "column_order = [\n",
    "    'reliable', 'unreliable', 'blacklist', 'no_consensus', 'deprecated', 'other'\n",
    "]\n",
    "display_names = {\n",
    "    'reliable': 'Generally reliable',\n",
    "    'unreliable': 'Generally unreliable',\n",
    "    'blacklist': 'Blacklisted',\n",
    "    'no_consensus': 'No consensus',\n",
    "    'deprecated': 'Deprecated',\n",
    "    'other': 'Other'\n",
    "}\n",
    "color_map = {\n",
    "    'reliable': 'green',\n",
    "    'unreliable': 'red',\n",
    "    'blacklist': 'black',\n",
    "    'no_consensus': 'yellow',\n",
    "    'deprecated': 'orange',\n",
    "    'other': 'grey'\n",
    "}\n",
    "\n",
    "# Add missing columns if they don't exist (set to 0)\n",
    "for col in ['wp_blacklist', 'wp_no_consensus', 'wp_deprecated', \n",
    "            'grok_blacklist', 'grok_no_consensus', 'grok_deprecated']:\n",
    "    if col not in filtered_df.columns:\n",
    "        filtered_df[col] = 0\n",
    "\n",
    "# --- Extract continent from region_topic ---\n",
    "continent_values = ['Asia', 'Europe', 'Americas', 'Africa', 'Oceania']\n",
    "\n",
    "def extract_continent(region_topic):\n",
    "    \"\"\"Extracts continent from region_topic string, e.g.: \n",
    "       'Geography.Regions.Americas.North_America' => 'Americas'\n",
    "       'Geography.Regions.Europe.Europe*' => 'Europe'\n",
    "    \"\"\"\n",
    "    if not isinstance(region_topic, str):\n",
    "        return None\n",
    "    parts = region_topic.split('.')\n",
    "    # Look for \"Geography.Regions.<Continent>\"\n",
    "    try:\n",
    "        r_idx = parts.index('Regions') + 1\n",
    "        if r_idx < len(parts):\n",
    "            cont = parts[r_idx]\n",
    "            # Canonicalize for special cases ('Americas', 'Asia', etc)\n",
    "            for v in continent_values:\n",
    "                if cont.lower().startswith(v.lower()):\n",
    "                    return v\n",
    "    except (ValueError, IndexError):\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Add a column to df for continent\n",
    "df_cont = df.copy()\n",
    "df_cont['continent'] = df_cont['region_topic'].apply(extract_continent)\n",
    "\n",
    "# --- Join with continent data ---\n",
    "# Get unique titles with their continents from df\n",
    "df_with_continents = df_cont[['title', 'continent']].dropna().copy()\n",
    "\n",
    "# Merge citation data with continent data - join directly on title\n",
    "citation_with_continents = pd.merge(\n",
    "    filtered_df, df_with_continents, on='title', how='inner'\n",
    ")\n",
    "\n",
    "# Define continent grid layout\n",
    "continent_grid_labels = [\n",
    "    ['Americas', 'Europe', 'Asia'],\n",
    "    [None, 'Africa', 'Oceania'],\n",
    "]\n",
    "\n",
    "print(\"== GEOSPATIAL CONTINENTS GRID ==\")\n",
    "for row in continent_grid_labels:\n",
    "    print([r if r is not None else \"---\" for r in row])\n",
    "\n",
    "nrows = len(continent_grid_labels)\n",
    "ncols = max(len(row) for row in continent_grid_labels)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows, ncols,\n",
    "    figsize=(4 * ncols, 3.5 * nrows),\n",
    "    squeeze=False\n",
    ")\n",
    "\n",
    "for row_idx, row in enumerate(continent_grid_labels):\n",
    "    for col_idx, label in enumerate(row):\n",
    "        ax = axes[row_idx][col_idx]\n",
    "        if label is None:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        # Only show if label is a known continent\n",
    "        if label not in continent_values:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        # Select rows for this continent\n",
    "        continent_df = citation_with_continents[citation_with_continents['continent'] == label].copy()\n",
    "        if continent_df.empty:\n",
    "            ax.axis('off')\n",
    "            continue\n",
    "        \n",
    "        # Aggregate counts for this continent\n",
    "        agg = {\n",
    "            'wp_reliable': continent_df['wp_reliable'].sum(),\n",
    "            'wp_unreliable': continent_df['wp_unreliable'].sum(),\n",
    "            'wp_blacklist': continent_df['wp_blacklist'].sum(),\n",
    "            'wp_no_consensus': continent_df['wp_no_consensus'].sum(),\n",
    "            'wp_deprecated': continent_df['wp_deprecated'].sum(),\n",
    "            'wp_other': continent_df['wp_other'].sum(),\n",
    "            'grok_reliable': continent_df['grok_reliable'].sum(),\n",
    "            'grok_unreliable': continent_df['grok_unreliable'].sum(),\n",
    "            'grok_blacklist': continent_df['grok_blacklist'].sum(),\n",
    "            'grok_no_consensus': continent_df['grok_no_consensus'].sum(),\n",
    "            'grok_deprecated': continent_df['grok_deprecated'].sum(),\n",
    "            'grok_other': continent_df['grok_other'].sum(),\n",
    "        }\n",
    "\n",
    "        # Make DF of shape: index=['Wikipedia', 'Grokipedia'], columns=column_order\n",
    "        wp_row = [agg[f'wp_{k}'] for k in column_order]\n",
    "        grok_row = [agg[f'grok_{k}'] for k in column_order]\n",
    "        prop_df = pd.DataFrame(\n",
    "            [wp_row, grok_row],\n",
    "            columns=column_order,\n",
    "            index=['Wikipedia', 'Grokipedia']\n",
    "        )\n",
    "        prop_df_norm = prop_df.div(prop_df.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "        # --- Plotting stacked bars ---\n",
    "        labels = ['Wikipedia', 'Grokipedia']\n",
    "        x = np.arange(len(labels))\n",
    "        bar_sep = 0.09\n",
    "        width = 0.18\n",
    "\n",
    "        # Set up stacking\n",
    "        bottoms = [0, 0]\n",
    "\n",
    "        # For synchronized stacking, process in column order:\n",
    "        for j, col_name in enumerate(column_order):\n",
    "            color = color_map.get(col_name, 'grey')\n",
    "            # WP bar proportions\n",
    "            wp_prop = prop_df_norm.loc['Wikipedia', col_name]\n",
    "            grok_prop = prop_df_norm.loc['Grokipedia', col_name]\n",
    "            ax.bar(x[0] - width/2 - bar_sep/2, wp_prop, width=width,\n",
    "                    bottom=bottoms[0], color=color, edgecolor='none', zorder=2, alpha=0.5)\n",
    "            ax.bar(x[1] + width/2 + bar_sep/2, grok_prop, width=width,\n",
    "                    bottom=bottoms[1], color=color, edgecolor='none', zorder=2, alpha=0.5)\n",
    "\n",
    "            # Diagonal change fill\n",
    "            wp_top = bottoms[0] + wp_prop\n",
    "            grok_top = bottoms[1] + grok_prop\n",
    "            ax.fill_between(\n",
    "                [x[0] - width/2, x[1] + width/2],\n",
    "                [wp_top, grok_top],\n",
    "                [bottoms[0], bottoms[1]],\n",
    "                color=color, alpha=0.14, zorder=1, linewidth=0\n",
    "            )\n",
    "            bottoms[0] += wp_prop\n",
    "            bottoms[1] += grok_prop\n",
    "\n",
    "        # Set axis ticks and labels\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(labels)\n",
    "        ax.set_ylabel(\"Proportion of Citations\")\n",
    "        pretty_label = label.replace('_', ' ').replace('-', ' ').replace('*', ' (all)').title()\n",
    "        ax.set_title(pretty_label)\n",
    "        \n",
    "        # Make axis tight with bars\n",
    "        ax.set_xlim(-0.23, 1.23)\n",
    "        ax.set_ylim(bottom=0, top=1.01)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "# Add legend to only the first visible axis\n",
    "legend_added = False\n",
    "for r in range(nrows):\n",
    "    for c in range(ncols):\n",
    "        if c >= len(continent_grid_labels[r]):\n",
    "            continue\n",
    "        ax = axes[r][c]\n",
    "        cell_label = continent_grid_labels[r][c]\n",
    "        if not legend_added and cell_label is not None:\n",
    "            legend_elements = [Patch(facecolor=color_map.get(col, 'grey'), label=display_names.get(col, col), alpha=0.55) for col in column_order]\n",
    "            ax.legend(handles=legend_elements, title='Source Status', loc='upper center', framealpha=0.9)\n",
    "            legend_added = True\n",
    "\n",
    "plt.suptitle(\"Citation Status Proportion by Continent: Wikipedia vs Grokipedia\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.savefig(f\"../graphics/citation_composition_by_continent_grid.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28afdb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot embedding similarity distributions by article class (FA, GA, B, C, Start, Stub), split by license status\n",
    "\n",
    "class_order = [\"FA\", \"GA\", \"B\", \"C\", \"Start\", \"Stub\"]\n",
    "pretty_class_labels = {\n",
    "    \"FA\": \"Featured Article\",\n",
    "    \"GA\": \"Good Article\",\n",
    "    \"B\": \"B-Class\",\n",
    "    \"C\": \"C-Class\",\n",
    "    \"Start\": \"Start-Class\",\n",
    "    \"Stub\": \"Stub-Class\",\n",
    "}\n",
    "\n",
    "# Prepare class column: make None/NaN -> \"Unknown\", keep uppercase\n",
    "df_class = df.copy()\n",
    "df_class[\"class\"] = df_class[\"class\"].astype(\"category\")\n",
    "df_class[\"class\"] = df_class[\"class\"].cat.set_categories(class_order, ordered=True)\n",
    "# Prepare lower-case titles for merge\n",
    "df_class[\"title_lc\"] = df_class[\"title\"].str.replace('_', ' ').str.lower()\n",
    "\n",
    "n_classes = len(class_order)\n",
    "fig, axes = plt.subplots(\n",
    "    2, 3, \n",
    "    figsize=(16, 8),\n",
    "    squeeze=False\n",
    ")\n",
    "\n",
    "for idx, class_label in enumerate(class_order):\n",
    "    row = idx // 3\n",
    "    col = idx % 3\n",
    "    ax = axes[row][col]\n",
    "    class_df = df_class[df_class[\"class\"] == class_label]\n",
    "    if class_df.empty:\n",
    "        ax.axis('off')\n",
    "        continue\n",
    "    class_w_license = pd.merge(\n",
    "        grokipedia_w_license_df, class_df, left_on=\"title\", right_on=\"title_lc\"\n",
    "    )\n",
    "    class_wo_license = pd.merge(\n",
    "        grokipedia_wo_license_df, class_df, left_on=\"title\", right_on=\"title_lc\"\n",
    "    )\n",
    "    h = ax.hist(\n",
    "        [class_w_license[\"similarity\"].dropna(), class_wo_license[\"similarity\"].dropna()],\n",
    "        bins=100,\n",
    "        color=[\"tab:blue\", \"tab:orange\"],\n",
    "        label=[\"With License\", \"Without License\"],\n",
    "        alpha=0.7,\n",
    "        histtype=\"stepfilled\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Similarity\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    pretty_label = pretty_class_labels.get(class_label, class_label)\n",
    "    ax.set_title(pretty_label)\n",
    "\n",
    "# Add legend to only the first axis\n",
    "axes[0][0].legend()\n",
    "plt.suptitle(\"Embedding Similarity Distributions by Article Quality Class and License Status\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "plt.savefig(\"../graphics/embedding_similarity_by_class_grid_license_split.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fdc3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
