{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e95d735",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Actually do the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb73a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec23cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = \"../results\"\n",
    "SUPP_DATA_DIR = \"../supplemental_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c63a4c",
   "metadata": {},
   "source": [
    "## Absolute comparison of domain occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/grok_domains.json', 'r') as f:\n",
    "    grok_domains = json.load(f)\n",
    "\n",
    "with open('../results/wp_domains.json', 'r') as f:\n",
    "    wp_domains = json.load(f)\n",
    "\n",
    "total_grok = 0\n",
    "total_wp = 0\n",
    "grok_domains_counter = Counter()\n",
    "wp_domains_counter = Counter()\n",
    "\n",
    "for a in grok_domains:\n",
    "    for d, c in a.items():\n",
    "        for domain, count in c.items():\n",
    "            grok_domains_counter[domain] += count\n",
    "            total_grok += count\n",
    "\n",
    "for a in wp_domains:\n",
    "    for d, c in a.items():\n",
    "        for domain, count in c.items():\n",
    "            wp_domains_counter[domain] += count\n",
    "            total_wp += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8734cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_citation_count_df = pd.DataFrame(list(wp_domains_counter.items()), columns=['domain', 'wiki_count'])\n",
    "wiki_citation_count_df['wiki_total_cites'] = total_wp\n",
    "wiki_citation_count_df['wiki_share'] = wiki_citation_count_df['wiki_count'] / total_wp\n",
    "grok_citation_count_df = pd.DataFrame(list(grok_domains_counter.items()), columns=['domain', 'grok_count'])\n",
    "grok_citation_count_df['grok_total_cites'] = total_grok\n",
    "grok_citation_count_df['grok_share'] = grok_citation_count_df['grok_count'] / total_grok\n",
    "citation_count_df = pd.merge(wiki_citation_count_df, grok_citation_count_df, on='domain', how='outer').fillna(0)\n",
    "citation_count_df['count_diff'] = citation_count_df['grok_count'] - citation_count_df['wiki_count']\n",
    "citation_count_df['share_diff'] = citation_count_df['grok_share'] - citation_count_df['wiki_share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fa743",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_count_df.to_csv('../results/citation_count_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_domains(domain):\n",
    "    if domain in ['bbc.co.uk', 'bbc.com', 'news.bbc.co.uk']:\n",
    "        return 'bbc.com'\n",
    "    if domain in ['twitter.com', 'x.com']:\n",
    "        return 'x/twitter.com'\n",
    "    if domain in ['edition.cnn.com', 'cnn.com']:\n",
    "        return 'cnn.com'\n",
    "    if domain in ['timesofindia.indiatimes.com', 'economictimes.indiatimes.com']:\n",
    "        return 'indiatimes.com'\n",
    "    return domain\n",
    "\n",
    "citation_count_df['combined_domain'] = citation_count_df['domain'].apply(combine_domains)\n",
    "# citation_count_df.sort_values('grok_share', ascending=False)[100:104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_grok_corrected = citation_count_df[['combined_domain', 'grok_share', 'wiki_share']].groupby('combined_domain').sum().sort_values('grok_share', ascending=False)[:100]\n",
    "t100_wiki_corrected = citation_count_df[['combined_domain', 'grok_share', 'wiki_share']].groupby('combined_domain').sum().sort_values('wiki_share', ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_grok_corrected.reset_index(inplace=True)\n",
    "t100_wiki_corrected.reset_index(inplace=True)\n",
    "t100_grok_corrected['domain'] = t100_grok_corrected['combined_domain']\n",
    "t100_wiki_corrected['domain'] = t100_wiki_corrected['combined_domain']\n",
    "t100_grok_corrected[['domain', 'grok_share']].to_csv('../results/t100_grok_corrected.csv', index=False)\n",
    "t100_wiki_corrected[['domain', 'wiki_share']].to_csv('../results/t100_wiki_corrected.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41062f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_grok = pd.read_csv('../results/t100_grok.csv')\n",
    "t100_wiki = pd.read_csv('../results/t100_wiki.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_grok_combined = pd.merge(t100_grok_corrected, t100_grok, left_on='combined_domain', right_on='domain', how='outer')\n",
    "t100_wiki_combined = pd.merge(t100_wiki_corrected, t100_wiki, left_on='combined_domain', right_on='domain', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af4f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b51abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_wiki_combined[~t100_wiki_combined.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c828b",
   "metadata": {},
   "outputs": [],
   "source": [
    ".to_csv('../results/t100_grok_corrected.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fb130",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_count_df.sort_values('grok_share', ascending=False)[100:104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da4c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_count_df.sort_values('wiki_share', ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_count_df.sort_values('share_diff', ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c3378",
   "metadata": {},
   "source": [
    "## (Un)reliable source additions / removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(f\"{RESULT_DIR}/domain_deltas.csv\")\n",
    "# df_wo_license = pd.read_csv(f'{RESULT_DIR}/domain_deltas_wo_license.csv')\n",
    "# df_w_license = pd.read_csv(f'{RESULT_DIR}/domain_deltas_w_license.csv')\n",
    "domain_set = (\n",
    "    set(\n",
    "        pd.read_csv(f'{SUPP_DATA_DIR}/domain_lists/openalex_journal_domains.csv')\n",
    "        .Domain.tolist()\n",
    "    ).union({\n",
    "        'academia.edu',\n",
    "        'arxiv.org',\n",
    "        'cambridge.org',\n",
    "        'ebsco.com',\n",
    "        'journals.uchicago.edu',\n",
    "        'jstor.org',\n",
    "        'mdpi.com',\n",
    "        'ncbi.nlm.nih.gov',\n",
    "        'papers.ssrn.com',\n",
    "        'researchgate.net',\n",
    "        'sciencedirect.com',\n",
    "        'tandfonline.com'\n",
    "    })\n",
    ")\n",
    "# reliability_df = pd.read_csv(f'{SUPP_DATA_DIR}/perennial_sources_enwiki/perennial_sources.csv')\n",
    "reliability_df = pd.read_csv(f'{SUPP_DATA_DIR}/perennial_sources_enwiki/enwiki_perennial_list.csv')\n",
    "reliability_df['domain'] = reliability_df['source']\n",
    "reliability_df = reliability_df[['domain', 'status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_license = pd.merge(df_w_license, reliability_df, on='domain')\n",
    "df_wo_license = pd.merge(df_wo_license, reliability_df, on='domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eec534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content adapted from Wikipedia (with license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Summary of link changes from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "df_w_license[['delta_sum', 'status']].groupby(df_w_license['status']).sum().drop('status', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde60668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content adapted from Wikipedia (with license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Sources deemed: 'generally_reliable' by the enwiki community\")\n",
    "print(\"Measuring: Total links with a domain removed from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "(\n",
    "    df_w_license[\n",
    "        df_w_license['reliability_status'] == 'generally_reliable']\n",
    "        .sort_values('delta_sum', ascending=True)[:25]\n",
    "        [['source_name', 'domain', 'delta_sum']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content adapted from Wikipedia (with license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Sources deemed: 'generally_reliable' by the enwiki community\")\n",
    "print(\"Measuring: Total links with a domain removed from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "(\n",
    "    df_w_license[\n",
    "        df_w_license['reliability_status'] == 'generally_reliable']\n",
    "        .sort_values('delta_sum', ascending=True)[:25]\n",
    "        [['source_name', 'domain', 'delta_sum']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a9cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content NOT adapted from Wikipedia (without license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Summary of link changes from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "df_wo_license[['delta_sum', 'reliability_status']].groupby(df_wo_license['reliability_status']).sum().drop('reliability_status', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19550418",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content NOT adapted from Wikipedia (without license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Sources deemed: 'generally_reliable' by the enwiki community\")\n",
    "print(\"Measuring: Total links with a domain removed from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "(\n",
    "    df_wo_license[\n",
    "        df_wo_license['reliability_status'] == 'generally_reliable']\n",
    "        .sort_values('delta_sum', ascending=True)[:25]\n",
    "        [['source_name', 'domain', 'delta_sum']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b934780",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content NOT adapted from Wikipedia (without license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Sources deemed: 'generally_unreliable' by the enwiki community\")\n",
    "print(\"Measuring: Total links with a domain added from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "(\n",
    "    df_wo_license[\n",
    "        df_wo_license['reliability_status'] == 'generally_unreliable']\n",
    "        .sort_values('delta_sum', ascending=False)[:25]\n",
    "        [['source_name', 'domain', 'delta_sum']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614bd1c",
   "metadata": {},
   "source": [
    "## How much more cited are specific domain types?\n",
    "\n",
    "Specifically looking at academic journal domains (which Grok seems to really like) and .gov and .mil domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a758d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_government_and_mil_domain(domain):\n",
    "    \"\"\"\n",
    "    Filter to domains including .gov, .mil, or .gov.country (e.g., .gov.au, .gov.uk, .gov.in, etc.)\n",
    "    Safely return False if domain is missing or not a string.\n",
    "    \"\"\"\n",
    "    gov_mil_pattern = re.compile(r'\\.gov(\\.|$)|\\.mil(\\.|$)')\n",
    "    if not isinstance(domain, str):\n",
    "        return False\n",
    "    return bool(gov_mil_pattern.search(domain))\n",
    "\n",
    "def is_journal_domain(domain):\n",
    "    return True if domain in domain_set else False\n",
    "\n",
    "gov_mil_domain_df = df[df.domain.apply(is_government_and_mil_domain)]\n",
    "journal_domain_df = df[df.domain.apply(is_journal_domain)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8566ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_academic_sources_net_cites = journal_domain_df.sort_values('delta_sum', ascending=False)[:100].delta_sum.sum()\n",
    "top_100_gov_mil_sources_net_cites = gov_mil_domain_df.sort_values('delta_sum', ascending=False)[:100].delta_sum.sum()\n",
    "academic_gov_mil_domains = set(journal_domain_df.sort_values('delta_sum', ascending=False)[:100].domain.tolist()).union(set(gov_mil_domain_df.sort_values('delta_sum', ascending=False)[:100].domain.tolist()))\n",
    "\n",
    "total_net_cites = df.delta_sum.sum()\n",
    "total_domains = df.domain.count()\n",
    "\n",
    "print(f\"Of a total net increase of {total_net_cites:,} citations:\")\n",
    "print(f\"  - {top_100_academic_sources_net_cites:,} ({top_100_academic_sources_net_cites/total_net_cites:.1%}) are from the top 100 journal domains\")\n",
    "print(f\"  - {top_100_gov_mil_sources_net_cites:,} ({top_100_gov_mil_sources_net_cites/total_net_cites:.1%}) are from the top 100 gov/mil domains\")\n",
    "print(f\"These sources correspond with only {len(academic_gov_mil_domains) / total_domains:.5%} of the cited domains on Grokipedia\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ece7b",
   "metadata": {},
   "source": [
    "## Shifts in article citation composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032278fe",
   "metadata": {},
   "source": [
    "#### Which articles had the largest shifts from enwiki-deemed reliable to unreliable sources in their composition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reliability_shift_maximal(reliability_df, result_dir='../results/overall/domains', fsuffix='_domains.json'):\n",
    "    \"\"\"\n",
    "    Find articles where (WP_reliable - WP_unreliable) - (grok_unreliable - grok_reliable) is maximal.\n",
    "    This identifies pages that went from well-sourced (reliable) in WP to poorly-sourced (unreliable) in Grok.\n",
    "    \n",
    "    Args:\n",
    "        reliability_df: DataFrame with 'domain' and 'status' columns\n",
    "        result_dir: Directory containing wp_domains.json and grok_domains.json\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples with article shift data, sorted by the maximal function\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_article_reliability_counts(json_file, reliability_df):\n",
    "        \"\"\"Load JSON and count citations by reliability status per article.\"\"\"\n",
    "        # Create lookup dict for reliability status by normalized domain\n",
    "        reliability_lookup = {}\n",
    "        for _, row in reliability_df.iterrows():\n",
    "            domain = row['domain']\n",
    "            if domain:\n",
    "                reliability_lookup[domain] = row.get('status', None)\n",
    "        \n",
    "        article_stats = []\n",
    "        \n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for article_dict in data:\n",
    "            for article_title, domains in article_dict.items():\n",
    "                if isinstance(domains, dict):\n",
    "                    rel_count = 0\n",
    "                    unrel_count = 0\n",
    "                    blacklist_count = 0\n",
    "                    no_consensus_count = 0\n",
    "                    deprecated_count = 0\n",
    "                    other_count = 0\n",
    "                    total_count = 0\n",
    "                    \n",
    "                    for domain, count in domains.items():\n",
    "                        total_count += count\n",
    "                        if domain and domain in reliability_lookup:\n",
    "                            status = reliability_lookup[domain]\n",
    "                            if status == 'Generally reliable':\n",
    "                                rel_count += count\n",
    "                            elif status == 'Generally unreliable':\n",
    "                                unrel_count += count\n",
    "                            elif status == 'Deprecated':\n",
    "                                deprecated_count += count\n",
    "                            elif status == 'No consensus':\n",
    "                                no_consensus_count += count\n",
    "                            elif status == 'Blacklisted':\n",
    "                                blacklist_count += count\n",
    "                            else:\n",
    "                                # Domain in lookup but has other status (e.g., deprecated, no consensus)\n",
    "                                other_count += count\n",
    "                        else:\n",
    "                            # Domain not in reliability lookup (unknown/other sources)\n",
    "                            other_count += count\n",
    "                    \n",
    "                    article_stats.append((article_title, rel_count, unrel_count, blacklist_count, no_consensus_count, deprecated_count, other_count, total_count))\n",
    "        \n",
    "        return article_stats\n",
    "    \n",
    "    # Get per-article reliability counts\n",
    "    wp_articles = get_article_reliability_counts(f'{result_dir}/wp{fsuffix}', reliability_df)\n",
    "    grok_articles = get_article_reliability_counts(f'{result_dir}/grok{fsuffix}', reliability_df)\n",
    "    \n",
    "    print(f\"Loaded {len(wp_articles)} articles from Wikipedia\")\n",
    "    print(f\"Loaded {len(grok_articles)} articles from Grokipedia\\n\")\n",
    "    \n",
    "    # Create dictionaries for quick lookup\n",
    "    wp_dict = {art[0]: art for art in wp_articles}\n",
    "    grok_dict = {art[0]: art for art in grok_articles}\n",
    "    \n",
    "    # Get all unique articles\n",
    "    all_articles = set(wp_dict.keys()) | set(grok_dict.keys())\n",
    "    \n",
    "    # Calculate the maximal function for each article\n",
    "    article_maximals = []\n",
    "    for article in all_articles:\n",
    "        wp_data = wp_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        grok_data = grok_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        \n",
    "        _, wp_rel, wp_unrel, wp_blacklist, wp_no_consensus, wp_deprecated, wp_other, wp_total = wp_data\n",
    "        _, grok_rel, grok_unrel, grok_blacklist, grok_no_consensus, grok_deprecated, grok_other, grok_total = grok_data\n",
    "        \n",
    "        # Calculate: (WP_reliable - WP_unreliable) - (grok_unreliable - grok_reliable)\n",
    "        # = WP_reliable - WP_unreliable - grok_unreliable + grok_reliable\n",
    "        maximal_value = (wp_rel - wp_unrel) - (grok_unrel - grok_rel)\n",
    "        \n",
    "        article_maximals.append((\n",
    "            article,\n",
    "            wp_rel, wp_unrel, wp_blacklist, wp_no_consensus, wp_deprecated, wp_other, wp_total,\n",
    "            grok_rel, grok_unrel, grok_blacklist, grok_no_consensus, grok_deprecated, grok_other, grok_total,\n",
    "            maximal_value\n",
    "        ))\n",
    "    \n",
    "    return article_maximals\n",
    "\n",
    "columns = [\n",
    "        'title', 'wp_reliable', 'wp_unreliable', 'wp_blacklist', 'wp_no_consensus', 'wp_deprecated', 'wp_other',\n",
    "        'wp_total', 'grok_reliable', 'grok_unreliable', 'grok_blacklist', 'grok_no_consensus', 'grok_deprecated',\n",
    "        'grok_other', 'grok_total', 'maximal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_maximals = find_reliability_shift_maximal(reliability_df)\n",
    "max_reliability_shift_df = pd.DataFrame(article_maximals, columns=columns)\n",
    "article_maximals_w_license = find_reliability_shift_maximal(reliability_df, fsuffix='_domains_w_license.json')\n",
    "max_reliability_shift_df_w_license = pd.DataFrame(article_maximals_w_license, columns=columns)\n",
    "article_maximals_wo_license = find_reliability_shift_maximal(reliability_df, fsuffix='_domains_wo_license.json')\n",
    "max_reliability_shift_df_wo_license = pd.DataFrame(article_maximals_wo_license, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reliability_shift_df.to_csv('../results/reliability_citation_diff.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_charts(filtered_df, fsuffix='', show=True, title=None):\n",
    "    \"\"\"\n",
    "    Plots stacked bar + diagonal comparison charts using the given DataFrame:\n",
    "    1. Stacked bar and overlay: Proportion of sources in each reliability category for Wikipedia and Grokipedia, with diagonal fills illustrating change.\n",
    "    2. Bar chart: Percentage of articles containing at least one source of each type (not 'other') for Wikipedia and Grokipedia.\n",
    "\n",
    "    Parameters:\n",
    "        filtered_df (pd.DataFrame): DataFrame, typically filtered on articles of interest.\n",
    "        show (bool): If True, calls plt.show() at end.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    # --- Setup categories, labels, colors ---\n",
    "    column_order = [\n",
    "        'reliable', 'unreliable', 'blacklist', 'no_consensus', 'deprecated', 'other'\n",
    "    ]\n",
    "    display_names = {\n",
    "        'reliable': 'Generally reliable',\n",
    "        'unreliable': 'Generally unreliable',\n",
    "        'blacklist': 'Blacklisted',\n",
    "        'no_consensus': 'No consensus',\n",
    "        'deprecated': 'Deprecated',\n",
    "        'other': 'Other'\n",
    "    }\n",
    "    color_map = {\n",
    "        'reliable': 'green',\n",
    "        'unreliable': 'red',\n",
    "        'blacklist': 'black',\n",
    "        'no_consensus': 'yellow',\n",
    "        'deprecated': 'orange',\n",
    "        'other': 'grey'\n",
    "    }\n",
    "\n",
    "    # --- Aggregate counts as \"wp\" and \"grok\" \"status\" table ---\n",
    "    agg = {\n",
    "        'wp_reliable': filtered_df['wp_reliable'].sum(),\n",
    "        'wp_unreliable': filtered_df['wp_unreliable'].sum(),\n",
    "        'wp_blacklist': filtered_df['wp_blacklist'].sum(),\n",
    "        'wp_no_consensus': filtered_df['wp_no_consensus'].sum(),\n",
    "        'wp_deprecated': filtered_df['wp_deprecated'].sum(),\n",
    "        'wp_other': filtered_df['wp_other'].sum(),\n",
    "        'grok_reliable': filtered_df['grok_reliable'].sum(),\n",
    "        'grok_unreliable': filtered_df['grok_unreliable'].sum(),\n",
    "        'grok_blacklist': filtered_df['grok_blacklist'].sum(),\n",
    "        'grok_no_consensus': filtered_df['grok_no_consensus'].sum(),\n",
    "        'grok_deprecated': filtered_df['grok_deprecated'].sum(),\n",
    "        'grok_other': filtered_df['grok_other'].sum(),\n",
    "    }\n",
    "\n",
    "    # Print the number of blacklisted sources\n",
    "    print(f\"Number of blacklisted sources (Wikipedia): {agg['wp_blacklist']}\")\n",
    "    print(f\"Number of blacklisted sources (Grokipedia): {agg['grok_blacklist']}\")\n",
    "    \n",
    "    # Print the number of pages/articles with at least one blacklisted source\n",
    "    wp_pages_with_blacklist = (filtered_df['wp_blacklist'] > 0).sum()\n",
    "    grok_pages_with_blacklist = (filtered_df['grok_blacklist'] > 0).sum()\n",
    "    print(f\"Number of pages with at least one blacklisted source (Wikipedia): {wp_pages_with_blacklist}\")\n",
    "    print(f\"Number of pages with at least one blacklisted source (Grokipedia): {grok_pages_with_blacklist}\")\n",
    "\n",
    "    # Make DF of shape: index=['Wikipedia', 'Grokipedia'], columns=column_order\n",
    "    wp_row = [agg[f'wp_{k}'] for k in column_order]\n",
    "    grok_row = [agg[f'grok_{k}'] for k in column_order]\n",
    "    prop_df = pd.DataFrame(\n",
    "        [wp_row, grok_row],\n",
    "        columns=column_order,\n",
    "        index=['Wikipedia', 'Grokipedia']\n",
    "    )\n",
    "    prop_df_norm = prop_df.div(prop_df.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    # --- Plotting stacked bars with diagonal overlays, make first plot narrower and better aligned ---\n",
    "    labels = ['Wikipedia', 'Grokipedia']\n",
    "    x = np.arange(len(labels))\n",
    "    bar_sep = 0.09  # reduce gap between bars\n",
    "    width = 0.18    # make bars narrower\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 7), gridspec_kw={'width_ratios': [1, 1.7]})\n",
    "    \n",
    "    # Set figure title (for whole figure)\n",
    "    plot_title = title if title is not None else \"Source Reliability Category Proportion: Wikipedia vs Grokipedia\"\n",
    "    fig.suptitle(plot_title, fontsize=18, y=0.98)\n",
    "    ax = axs[0]\n",
    "\n",
    "    # Set up stacking\n",
    "    bottoms = [0, 0]\n",
    "    bars_wp = []\n",
    "    bars_grok = []\n",
    "\n",
    "    # For synchronized stacking, process in column order:\n",
    "    print(\"Wikipedia proportions:\")\n",
    "    print(prop_df_norm.loc['Wikipedia'])\n",
    "    print(\"\\nGrokipedia proportions:\")\n",
    "    print(prop_df_norm.loc['Grokipedia'])\n",
    "    for j, col in enumerate(column_order):\n",
    "        color = color_map.get(col, 'grey')\n",
    "        # WP bar proportions\n",
    "        wp_prop = prop_df_norm.loc['Wikipedia', col]\n",
    "        grok_prop = prop_df_norm.loc['Grokipedia', col]\n",
    "        bar_wp = ax.bar(x[0] - width/2 - bar_sep/2, wp_prop, width=width,\n",
    "                        bottom=bottoms[0], color=color, edgecolor='none', zorder=2, alpha=0.8)\n",
    "        bar_grok = ax.bar(x[1] + width/2 + bar_sep/2, grok_prop, width=width,\n",
    "                          bottom=bottoms[1], color=color, edgecolor='none', zorder=2, alpha=0.8)\n",
    "\n",
    "        # Diagonal change fill\n",
    "        wp_top = bottoms[0] + wp_prop\n",
    "        grok_top = bottoms[1] + grok_prop\n",
    "        ax.fill_between(\n",
    "            [x[0] - width/2, x[1] + width/2],\n",
    "            [wp_top, grok_top],\n",
    "            [bottoms[0], bottoms[1]],\n",
    "            color=color, alpha=0.25, zorder=1, linewidth=0\n",
    "        )\n",
    "        bars_wp.append(bar_wp)\n",
    "        bars_grok.append(bar_grok)\n",
    "        bottoms[0] += wp_prop\n",
    "        bottoms[1] += grok_prop\n",
    "\n",
    "    # Set axis ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, fontsize=16)\n",
    "    ax.set_ylabel(\"Proportion of Citations\", fontsize=16)\n",
    "    ax.set_title(\"Source Status Proportion: Wikipedia vs Grokipedia\", fontsize=16)\n",
    "\n",
    "    # Make axis tight with bars, remove excess white space\n",
    "    # Bars are at x[0]=0 and x[1]=1, with width=0.18 and bar_sep=0.09\n",
    "    # Left bar spans: -0.18 to 0, right bar spans: 1 to 1.18\n",
    "    # Add small padding: 0.05 on each side\n",
    "    ax.set_xlim(-0.23, 1.23)\n",
    "    ax.set_ylim(bottom=0, top=1.01)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    # Custom legend patch (color by status) - move inside plot to reduce whitespace\n",
    "    legend_elements = [Patch(facecolor=color_map.get(col, 'grey'), label=display_names.get(col, col), alpha=0.8) for col in column_order]\n",
    "    ax.legend(handles=legend_elements, title='Source Status', loc='upper center', framealpha=0.9)\n",
    "\n",
    "    # Tighten subplot spacing to reduce whitespace\n",
    "    fig.subplots_adjust(wspace=0.15, left=0.05, right=0.97, top=0.92, bottom=0.1)\n",
    "\n",
    "    # ---- New plot: % of articles containing at least 1 in each source type (not 'other') ----\n",
    "\n",
    "    ax2 = axs[1]\n",
    "    main_types = ['reliable', 'unreliable', 'blacklist', 'no_consensus', 'deprecated']\n",
    "    type_labels = [display_names[t] for t in main_types]\n",
    "    bar_x = np.arange(len(main_types))\n",
    "    bar_width = 0.36\n",
    "\n",
    "    n_articles = len(filtered_df)\n",
    "    percentages = {'Wikipedia': [], 'Grokipedia': []}\n",
    "    for source_type in main_types:\n",
    "        wp_col = f'wp_{source_type}'\n",
    "        grok_col = f'grok_{source_type}'\n",
    "        wp_count = (filtered_df[wp_col] > 0).sum()\n",
    "        grok_count = (filtered_df[grok_col] > 0).sum()\n",
    "        percentages['Wikipedia'].append(wp_count / n_articles * 100)\n",
    "        percentages['Grokipedia'].append(grok_count / n_articles * 100)\n",
    "\n",
    "    # Plot as side-by-side bars\n",
    "    ax2.bar(bar_x - bar_width/2, percentages['Wikipedia'], bar_width,\n",
    "           label='Wikipedia', color=\"#4977bc\", edgecolor='black', alpha=0.7)\n",
    "    ax2.bar(bar_x + bar_width/2, percentages['Grokipedia'], bar_width,\n",
    "           label='Grokipedia', color=\"#e86b54\", edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for i, (wp, gk) in enumerate(zip(percentages['Wikipedia'], percentages['Grokipedia'])):\n",
    "        ax2.text(i - bar_width/2, wp + 1, f\"{wp:.1f}%\", ha='center', va='bottom', fontsize=14, color=\"#10426b\")\n",
    "        ax2.text(i + bar_width/2, gk + 1, f\"{gk:.1f}%\", ha='center', va='bottom', fontsize=14, color=\"#7a230c\")\n",
    "\n",
    "    ax2.set_xticks(bar_x)\n",
    "    ax2.set_xticklabels(type_labels, rotation=14, fontsize=14)\n",
    "    ax2.set_ylim(0, 105)\n",
    "    ax2.set_ylabel(\"Percent of Articles\", fontsize=16)\n",
    "    ax2.set_title(\"% of Articles Citing Any Source in Category\", fontsize=16)\n",
    "    ax2.legend(loc='upper right', fontsize=14)\n",
    "    ax2.grid(axis='y', linestyle=':', alpha=0.4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'../graphics/overall_grok_wp_cite_composition_{fsuffix}.pdf')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8717be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_charts(max_reliability_shift_df, title='Source Reliability Category Comparison (all articles)')\n",
    "plot_reliability_charts(max_reliability_shift_df_w_license, fsuffix='_w_license', title='Source Reliability Category Comparison (all articles with CC-license)')\n",
    "plot_reliability_charts(max_reliability_shift_df_wo_license, fsuffix='_wo_license', title='Source Reliability Category Comparison (all articles without CC-license)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e782c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reliability_shift_df.to_csv(f'{RESULT_DIR}/reliability_citation_diff.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee00cc",
   "metadata": {},
   "source": [
    "### Using Lin et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82212332",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reliability = pd.read_csv(f'../supplemental_data/news_reliability/LinRating_Join.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c31b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reliability['reliability_score'] = lin_reliability['pc1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bdc9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reliability_shift_by_buckets(lin_reliability, result_dir='../results/overall/domains', fsuffix='_domains.json'):\n",
    "    \"\"\"\n",
    "    Find articles where reliability shifts occur, grouped by reliability score buckets (0.2-sized buckets).\n",
    "    Uses lin_reliability DataFrame with reliability_score instead of discrete status categories.\n",
    "    \n",
    "    Args:\n",
    "        lin_reliability: DataFrame with 'domain' and 'reliability_score' columns\n",
    "        result_dir: Directory containing wp_domains.json and grok_domains.json\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples with article shift data grouped by reliability score buckets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define buckets: 0-0.2, 0.2-0.4, 0.4-0.6, 0.6-0.8, 0.8-1.0\n",
    "    bucket_size = 0.2\n",
    "    buckets = [(i * bucket_size, (i + 1) * bucket_size) for i in range(5)]\n",
    "    bucket_labels = [f\"{i * bucket_size:.1f}-{(i + 1) * bucket_size:.1f}\" for i in range(5)]\n",
    "    \n",
    "    def get_bucket(score):\n",
    "        \"\"\"Assign a reliability score to a bucket.\"\"\"\n",
    "        if pd.isna(score):\n",
    "            return None\n",
    "        for i, (low, high) in enumerate(buckets):\n",
    "            if low <= score < high:\n",
    "                return i\n",
    "        # Handle edge case: score == 1.0\n",
    "        if score == 1.0:\n",
    "            return 4\n",
    "        return None\n",
    "    \n",
    "    def get_article_reliability_bucket_counts(json_file, lin_reliability):\n",
    "        \"\"\"Load JSON and count citations by reliability score bucket per article.\"\"\"\n",
    "        # Create lookup dict for reliability score by domain\n",
    "        # Handle both 'reliability_score' and 'pc1' column names\n",
    "        score_col = 'reliability_score' if 'reliability_score' in lin_reliability.columns else 'pc1'\n",
    "        reliability_lookup = {}\n",
    "        for _, row in lin_reliability.iterrows():\n",
    "            domain = row['domain']\n",
    "            if domain and pd.notna(row.get(score_col)):\n",
    "                reliability_lookup[domain] = row[score_col]\n",
    "        \n",
    "        article_stats = []\n",
    "        \n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for article_dict in data:\n",
    "            for article_title, domains in article_dict.items():\n",
    "                if isinstance(domains, dict):\n",
    "                    # Initialize counts for each bucket\n",
    "                    bucket_counts = {i: 0 for i in range(5)}\n",
    "                    other_count = 0\n",
    "                    total_count = 0\n",
    "                    \n",
    "                    for domain, count in domains.items():\n",
    "                        total_count += count\n",
    "                        if domain and domain in reliability_lookup:\n",
    "                            score = reliability_lookup[domain]\n",
    "                            bucket_idx = get_bucket(score)\n",
    "                            if bucket_idx is not None:\n",
    "                                bucket_counts[bucket_idx] += count\n",
    "                            else:\n",
    "                                other_count += count\n",
    "                        else:\n",
    "                            # Domain not in reliability lookup (unknown/other sources)\n",
    "                            other_count += count\n",
    "                    \n",
    "                    # Return tuple: (title, bucket_0, bucket_1, bucket_2, bucket_3, bucket_4, other, total)\n",
    "                    article_stats.append((\n",
    "                        article_title,\n",
    "                        bucket_counts[0], bucket_counts[1], bucket_counts[2], \n",
    "                        bucket_counts[3], bucket_counts[4], other_count, total_count\n",
    "                    ))\n",
    "        \n",
    "        return article_stats\n",
    "    \n",
    "    # Get per-article reliability bucket counts\n",
    "    wp_articles = get_article_reliability_bucket_counts(f'{result_dir}/wp{fsuffix}', lin_reliability)\n",
    "    grok_articles = get_article_reliability_bucket_counts(f'{result_dir}/grok{fsuffix}', lin_reliability)\n",
    "    \n",
    "    print(f\"Loaded {len(wp_articles)} articles from Wikipedia\")\n",
    "    print(f\"Loaded {len(grok_articles)} articles from Grokipedia\\n\")\n",
    "    \n",
    "    # Create dictionaries for quick lookup\n",
    "    wp_dict = {art[0]: art for art in wp_articles}\n",
    "    grok_dict = {art[0]: art for art in grok_articles}\n",
    "    \n",
    "    # Get all unique articles\n",
    "    all_articles = set(wp_dict.keys()) | set(grok_dict.keys())\n",
    "    \n",
    "    # Calculate shifts for each article, grouped by buckets\n",
    "    article_bucket_shifts = []\n",
    "    for article in all_articles:\n",
    "        wp_data = wp_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        grok_data = grok_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        \n",
    "        _, wp_b0, wp_b1, wp_b2, wp_b3, wp_b4, wp_other, wp_total = wp_data\n",
    "        _, grok_b0, grok_b1, grok_b2, grok_b3, grok_b4, grok_other, grok_total = grok_data\n",
    "        \n",
    "        # Calculate shift for each bucket: WP_count - Grok_count\n",
    "        # Positive means WP has more, negative means Grok has more\n",
    "        shifts = [\n",
    "            wp_b0 - grok_b0,\n",
    "            wp_b1 - grok_b1,\n",
    "            wp_b2 - grok_b2,\n",
    "            wp_b3 - grok_b3,\n",
    "            wp_b4 - grok_b4\n",
    "        ]\n",
    "        \n",
    "        # Calculate overall shift metric: weighted sum (higher buckets weighted more)\n",
    "        # This gives more weight to shifts in higher reliability buckets\n",
    "        weighted_shift = sum(shifts[i] * (i + 1) for i in range(5))\n",
    "        \n",
    "        article_bucket_shifts.append((\n",
    "            article,\n",
    "            wp_b0, wp_b1, wp_b2, wp_b3, wp_b4, wp_other, wp_total,\n",
    "            grok_b0, grok_b1, grok_b2, grok_b3, grok_b4, grok_other, grok_total,\n",
    "            shifts[0], shifts[1], shifts[2], shifts[3], shifts[4],  # individual bucket shifts\n",
    "            weighted_shift  # overall weighted shift\n",
    "        ))\n",
    "    \n",
    "    return article_bucket_shifts\n",
    "\n",
    "bucket_columns = [\n",
    "    'title',\n",
    "    'wp_bucket_0_0.2', 'wp_bucket_0.2_0.4', 'wp_bucket_0.4_0.6', 'wp_bucket_0.6_0.8', 'wp_bucket_0.8_1.0', 'wp_other', 'wp_total',\n",
    "    'grok_bucket_0_0.2', 'grok_bucket_0.2_0.4', 'grok_bucket_0.4_0.6', 'grok_bucket_0.6_0.8', 'grok_bucket_0.8_1.0', 'grok_other', 'grok_total',\n",
    "    'shift_bucket_0_0.2', 'shift_bucket_0.2_0.4', 'shift_bucket_0.4_0.6', 'shift_bucket_0.6_0.8', 'shift_bucket_0.8_1.0',\n",
    "    'weighted_shift'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_maximals_lin = find_reliability_shift_by_buckets(lin_reliability)\n",
    "max_reliability_shift_df_lin = pd.DataFrame(article_maximals_lin, columns=bucket_columns)\n",
    "article_maximals_w_license_lin = find_reliability_shift_by_buckets(lin_reliability, fsuffix='_domains_w_license.json')\n",
    "max_reliability_shift_df_w_license_lin = pd.DataFrame(article_maximals_w_license_lin, columns=bucket_columns)\n",
    "article_maximals_wo_license_lin = find_reliability_shift_by_buckets(lin_reliability, fsuffix='_domains_wo_license.json')\n",
    "max_reliability_shift_df_wo_license_lin = pd.DataFrame(article_maximals_wo_license_lin, columns=bucket_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ce934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_bucket_charts(filtered_df, fsuffix='', show=True, title=None):\n",
    "    \"\"\"\n",
    "    Plots stacked bar + diagonal comparison charts using reliability score buckets:\n",
    "    1. Stacked bar and overlay: Proportion of sources in each reliability bucket for Wikipedia and Grokipedia, with diagonal fills illustrating change.\n",
    "    2. Bar chart: Percentage of articles containing at least one source in each bucket for Wikipedia and Grokipedia.\n",
    "\n",
    "    Parameters:\n",
    "        filtered_df (pd.DataFrame): DataFrame with bucket columns (from find_reliability_shift_by_buckets)\n",
    "        fsuffix (str): Suffix for output filename\n",
    "        show (bool): If True, calls plt.show() at end.\n",
    "        title (str): Optional custom title for the whole figure. If None, uses default title.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.patches import Patch\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    # --- Setup buckets, labels, colors ---\n",
    "    bucket_labels = ['0.0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0', 'other']\n",
    "    display_names = {\n",
    "        '0.0-0.2': '0.0-0.2',\n",
    "        '0.2-0.4': '0.2-0.4',\n",
    "        '0.4-0.6': '0.4-0.6',\n",
    "        '0.6-0.8': '0.6-0.8',\n",
    "        '0.8-1.0': '0.8-1.0',\n",
    "        'other': 'No score'\n",
    "    }\n",
    "    \n",
    "    # Create color map: green (for 1.0), yellow (for 0.5), red (for 0.0), gray for 'other'\n",
    "    from matplotlib.colors import to_hex, LinearSegmentedColormap\n",
    "    \n",
    "    # Create a green-yellow-red colormap, where 1.0 is green, 0.5 is yellow, 0.0 is red\n",
    "    spect_cmap = LinearSegmentedColormap.from_list(\n",
    "        \"green_yellow_red\", [(0.0, \"#D73027\"), (0.5, \"#FEE08B\"), (1.0, \"#1A9850\")]  # red, yellow, green\n",
    "    )\n",
    "    \n",
    "    # Map bucket labels to their midpoint values for colormap\n",
    "    bucket_midpoints = {\n",
    "        '0.0-0.2': 0.1,  # Red end\n",
    "        '0.2-0.4': 0.3,  # Red-yellow transition\n",
    "        '0.4-0.6': 0.5,  # Yellow (middle)\n",
    "        '0.6-0.8': 0.7,  # Yellow-green transition\n",
    "        '0.8-1.0': 0.9,  # Green end\n",
    "    }\n",
    "    \n",
    "    # Generate colors for each bucket using the colormap\n",
    "    color_map = {}\n",
    "    for bucket_label in bucket_labels:\n",
    "        if bucket_label == 'other':\n",
    "            color_map[bucket_label] = 'grey'\n",
    "        else:\n",
    "            midpoint = bucket_midpoints[bucket_label]\n",
    "            color_map[bucket_label] = to_hex(spect_cmap(midpoint))\n",
    "\n",
    "    # --- Aggregate counts as \"wp\" and \"grok\" bucket table ---\n",
    "    # Map bucket labels to actual column name suffixes\n",
    "    bucket_to_col = {\n",
    "        '0.0-0.2': '0_0.2',\n",
    "        '0.2-0.4': '0.2_0.4',\n",
    "        '0.4-0.6': '0.4_0.6',\n",
    "        '0.6-0.8': '0.6_0.8',\n",
    "        '0.8-1.0': '0.8_1.0'\n",
    "    }\n",
    "    \n",
    "    agg = {}\n",
    "    for bucket_label in bucket_labels:\n",
    "        if bucket_label == 'other':\n",
    "            agg[f'wp_{bucket_label}'] = filtered_df['wp_other'].sum()\n",
    "            agg[f'grok_{bucket_label}'] = filtered_df['grok_other'].sum()\n",
    "        else:\n",
    "            # Use the correct column name format\n",
    "            col_suffix = bucket_to_col[bucket_label]\n",
    "            agg[f'wp_{bucket_label}'] = filtered_df[f'wp_bucket_{col_suffix}'].sum()\n",
    "            agg[f'grok_{bucket_label}'] = filtered_df[f'grok_bucket_{col_suffix}'].sum()\n",
    "\n",
    "    # Make DF of shape: index=['Wikipedia', 'Grokipedia'], columns=bucket_labels\n",
    "    wp_row = [agg[f'wp_{k}'] for k in bucket_labels]\n",
    "    grok_row = [agg[f'grok_{k}'] for k in bucket_labels]\n",
    "    prop_df = pd.DataFrame(\n",
    "        [wp_row, grok_row],\n",
    "        columns=bucket_labels,\n",
    "        index=['Wikipedia', 'Grokipedia']\n",
    "    )\n",
    "    prop_df_norm = prop_df.div(prop_df.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    # --- Plotting stacked bars with diagonal overlays ---\n",
    "    labels = ['Wikipedia', 'Grokipedia']\n",
    "    x = np.arange(len(labels))\n",
    "    bar_sep = 0.09\n",
    "    width = 0.18\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 7), gridspec_kw={'width_ratios': [1, 1.7]})\n",
    "    ax = axs[0]\n",
    "\n",
    "    # Set up stacking\n",
    "    bottoms = [0, 0]\n",
    "    bars_wp = []\n",
    "    bars_grok = []\n",
    "\n",
    "    # For synchronized stacking, process in bucket order:\n",
    "    for j, bucket_label in enumerate(bucket_labels):\n",
    "        color = color_map.get(bucket_label, 'grey')\n",
    "        # WP bar proportions\n",
    "        wp_prop = prop_df_norm.loc['Wikipedia', bucket_label]\n",
    "        grok_prop = prop_df_norm.loc['Grokipedia', bucket_label]\n",
    "        bar_wp = ax.bar(x[0] - width/2 - bar_sep/2, wp_prop, width=width,\n",
    "                        bottom=bottoms[0], color=color, edgecolor='none', zorder=2)\n",
    "        bar_grok = ax.bar(x[1] + width/2 + bar_sep/2, grok_prop, width=width,\n",
    "                          bottom=bottoms[1], color=color, edgecolor='none', zorder=2)\n",
    "\n",
    "        # Diagonal change fill\n",
    "        wp_top = bottoms[0] + wp_prop\n",
    "        grok_top = bottoms[1] + grok_prop\n",
    "        ax.fill_between(\n",
    "            [x[0] - width/2, x[1] + width/2],\n",
    "            [wp_top, grok_top],\n",
    "            [bottoms[0], bottoms[1]],\n",
    "            color=color, alpha=0.25, zorder=1, linewidth=0\n",
    "        )\n",
    "        bars_wp.append(bar_wp)\n",
    "        bars_grok.append(bar_grok)\n",
    "        bottoms[0] += wp_prop\n",
    "        bottoms[1] += grok_prop\n",
    "\n",
    "    # Set axis ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, fontsize=16)\n",
    "    ax.set_ylabel(\"Proportion of Citations\", fontsize=16)\n",
    "    ax.set_title(\"Source Reliability Score Proportion: Wikipedia vs Grokipedia\", fontsize=18)\n",
    "\n",
    "    # Make axis tight with bars\n",
    "    ax.set_xlim(-0.23, 1.23)\n",
    "    ax.set_ylim(bottom=0, top=1.01)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    # Custom legend patch (color by bucket) - reversed order - move inside plot to reduce whitespace\n",
    "    legend_elements = [Patch(facecolor=color_map.get(bucket_label, 'grey'), label=display_names.get(bucket_label, bucket_label), alpha=0.55) for bucket_label in reversed(bucket_labels)]\n",
    "    ax.legend(handles=legend_elements, title='Reliability Score', loc='upper center', framealpha=0.9, fontsize=16)\n",
    "\n",
    "    # Set figure title (for whole figure)\n",
    "    plot_title = title if title is not None else \"Source Reliability Score Proportion: Wikipedia vs Grokipedia\"\n",
    "    fig.suptitle(plot_title, fontsize=16, y=0.98)\n",
    "    \n",
    "    # Tighten subplot spacing to reduce whitespace (leave room for suptitle)\n",
    "    fig.subplots_adjust(wspace=0.15, left=0.05, right=0.97, top=0.88, bottom=0.1)\n",
    "\n",
    "    # ---- New plot: % of articles containing at least 1 in each bucket ----\n",
    "    ax2 = axs[1]\n",
    "    main_buckets = ['0.0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
    "    bucket_display_labels = [display_names[b] for b in main_buckets]\n",
    "    bar_x = np.arange(len(main_buckets))\n",
    "    bar_width = 0.36\n",
    "\n",
    "    n_articles = len(filtered_df)\n",
    "    percentages = {'Wikipedia': [], 'Grokipedia': []}\n",
    "    # Print the number of articles containing at least one 0-0.2-reliability citation for Wikipedia and Grokipedia\n",
    "    wp_0_0_2_col = 'wp_bucket_0_0.2'\n",
    "    grok_0_0_2_col = 'grok_bucket_0_0.2'\n",
    "    wp_0_0_2_num = (filtered_df[wp_0_0_2_col] > 0).sum()\n",
    "    grok_0_0_2_num = (filtered_df[grok_0_0_2_col] > 0).sum()\n",
    "    print(f\"Number of articles with at least one 0-0.2 source: Wikipedia: {wp_0_0_2_num}, Grokipedia: {grok_0_0_2_num}\")\n",
    "\n",
    "    # Print the number of pages (rows) with a 0-0.2 source for Wikipedia and for Grokipedia\n",
    "    print(f\"Number of pages with a 0-0.2 source in Wikipedia: {wp_0_0_2_num}\")\n",
    "    print(f\"Number of pages with a 0-0.2 source in Grokipedia: {grok_0_0_2_num}\")\n",
    "\n",
    "    for bucket_label in main_buckets:\n",
    "        # Use the correct column name format\n",
    "        col_suffix = bucket_to_col[bucket_label]\n",
    "        wp_col = f'wp_bucket_{col_suffix}'\n",
    "        grok_col = f'grok_bucket_{col_suffix}'\n",
    "        wp_count = (filtered_df[wp_col] > 0).sum()\n",
    "        grok_count = (filtered_df[grok_col] > 0).sum()\n",
    "        percentages['Wikipedia'].append(wp_count / n_articles * 100)\n",
    "        percentages['Grokipedia'].append(grok_count / n_articles * 100)\n",
    "\n",
    "    # Plot as side-by-side bars with blue/red scheme (same as reliability chart)\n",
    "    wp_color = \"#4977bc\"  # Blue for Wikipedia\n",
    "    grok_color = \"#e86b54\"  # Red for Grokipedia\n",
    "    ax2.bar(bar_x - bar_width/2, percentages['Wikipedia'], bar_width,\n",
    "           label='Wikipedia', color=wp_color, edgecolor='black', alpha=0.7)\n",
    "    ax2.bar(bar_x + bar_width/2, percentages['Grokipedia'], bar_width,\n",
    "           label='Grokipedia', color=grok_color, edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for i, (wp, gk) in enumerate(zip(percentages['Wikipedia'], percentages['Grokipedia'])):\n",
    "        ax2.text(i - bar_width/2, wp + 1, f\"{wp:.1f}%\", ha='center', va='bottom', fontsize=10, color=\"#10426b\")\n",
    "        ax2.text(i + bar_width/2, gk + 1, f\"{gk:.1f}%\", ha='center', va='bottom', fontsize=10, color=\"#7a230c\")\n",
    "\n",
    "    ax2.set_xticks(bar_x)\n",
    "    ax2.set_xticklabels(bucket_display_labels, rotation=14, fontsize=16)\n",
    "    ax2.set_ylim(0, 105)\n",
    "    ax2.set_ylabel(\"Percent of Articles\", fontsize=16)\n",
    "    ax2.set_title(\"% of Articles Citing Any Source in Reliability Bucket\", fontsize=18)\n",
    "    ax2.legend(loc='upper right', fontsize=16)\n",
    "    ax2.grid(axis='y', linestyle=':', alpha=0.4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'../graphics/overall_grok_wp_cite_composition_lin_{fsuffix}.pdf')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b879c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_bucket_charts(max_reliability_shift_df_lin, title='Source Reliability Score Comparison (all articles)')\n",
    "plot_reliability_bucket_charts(max_reliability_shift_df_w_license_lin, fsuffix='_w_license', title='Source Reliability Score Comparison (all articles with CC-license)')\n",
    "plot_reliability_bucket_charts(max_reliability_shift_df_wo_license_lin, fsuffix='_wo_license', title='Source Reliability Score Comparison (all articles without CC-license)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0298f40",
   "metadata": {},
   "source": [
    "## Examining Book Citation Patterns in Grokipedia Articles\n",
    "\n",
    "Finding articles with book citations and examining their reference structure.\n",
    "\n",
    "\n",
    "(tktk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87adc6",
   "metadata": {},
   "source": [
    "## Getting Twitter / X usernames that were cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_object_at_offset(path, offset):\n",
    "    if offset is None:\n",
    "        raise ValueError(\"Offset must not be None. Check the index lookup.\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        f.seek(offset)\n",
    "        line = f.readline()\n",
    "        return json.loads(line)\n",
    "\n",
    "# Patterns as before\n",
    "SITE_PATTERNS = {\n",
    "    \"twitter.com\": re.compile(r\"https?://(?:www\\.)?(?:twitter\\.com|x\\.com)/([A-Za-z0-9_]{1,15})(?:[/?#]|$)\"),\n",
    "    \"x.com\": re.compile(r\"https?://(?:www\\.)?x\\.com/([A-Za-z0-9_]{1,15})(?:[/?#]|$)\"),\n",
    "    \"facebook.com\": re.compile(r\"https?://(?:www\\.)?facebook\\.com/(?!(pages|groups|events|marketplace|watch|gaming|live|photo\\.php)(?:/|$))([A-Za-z0-9_.]{5,})(?:[/?#]|$)\"),\n",
    "    \"youtube.com\": re.compile(\n",
    "        r\"https?://(?:www\\.)?youtube\\.com/(?:(?:user|c|channel)/([A-Za-z0-9._\\-]{1,64}))(?:[/?#]|$)\"\n",
    "    ),\n",
    "    \"youtu.be\": re.compile(r\"https?://youtu\\.be/([A-Za-z0-9_\\-]{6,})(?:[/?#]|$)\"),\n",
    "    \"reddit.com\": re.compile(r\"https?://(?:www\\.)?reddit\\.com/user/([A-Za-z0-9_\\-]{1,20})(?:[/?#]|$)\"),\n",
    "    \"instagram.com\": re.compile(r\"https?://(?:www\\.)?instagram\\.com/([A-Za-z0-9_.]{1,30})(?:[/?#]|$)\"),\n",
    "    \"tiktok.com\": re.compile(r\"https?://(?:www\\.)?tiktok\\.com/@([A-Za-z0-9_.]{1,30})(?:[/?#]|$)\"),\n",
    "    \"truthsocial.com\": re.compile(r\"https?://(?:www\\.)?truthsocial\\.com/@([A-Za-z0-9_.]{1,30})(?:[/?#]|$)\"),\n",
    "    \"bsky.app\": re.compile(r\"https?://(?:www\\.)?bsky\\.app/profile/([A-Za-z0-9_.\\-]+)(?:[/?#]|$)\"),\n",
    "    \"pinterest.com\": re.compile(r\"https?://(?:www\\.)?pinterest\\.com/([A-Za-z0-9_/.\\-]{3,})/?(?:[/?#]|$)\"),\n",
    "}\n",
    "ALL_PATTERNS = [(name, pat) for name, pat in SITE_PATTERNS.items()]\n",
    "\n",
    "# Usernames known to be spurious, e.g. from X/Twitter link shortener\n",
    "SPURIOUS_X_USERNAMES = {\n",
    "    \"PRODU\", 'search'\n",
    "}\n",
    "\n",
    "# Values considered NOT real usernames -- skip these by site (case-sensitive)\n",
    "SKIP_FACEBOOK_USERNAMES = set([\n",
    "    \"photo.php\", 'story.php', 'photo', 'media', 'notes', 'business', 'permalink.php', 'search'\n",
    "])\n",
    "# Only numbers (facebook.com/123...), very likely not a real username\n",
    "FACEBOOK_NUMERIC_RE = re.compile(r'^\\d+$')\n",
    "\n",
    "SKIP_INSTAGRAM_USERNAMES = set([\n",
    "    \"reel\", 'tv'\n",
    "])\n",
    "# Skip youtube channel IDs like \"UCtWQDzuH1e84SebEyZN_aXw\": channel IDs always start with \"UC\" and are 24 chars\n",
    "YOUTUBE_CHANNEL_ID_RE = re.compile(r\"^UC[\\w-]{22}$\")\n",
    "\n",
    "# X.com \"i\" pseudo-username (used for i/grok/share/ and others: always skip these, and possibly log for debug)\n",
    "SKIP_X_USERNAMES = set([\n",
    "    \"i\",\n",
    "])\n",
    "\n",
    "def get_social_usernames_with_counts(domains_data, idx_path, data_path, test_mode=False, print_debug=False):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "        domains_data: loaded from *_domains.json [{title: {...}}, ...]\n",
    "        idx_path: path to cached index\n",
    "        data_path: article data file (either grokipedia or wikipedia)\n",
    "        test_mode: if True, only process the first 1000 articles for a quick test.\n",
    "    Returns:\n",
    "        dict mapping titles to {site: Counter({username: count, ...}), ...} for user/social platforms.\n",
    "    \"\"\"\n",
    "    print(f\"Opening index file: {idx_path}\")\n",
    "    with open(idx_path, \"rb\") as f:\n",
    "        idx_data = pickle.load(f)\n",
    "    print(f\"Index loaded. Number of entries: {len(idx_data)}\")\n",
    "\n",
    "    # Flatten input to [(title, domains)] list\n",
    "    if isinstance(domains_data, dict):\n",
    "        items_list = list(domains_data.items())\n",
    "    else:\n",
    "        items_list = []\n",
    "        for d in domains_data:\n",
    "            for title, domains in d.items():\n",
    "                items_list.append((title, domains))\n",
    "    print(f\"items_list created with {len(items_list)} articles\")\n",
    "\n",
    "    results = {}\n",
    "    skipped_x_i = []  # For optional debug reporting\n",
    "\n",
    "    iter_items = items_list\n",
    "    bar_length = min(1000, len(items_list)) if test_mode else len(items_list)\n",
    "    with tqdm(total=bar_length, desc=\"Extracting social usernames\", unit=\"article\") as pbar:\n",
    "        for i, (title_orig, domains) in enumerate(iter_items):\n",
    "            if test_mode and i >= 1000:\n",
    "                print(\"Test mode enabled. Breaking after 1000 records.\")\n",
    "                break\n",
    "\n",
    "            title_lookup = title_orig.replace(\"_\", \" \").lower()\n",
    "            title_for_result = title_lookup\n",
    "\n",
    "            relevant = any(dom in SITE_PATTERNS for dom in domains)\n",
    "            if not relevant:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            if i % 10000 == 0 and i != 0:\n",
    "                print(f\"Processing index {i} / {len(items_list)}: '{title_for_result}'...\")\n",
    "\n",
    "            line_idx = idx_data.get(title_orig)\n",
    "            if line_idx is None:\n",
    "                line_idx = idx_data.get(title_lookup)\n",
    "            if line_idx is None:\n",
    "                if print_debug:\n",
    "                    print(f\"  WARNING: No index for title '{title_for_result}'\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ref = get_object_at_offset(data_path, line_idx)\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not fetch article for '{title_for_result}' (offset: {line_idx}): {e}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            urls = []\n",
    "\n",
    "            # This follows the extraction logic in 05_preprocessing.ipynb\n",
    "            if 'wikipedia' in data_path:\n",
    "                references = ref.get(\"references\", [])\n",
    "                if i < 5 and print_debug:\n",
    "                    print(f\"  [DEBUG] First refs for '{title_for_result}': {references[:2]}\")\n",
    "                for single_ref in references:\n",
    "                    metadata = single_ref.get(\"metadata\") if isinstance(single_ref, dict) else None\n",
    "                    url = metadata.get(\"url\") if isinstance(metadata, dict) else None\n",
    "                    if url:\n",
    "                        urls.append(url)\n",
    "            else:\n",
    "                references = ref.get(\"data\", {}).get(\"references\", [])\n",
    "                if i < 5 and print_debug:\n",
    "                    print(f\"  [DEBUG] First grok refs for '{title_for_result}': {references[:2]}\")\n",
    "                for single_ref in references:\n",
    "                    link = single_ref.get(\"link\") if isinstance(single_ref, dict) else None\n",
    "                    href = link.get(\"href\") if isinstance(link, dict) else None\n",
    "                    if href:\n",
    "                        urls.append(href)\n",
    "\n",
    "            if i < 5 and print_debug:\n",
    "                print(f\"  [DEBUG] Extracted URLs for '{title_for_result}': {urls[:5]}\")\n",
    "\n",
    "            usernames_by_site = defaultdict(Counter)\n",
    "            for url in urls:\n",
    "                if not url:\n",
    "                    continue\n",
    "                for site, pat in ALL_PATTERNS:\n",
    "                    if site in url:\n",
    "                        match = pat.search(url)\n",
    "                        if match:\n",
    "                            # Disallow youtube.com/watch etc. as username\n",
    "                            if site == \"youtube.com\" and \"watch\" in url and not \"/user/\" in url and not \"/c/\" in url and not \"/channel/\" in url:\n",
    "                                continue\n",
    "                            # Disallow instagram.com/p/ as usernames\n",
    "                            if site == \"instagram.com\" and re.search(r\"/p/[^/?#]+\", url):\n",
    "                                continue\n",
    "                            # facebook.com pattern has group 2, rest have group 1\n",
    "                            if site == \"facebook.com\" and match.lastindex and match.lastindex > 1:\n",
    "                                username = match.group(2)\n",
    "                            else:\n",
    "                                username = match.group(1)\n",
    "\n",
    "                            # Skip site-specific username types\n",
    "                            if site == \"facebook.com\":\n",
    "                                if username in SKIP_FACEBOOK_USERNAMES:\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped facebook.com username '{username}' in URL: {url}\")\n",
    "                                    continue\n",
    "                                if FACEBOOK_NUMERIC_RE.match(username):\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped numeric facebook.com username '{username}' in URL: {url}\")\n",
    "                                    continue\n",
    "                            elif site == \"instagram.com\":\n",
    "                                if username in SKIP_INSTAGRAM_USERNAMES:\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped instagram.com username '{username}' in URL: {url}\")\n",
    "                                    continue\n",
    "                            elif site == \"youtube.com\":\n",
    "                                if YOUTUBE_CHANNEL_ID_RE.match(username):\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped youtube.com channel ID '{username}' in URL: {url}\")\n",
    "                                    continue\n",
    "                            elif site in {\"x.com\", \"twitter.com\"}:\n",
    "                                if username in SKIP_X_USERNAMES:\n",
    "                                    skipped_x_i.append((username, url, title_for_result))\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped x.com username '{username}' in URL: {url}\")\n",
    "                                    continue\n",
    "                                if username in SPURIOUS_X_USERNAMES:\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped spurious {site} username '{username}' from URL: {url}\")\n",
    "                                    continue\n",
    "\n",
    "                            if username:\n",
    "                                usernames_by_site[site][username] += 1\n",
    "                                if print_debug:\n",
    "                                    print(f\"    [DEBUG] Matched {site} username '{username}' in URL: {url}\")\n",
    "\n",
    "            # Only save in results if found any usernames\n",
    "            if usernames_by_site and print_debug:\n",
    "                print(f\"  Found usernames for {title_for_result}: {[ (site, dict(counter)) for site, counter in usernames_by_site.items() if counter ]}\")\n",
    "                results[title_for_result] = {site: dict(counter) for site, counter in usernames_by_site.items() if counter}\n",
    "            elif usernames_by_site:\n",
    "                results[title_for_result] = {site: dict(counter) for site, counter in usernames_by_site.items() if counter}\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Total articles with any matching usernames: {len(results)}\")\n",
    "    return results, skipped_x_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18f35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/overall/domains/wp_domains.json', 'r') as f:\n",
    "    wp_domains = json.load(f)\n",
    "\n",
    "with open('../results/overall/domains/grok_domains.json', 'r') as f:\n",
    "    grok_domains = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04018c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_usernames_by_title, wp_grok_convo_links = get_social_usernames_with_counts(wp_domains, '../results/overall/cached_wiki_idx.pkl', '../grokipedia_wikipedia_articles.ndjson')\n",
    "grok_usernames_by_title, grok_grok_convo_links = get_social_usernames_with_counts(grok_domains, '../results/overall/cached_grok_idx.pkl', '../grokipedia_scrape.ndjson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b263de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/wp_usernames_by_title.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"title\", \"site\", \"username\", \"count\"])\n",
    "    for title, site_dict in wp_usernames_by_title.items():\n",
    "        for site, usernames in site_dict.items():\n",
    "            for username, count in usernames.items():\n",
    "                writer.writerow([title, site, username, count])\n",
    "\n",
    "with open('../results/grok_usernames_by_title.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"title\", \"site\", \"username\", \"count\"])\n",
    "    for title, site_dict in grok_usernames_by_title.items():\n",
    "        for site, usernames in site_dict.items():\n",
    "            for username, count in usernames.items():\n",
    "                writer.writerow([title, site, username, count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e946cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = {}\n",
    "\n",
    "for (_, link, title) in grok_grok_convo_links:\n",
    "    if 'i/grok/share' in link:\n",
    "        if title not in pages:\n",
    "            pages[title] = set()\n",
    "\n",
    "        link_id = link.strip().split('/')[-1]\n",
    "        # Add link only if there is no other link in pages[title] with the same final part\n",
    "        if link_id not in {l.strip().split('/')[-1] for l in pages[title]}:\n",
    "            pages[title].add(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb052de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/grok_grok_convo_links.txt', 'w') as f:\n",
    "    for page, links in pages.items():\n",
    "        for link in links:\n",
    "            f.write(f'{page},{link}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_username_df = pd.read_csv('../results/wp_usernames_by_title.csv')\n",
    "grok_username_df = pd.read_csv('../results/grok_usernames_by_title.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c552a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i[1] for i in wp_grok_convo_links if 'i/grok/share' in i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i[1] for i in grok_grok_convo_links if 'i/communitynotes/share' in i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35097e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/grok_grok_convo_links.txt', 'w') as f:\n",
    "    for i in grok_grok_convo_links:\n",
    "        if 'i/grok/share' in i[1]:\n",
    "            f.write(f'{i[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_username_df = pd.merge(wp_username_df, grok_username_df, on=['title', 'site', 'username'], suffixes=('_wp', '_grok'), how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21633b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_username_df['diff'] = combined_username_df['count_grok'] - combined_username_df['count_wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee76244",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_username_df[combined_username_df['title'] == 'elon musk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_username_df.sort_values(by='diff', ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames_by_site_df = combined_username_df[['site', 'username', 'count_wp', 'count_grok', 'diff']].groupby(['site', 'username']).sum().reset_index().sort_values(by='count_grok', ascending=False)#.sort_values(by='diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames_by_site_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a884538",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames_by_site_df[usernames_by_site_df['site'].isin(['twitter.com', 'x.com'])][['username', 'count_wp', 'count_grok', 'diff']].groupby('username').sum().sort_values(by='diff', ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425807c6",
   "metadata": {},
   "source": [
    "### Analysis of fringe sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fringe_sites = {\n",
    "    'unz.com',\n",
    "    'vdare.com',\n",
    "    'frontpagemag.com',\n",
    "    'jihadwatch.org',\n",
    "    'lifesitenews.com',\n",
    "    'thegatewaypundit.com',\n",
    "    'globalresearch.ca',\n",
    "    'voltairenet.org',\n",
    "    'infowars.com',\n",
    "    'stormfront.org',\n",
    "}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "fringe_rows = []\n",
    "for page in grok_domains:\n",
    "    title = list(page.keys())[0]\n",
    "    domains = list(page.values())[0]\n",
    "    for site in fringe_sites:\n",
    "        if site in domains:\n",
    "            fringe_rows.append({\n",
    "                'title': title,\n",
    "                'site': site,\n",
    "                'citations': domains[site]\n",
    "            })\n",
    "\n",
    "fringe_df = pd.DataFrame(fringe_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79011a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fringe_df.sort_values(by='citations', ascending=False).to_csv('../results/fringe_citations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe851d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_sites = {\n",
    "    'wikipedia.org',\n",
    "    'wiktionary.org',\n",
    "    'wikiquote.org',\n",
    "    'wikisource.org',\n",
    "    'wikiversity.org',\n",
    "    'wikivoyage.org',\n",
    "    'wiktionary.org',\n",
    "    'wikidata.org',\n",
    "    'wikibooks.org',\n",
    "    'wikinews.org',\n",
    "    'wikispecies.org',\n",
    "    'wikiversity.org',\n",
    "    'wikivoyage.org',\n",
    "}\n",
    "\n",
    "wikipedia_rows = []\n",
    "for page in grok_domains:\n",
    "    title = list(page.keys())[0]\n",
    "    domains = list(page.values())[0]\n",
    "    for site in wikipedia_sites:\n",
    "        for domain in domains:\n",
    "            if site in domain:\n",
    "                wikipedia_rows.append({\n",
    "                    'title': title,\n",
    "                    'site': domain,\n",
    "                    'citations': domains[domain]\n",
    "                })\n",
    "\n",
    "wikipedia_df = pd.DataFrame(wikipedia_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543111c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_df.sort_values(by='citations', ascending=False).to_csv('../results/wikipedia_citations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69947487",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_df.site.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37541d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_citations = 0\n",
    "for page in grok_domains:\n",
    "    domains = list(page.values())[0]\n",
    "    total_citations += sum(domains.values())\n",
    "total_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6111c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
