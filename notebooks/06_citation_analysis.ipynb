{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e95d735",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "Actually do the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bb73a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec23cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = \"../results\"\n",
    "SUPP_DATA_DIR = \"../supplemental_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c63a4c",
   "metadata": {},
   "source": [
    "## Absolute comparison of domain occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/grok_domains.json', 'r') as f:\n",
    "    grok_domains = json.load(f)\n",
    "\n",
    "with open('../results/wp_domains.json', 'r') as f:\n",
    "    wp_domains = json.load(f)\n",
    "\n",
    "total_grok = 0\n",
    "total_wp = 0\n",
    "grok_domains_counter = Counter()\n",
    "wp_domains_counter = Counter()\n",
    "\n",
    "for a in grok_domains:\n",
    "    for d, c in a.items():\n",
    "        for domain, count in c.items():\n",
    "            grok_domains_counter[domain] += count\n",
    "            total_grok += count\n",
    "\n",
    "for a in wp_domains:\n",
    "    for d, c in a.items():\n",
    "        for domain, count in c.items():\n",
    "            wp_domains_counter[domain] += count\n",
    "            total_wp += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8734cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_citation_count_df = pd.DataFrame(list(wp_domains_counter.items()), columns=['domain', 'wiki_count'])\n",
    "wiki_citation_count_df['wiki_total_cites'] = total_wp\n",
    "wiki_citation_count_df['wiki_share'] = wiki_citation_count_df['wiki_count'] / total_wp\n",
    "grok_citation_count_df = pd.DataFrame(list(grok_domains_counter.items()), columns=['domain', 'grok_count'])\n",
    "grok_citation_count_df['grok_total_cites'] = total_grok\n",
    "grok_citation_count_df['grok_share'] = grok_citation_count_df['grok_count'] / total_grok\n",
    "citation_count_df = pd.merge(wiki_citation_count_df, grok_citation_count_df, on='domain', how='outer').fillna(0)\n",
    "citation_count_df['count_diff'] = citation_count_df['grok_count'] - citation_count_df['wiki_count']\n",
    "citation_count_df['share_diff'] = citation_count_df['grok_share'] - citation_count_df['wiki_share']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885fa743",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_count_df.to_csv('../results/citation_count_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be59deb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_domains(domain):\n",
    "    if domain in ['bbc.co.uk', 'bbc.com', 'news.bbc.co.uk']:\n",
    "        return 'bbc.com'\n",
    "    if domain in ['twitter.com', 'x.com']:\n",
    "        return 'x/twitter.com'\n",
    "    if domain in ['edition.cnn.com', 'cnn.com']:\n",
    "        return 'cnn.com'\n",
    "    if domain in ['timesofindia.indiatimes.com', 'economictimes.indiatimes.com']:\n",
    "        return 'indiatimes.com'\n",
    "    return domain\n",
    "\n",
    "citation_count_df['combined_domain'] = citation_count_df['domain'].apply(combine_domains)\n",
    "# citation_count_df.sort_values('grok_share', ascending=False)[100:104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626d159",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_grok_corrected = citation_count_df[['combined_domain', 'grok_share', 'wiki_share']].groupby('combined_domain').sum().sort_values('grok_share', ascending=False)[:100]\n",
    "t100_wiki_corrected = citation_count_df[['combined_domain', 'grok_share', 'wiki_share']].groupby('combined_domain').sum().sort_values('wiki_share', ascending=False)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_grok_corrected.reset_index(inplace=True)\n",
    "t100_wiki_corrected.reset_index(inplace=True)\n",
    "t100_grok_corrected['domain'] = t100_grok_corrected['combined_domain']\n",
    "t100_wiki_corrected['domain'] = t100_wiki_corrected['combined_domain']\n",
    "t100_grok_corrected[['domain', 'grok_share']].to_csv('../results/t100_grok_corrected.csv', index=False)\n",
    "t100_wiki_corrected[['domain', 'wiki_share']].to_csv('../results/t100_wiki_corrected.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41062f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_grok = pd.read_csv('../results/t100_grok.csv')\n",
    "t100_wiki = pd.read_csv('../results/t100_wiki.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e20a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_grok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa09aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_grok_combined = pd.merge(t100_grok_corrected, t100_grok, left_on='combined_domain', right_on='domain', how='outer')\n",
    "t100_wiki_combined = pd.merge(t100_wiki_corrected, t100_wiki, left_on='combined_domain', right_on='domain', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af4f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b51abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t100_wiki_combined[~t100_wiki_combined.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c828b",
   "metadata": {},
   "outputs": [],
   "source": [
    ".to_csv('../results/t100_grok_corrected.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3fb130",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_count_df.sort_values('grok_share', ascending=False)[100:104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da4c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_count_df.sort_values('wiki_share', ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "citation_count_df.sort_values('share_diff', ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271c3378",
   "metadata": {},
   "source": [
    "## (Un)reliable source additions / removals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86bd94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(f\"{RESULT_DIR}/domain_deltas.csv\")\n",
    "# df_wo_license = pd.read_csv(f'{RESULT_DIR}/domain_deltas_wo_license.csv')\n",
    "# df_w_license = pd.read_csv(f'{RESULT_DIR}/domain_deltas_w_license.csv')\n",
    "domain_set = (\n",
    "    set(\n",
    "        pd.read_csv(f'{SUPP_DATA_DIR}/domain_lists/openalex_journal_domains.csv')\n",
    "        .Domain.tolist()\n",
    "    ).union({\n",
    "        'academia.edu',\n",
    "        'arxiv.org',\n",
    "        'cambridge.org',\n",
    "        'ebsco.com',\n",
    "        'journals.uchicago.edu',\n",
    "        'jstor.org',\n",
    "        'mdpi.com',\n",
    "        'ncbi.nlm.nih.gov',\n",
    "        'papers.ssrn.com',\n",
    "        'researchgate.net',\n",
    "        'sciencedirect.com',\n",
    "        'tandfonline.com'\n",
    "    })\n",
    ")\n",
    "# reliability_df = pd.read_csv(f'{SUPP_DATA_DIR}/perennial_sources_enwiki/perennial_sources.csv')\n",
    "reliability_df = pd.read_csv(f'{SUPP_DATA_DIR}/perennial_sources_enwiki/enwiki_perennial_list.csv')\n",
    "reliability_df['domain'] = reliability_df['source']\n",
    "reliability_df = reliability_df[['domain', 'status']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f9bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_w_license = pd.merge(df_w_license, reliability_df, on='domain')\n",
    "df_wo_license = pd.merge(df_wo_license, reliability_df, on='domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eec534",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content adapted from Wikipedia (with license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Summary of link changes from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "df_w_license[['delta_sum', 'status']].groupby(df_w_license['status']).sum().drop('status', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde60668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content adapted from Wikipedia (with license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Sources deemed: 'generally_reliable' by the enwiki community\")\n",
    "print(\"Measuring: Total links with a domain removed from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "(\n",
    "    df_w_license[\n",
    "        df_w_license['reliability_status'] == 'generally_reliable']\n",
    "        .sort_values('delta_sum', ascending=True)[:25]\n",
    "        [['source_name', 'domain', 'delta_sum']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content adapted from Wikipedia (with license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Sources deemed: 'generally_reliable' by the enwiki community\")\n",
    "print(\"Measuring: Total links with a domain removed from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "(\n",
    "    df_w_license[\n",
    "        df_w_license['reliability_status'] == 'generally_reliable']\n",
    "        .sort_values('delta_sum', ascending=True)[:25]\n",
    "        [['source_name', 'domain', 'delta_sum']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a9cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content NOT adapted from Wikipedia (without license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Summary of link changes from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "df_wo_license[['delta_sum', 'reliability_status']].groupby(df_wo_license['reliability_status']).sum().drop('reliability_status', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19550418",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content NOT adapted from Wikipedia (without license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Sources deemed: 'generally_reliable' by the enwiki community\")\n",
    "print(\"Measuring: Total links with a domain removed from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "\n",
    "(\n",
    "    df_wo_license[\n",
    "        df_wo_license['reliability_status'] == 'generally_reliable']\n",
    "        .sort_values('delta_sum', ascending=True)[:25]\n",
    "        [['source_name', 'domain', 'delta_sum']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b934780",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"Grokipedia content NOT adapted from Wikipedia (without license)\")\n",
    "print(\"=\"*100)\n",
    "print(\"Sources deemed: 'generally_unreliable' by the enwiki community\")\n",
    "print(\"Measuring: Total links with a domain added from Wikipedia --> Grokipedia\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "(\n",
    "    df_wo_license[\n",
    "        df_wo_license['reliability_status'] == 'generally_unreliable']\n",
    "        .sort_values('delta_sum', ascending=False)[:25]\n",
    "        [['source_name', 'domain', 'delta_sum']]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2614bd1c",
   "metadata": {},
   "source": [
    "## How much more cited are specific domain types?\n",
    "\n",
    "Specifically looking at academic journal domains (which Grok seems to really like) and .gov and .mil domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a758d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_government_and_mil_domain(domain):\n",
    "    \"\"\"\n",
    "    Filter to domains including .gov, .mil, or .gov.country (e.g., .gov.au, .gov.uk, .gov.in, etc.)\n",
    "    Safely return False if domain is missing or not a string.\n",
    "    \"\"\"\n",
    "    gov_mil_pattern = re.compile(r'\\.gov(\\.|$)|\\.mil(\\.|$)')\n",
    "    if not isinstance(domain, str):\n",
    "        return False\n",
    "    return bool(gov_mil_pattern.search(domain))\n",
    "\n",
    "def is_journal_domain(domain):\n",
    "    return True if domain in domain_set else False\n",
    "\n",
    "gov_mil_domain_df = df[df.domain.apply(is_government_and_mil_domain)]\n",
    "journal_domain_df = df[df.domain.apply(is_journal_domain)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8566ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_academic_sources_net_cites = journal_domain_df.sort_values('delta_sum', ascending=False)[:100].delta_sum.sum()\n",
    "top_100_gov_mil_sources_net_cites = gov_mil_domain_df.sort_values('delta_sum', ascending=False)[:100].delta_sum.sum()\n",
    "academic_gov_mil_domains = set(journal_domain_df.sort_values('delta_sum', ascending=False)[:100].domain.tolist()).union(set(gov_mil_domain_df.sort_values('delta_sum', ascending=False)[:100].domain.tolist()))\n",
    "\n",
    "total_net_cites = df.delta_sum.sum()\n",
    "total_domains = df.domain.count()\n",
    "\n",
    "print(f\"Of a total net increase of {total_net_cites:,} citations:\")\n",
    "print(f\"  - {top_100_academic_sources_net_cites:,} ({top_100_academic_sources_net_cites/total_net_cites:.1%}) are from the top 100 journal domains\")\n",
    "print(f\"  - {top_100_gov_mil_sources_net_cites:,} ({top_100_gov_mil_sources_net_cites/total_net_cites:.1%}) are from the top 100 gov/mil domains\")\n",
    "print(f\"These sources correspond with only {len(academic_gov_mil_domains) / total_domains:.5%} of the cited domains on Grokipedia\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ece7b",
   "metadata": {},
   "source": [
    "## Shifts in article citation composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032278fe",
   "metadata": {},
   "source": [
    "#### Which articles had the largest shifts from enwiki-deemed reliable to unreliable sources in their composition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reliability_shift_maximal(reliability_df, result_dir='../results/overall/domains', fsuffix='_domains.json'):\n",
    "    \"\"\"\n",
    "    Find articles where (WP_reliable - WP_unreliable) - (grok_unreliable - grok_reliable) is maximal.\n",
    "    This identifies pages that went from well-sourced (reliable) in WP to poorly-sourced (unreliable) in Grok.\n",
    "    \n",
    "    Args:\n",
    "        reliability_df: DataFrame with 'domain' and 'status' columns\n",
    "        result_dir: Directory containing wp_domains.json and grok_domains.json\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples with article shift data, sorted by the maximal function\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_article_reliability_counts(json_file, reliability_df):\n",
    "        \"\"\"Load JSON and count citations by reliability status per article.\"\"\"\n",
    "        # Create lookup dict for reliability status by normalized domain\n",
    "        reliability_lookup = {}\n",
    "        for _, row in reliability_df.iterrows():\n",
    "            domain = row['domain']\n",
    "            if domain:\n",
    "                reliability_lookup[domain] = row.get('status', None)\n",
    "        \n",
    "        article_stats = []\n",
    "        \n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for article_dict in data:\n",
    "            for article_title, domains in article_dict.items():\n",
    "                if isinstance(domains, dict):\n",
    "                    rel_count = 0\n",
    "                    unrel_count = 0\n",
    "                    blacklist_count = 0\n",
    "                    no_consensus_count = 0\n",
    "                    deprecated_count = 0\n",
    "                    other_count = 0\n",
    "                    total_count = 0\n",
    "                    \n",
    "                    for domain, count in domains.items():\n",
    "                        total_count += count\n",
    "                        if domain and domain in reliability_lookup:\n",
    "                            status = reliability_lookup[domain]\n",
    "                            if status == 'Generally reliable':\n",
    "                                rel_count += count\n",
    "                            elif status == 'Generally unreliable':\n",
    "                                unrel_count += count\n",
    "                            elif status == 'Deprecated':\n",
    "                                deprecated_count += count\n",
    "                            elif status == 'No consensus':\n",
    "                                no_consensus_count += count\n",
    "                            elif status == 'Blacklisted':\n",
    "                                blacklist_count += count\n",
    "                            else:\n",
    "                                # Domain in lookup but has other status (e.g., deprecated, no consensus)\n",
    "                                other_count += count\n",
    "                        else:\n",
    "                            # Domain not in reliability lookup (unknown/other sources)\n",
    "                            other_count += count\n",
    "                    \n",
    "                    article_stats.append((article_title, rel_count, unrel_count, blacklist_count, no_consensus_count, deprecated_count, other_count, total_count))\n",
    "        \n",
    "        return article_stats\n",
    "    \n",
    "    # Get per-article reliability counts\n",
    "    wp_articles = get_article_reliability_counts(f'{result_dir}/wp{fsuffix}', reliability_df)\n",
    "    grok_articles = get_article_reliability_counts(f'{result_dir}/grok{fsuffix}', reliability_df)\n",
    "    \n",
    "    print(f\"Loaded {len(wp_articles)} articles from Wikipedia\")\n",
    "    print(f\"Loaded {len(grok_articles)} articles from Grokipedia\\n\")\n",
    "    \n",
    "    # Create dictionaries for quick lookup\n",
    "    wp_dict = {art[0]: art for art in wp_articles}\n",
    "    grok_dict = {art[0]: art for art in grok_articles}\n",
    "    \n",
    "    # Get all unique articles\n",
    "    all_articles = set(wp_dict.keys()) | set(grok_dict.keys())\n",
    "    \n",
    "    # Calculate the maximal function for each article\n",
    "    article_maximals = []\n",
    "    for article in all_articles:\n",
    "        wp_data = wp_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        grok_data = grok_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        \n",
    "        _, wp_rel, wp_unrel, wp_blacklist, wp_no_consensus, wp_deprecated, wp_other, wp_total = wp_data\n",
    "        _, grok_rel, grok_unrel, grok_blacklist, grok_no_consensus, grok_deprecated, grok_other, grok_total = grok_data\n",
    "        \n",
    "        # Calculate: (WP_reliable - WP_unreliable) - (grok_unreliable - grok_reliable)\n",
    "        # = WP_reliable - WP_unreliable - grok_unreliable + grok_reliable\n",
    "        maximal_value = (wp_rel - wp_unrel) - (grok_unrel - grok_rel)\n",
    "        \n",
    "        article_maximals.append((\n",
    "            article,\n",
    "            wp_rel, wp_unrel, wp_blacklist, wp_no_consensus, wp_deprecated, wp_other, wp_total,\n",
    "            grok_rel, grok_unrel, grok_blacklist, grok_no_consensus, grok_deprecated, grok_other, grok_total,\n",
    "            maximal_value\n",
    "        ))\n",
    "    \n",
    "    return article_maximals\n",
    "\n",
    "columns = [\n",
    "        'title', 'wp_reliable', 'wp_unreliable', 'wp_blacklist', 'wp_no_consensus', 'wp_deprecated', 'wp_other',\n",
    "        'wp_total', 'grok_reliable', 'grok_unreliable', 'grok_blacklist', 'grok_no_consensus', 'grok_deprecated',\n",
    "        'grok_other', 'grok_total', 'maximal'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_maximals = find_reliability_shift_maximal(reliability_df)\n",
    "max_reliability_shift_df = pd.DataFrame(article_maximals, columns=columns)\n",
    "article_maximals_w_license = find_reliability_shift_maximal(reliability_df, fsuffix='_domains_w_license.json')\n",
    "max_reliability_shift_df_w_license = pd.DataFrame(article_maximals_w_license, columns=columns)\n",
    "article_maximals_wo_license = find_reliability_shift_maximal(reliability_df, fsuffix='_domains_wo_license.json')\n",
    "max_reliability_shift_df_wo_license = pd.DataFrame(article_maximals_wo_license, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1a45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reliability_shift_df.to_csv('../results/reliability_citation_diff.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_charts(filtered_df, fsuffix='', show=True, title=None):\n",
    "    \"\"\"\n",
    "    Plots stacked bar + diagonal comparison charts using the given DataFrame:\n",
    "    1. Stacked bar and overlay: Proportion of sources in each reliability category for Wikipedia and Grokipedia, with diagonal fills illustrating change.\n",
    "    2. Bar chart: Percentage of articles containing at least one source of each type (not 'other') for Wikipedia and Grokipedia.\n",
    "\n",
    "    Parameters:\n",
    "        filtered_df (pd.DataFrame): DataFrame, typically filtered on articles of interest.\n",
    "        show (bool): If True, calls plt.show() at end.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    # --- Setup categories, labels, colors ---\n",
    "    column_order = [\n",
    "        'reliable', 'unreliable', 'blacklist', 'no_consensus', 'deprecated', 'other'\n",
    "    ]\n",
    "    display_names = {\n",
    "        'reliable': 'Generally reliable',\n",
    "        'unreliable': 'Generally unreliable',\n",
    "        'blacklist': 'Blacklisted',\n",
    "        'no_consensus': 'No consensus',\n",
    "        'deprecated': 'Deprecated',\n",
    "        'other': 'Other'\n",
    "    }\n",
    "    color_map = {\n",
    "        'reliable': 'green',\n",
    "        'unreliable': 'red',\n",
    "        'blacklist': 'black',\n",
    "        'no_consensus': 'yellow',\n",
    "        'deprecated': 'orange',\n",
    "        'other': 'grey'\n",
    "    }\n",
    "\n",
    "    # --- Aggregate counts as \"wp\" and \"grok\" \"status\" table ---\n",
    "    agg = {\n",
    "        'wp_reliable': filtered_df['wp_reliable'].sum(),\n",
    "        'wp_unreliable': filtered_df['wp_unreliable'].sum(),\n",
    "        'wp_blacklist': filtered_df['wp_blacklist'].sum(),\n",
    "        'wp_no_consensus': filtered_df['wp_no_consensus'].sum(),\n",
    "        'wp_deprecated': filtered_df['wp_deprecated'].sum(),\n",
    "        'wp_other': filtered_df['wp_other'].sum(),\n",
    "        'grok_reliable': filtered_df['grok_reliable'].sum(),\n",
    "        'grok_unreliable': filtered_df['grok_unreliable'].sum(),\n",
    "        'grok_blacklist': filtered_df['grok_blacklist'].sum(),\n",
    "        'grok_no_consensus': filtered_df['grok_no_consensus'].sum(),\n",
    "        'grok_deprecated': filtered_df['grok_deprecated'].sum(),\n",
    "        'grok_other': filtered_df['grok_other'].sum(),\n",
    "    }\n",
    "\n",
    "    # Print the number of blacklisted sources\n",
    "    print(f\"Number of blacklisted sources (Wikipedia): {agg['wp_blacklist']}\")\n",
    "    print(f\"Number of blacklisted sources (Grokipedia): {agg['grok_blacklist']}\")\n",
    "    \n",
    "    # Print the number of pages/articles with at least one blacklisted source\n",
    "    wp_pages_with_blacklist = (filtered_df['wp_blacklist'] > 0).sum()\n",
    "    grok_pages_with_blacklist = (filtered_df['grok_blacklist'] > 0).sum()\n",
    "    print(f\"Number of pages with at least one blacklisted source (Wikipedia): {wp_pages_with_blacklist}\")\n",
    "    print(f\"Number of pages with at least one blacklisted source (Grokipedia): {grok_pages_with_blacklist}\")\n",
    "\n",
    "    # Make DF of shape: index=['Wikipedia', 'Grokipedia'], columns=column_order\n",
    "    wp_row = [agg[f'wp_{k}'] for k in column_order]\n",
    "    grok_row = [agg[f'grok_{k}'] for k in column_order]\n",
    "    prop_df = pd.DataFrame(\n",
    "        [wp_row, grok_row],\n",
    "        columns=column_order,\n",
    "        index=['Wikipedia', 'Grokipedia']\n",
    "    )\n",
    "    prop_df_norm = prop_df.div(prop_df.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    # --- Plotting stacked bars with diagonal overlays, make first plot narrower and better aligned ---\n",
    "    labels = ['Wikipedia', 'Grokipedia']\n",
    "    x = np.arange(len(labels))\n",
    "    bar_sep = 0.09  # reduce gap between bars\n",
    "    width = 0.18    # make bars narrower\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 7), gridspec_kw={'width_ratios': [1, 1.7]})\n",
    "    \n",
    "    # Set figure title (for whole figure)\n",
    "    plot_title = title if title is not None else \"Source Reliability Category Proportion: Wikipedia vs Grokipedia\"\n",
    "    fig.suptitle(plot_title, fontsize=18, y=0.98)\n",
    "    ax = axs[0]\n",
    "\n",
    "    # Set up stacking\n",
    "    bottoms = [0, 0]\n",
    "    bars_wp = []\n",
    "    bars_grok = []\n",
    "\n",
    "    # For synchronized stacking, process in column order:\n",
    "    print(\"Wikipedia proportions:\")\n",
    "    print(prop_df_norm.loc['Wikipedia'])\n",
    "    print(\"\\nGrokipedia proportions:\")\n",
    "    print(prop_df_norm.loc['Grokipedia'])\n",
    "    for j, col in enumerate(column_order):\n",
    "        color = color_map.get(col, 'grey')\n",
    "        # WP bar proportions\n",
    "        wp_prop = prop_df_norm.loc['Wikipedia', col]\n",
    "        grok_prop = prop_df_norm.loc['Grokipedia', col]\n",
    "        bar_wp = ax.bar(x[0] - width/2 - bar_sep/2, wp_prop, width=width,\n",
    "                        bottom=bottoms[0], color=color, edgecolor='none', zorder=2, alpha=0.8)\n",
    "        bar_grok = ax.bar(x[1] + width/2 + bar_sep/2, grok_prop, width=width,\n",
    "                          bottom=bottoms[1], color=color, edgecolor='none', zorder=2, alpha=0.8)\n",
    "\n",
    "        # Diagonal change fill\n",
    "        wp_top = bottoms[0] + wp_prop\n",
    "        grok_top = bottoms[1] + grok_prop\n",
    "        ax.fill_between(\n",
    "            [x[0] - width/2, x[1] + width/2],\n",
    "            [wp_top, grok_top],\n",
    "            [bottoms[0], bottoms[1]],\n",
    "            color=color, alpha=0.25, zorder=1, linewidth=0\n",
    "        )\n",
    "        bars_wp.append(bar_wp)\n",
    "        bars_grok.append(bar_grok)\n",
    "        bottoms[0] += wp_prop\n",
    "        bottoms[1] += grok_prop\n",
    "\n",
    "    # Set axis ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, fontsize=16)\n",
    "    ax.set_ylabel(\"Proportion of Citations\", fontsize=16)\n",
    "    ax.set_title(\"Source Status Proportion: Wikipedia vs Grokipedia\", fontsize=16)\n",
    "\n",
    "    # Make axis tight with bars, remove excess white space\n",
    "    # Bars are at x[0]=0 and x[1]=1, with width=0.18 and bar_sep=0.09\n",
    "    # Left bar spans: -0.18 to 0, right bar spans: 1 to 1.18\n",
    "    # Add small padding: 0.05 on each side\n",
    "    ax.set_xlim(-0.23, 1.23)\n",
    "    ax.set_ylim(bottom=0, top=1.01)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    # Custom legend patch (color by status) - move inside plot to reduce whitespace\n",
    "    legend_elements = [Patch(facecolor=color_map.get(col, 'grey'), label=display_names.get(col, col), alpha=0.8) for col in column_order]\n",
    "    ax.legend(handles=legend_elements, title='Source Status', loc='upper center', framealpha=0.9)\n",
    "\n",
    "    # Tighten subplot spacing to reduce whitespace\n",
    "    fig.subplots_adjust(wspace=0.15, left=0.05, right=0.97, top=0.92, bottom=0.1)\n",
    "\n",
    "    # ---- New plot: % of articles containing at least 1 in each source type (not 'other') ----\n",
    "\n",
    "    ax2 = axs[1]\n",
    "    main_types = ['reliable', 'unreliable', 'blacklist', 'no_consensus', 'deprecated']\n",
    "    type_labels = [display_names[t] for t in main_types]\n",
    "    bar_x = np.arange(len(main_types))\n",
    "    bar_width = 0.36\n",
    "\n",
    "    n_articles = len(filtered_df)\n",
    "    percentages = {'Wikipedia': [], 'Grokipedia': []}\n",
    "    for source_type in main_types:\n",
    "        wp_col = f'wp_{source_type}'\n",
    "        grok_col = f'grok_{source_type}'\n",
    "        wp_count = (filtered_df[wp_col] > 0).sum()\n",
    "        grok_count = (filtered_df[grok_col] > 0).sum()\n",
    "        percentages['Wikipedia'].append(wp_count / n_articles * 100)\n",
    "        percentages['Grokipedia'].append(grok_count / n_articles * 100)\n",
    "\n",
    "    # Plot as side-by-side bars\n",
    "    ax2.bar(bar_x - bar_width/2, percentages['Wikipedia'], bar_width,\n",
    "           label='Wikipedia', color=\"#4977bc\", edgecolor='black', alpha=0.7)\n",
    "    ax2.bar(bar_x + bar_width/2, percentages['Grokipedia'], bar_width,\n",
    "           label='Grokipedia', color=\"#e86b54\", edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for i, (wp, gk) in enumerate(zip(percentages['Wikipedia'], percentages['Grokipedia'])):\n",
    "        ax2.text(i - bar_width/2, wp + 1, f\"{wp:.1f}%\", ha='center', va='bottom', fontsize=14, color=\"#10426b\")\n",
    "        ax2.text(i + bar_width/2, gk + 1, f\"{gk:.1f}%\", ha='center', va='bottom', fontsize=14, color=\"#7a230c\")\n",
    "\n",
    "    ax2.set_xticks(bar_x)\n",
    "    ax2.set_xticklabels(type_labels, rotation=14, fontsize=14)\n",
    "    ax2.set_ylim(0, 105)\n",
    "    ax2.set_ylabel(\"Percent of Articles\", fontsize=16)\n",
    "    ax2.set_title(\"% of Articles Citing Any Source in Category\", fontsize=16)\n",
    "    ax2.legend(loc='upper right', fontsize=14)\n",
    "    ax2.grid(axis='y', linestyle=':', alpha=0.4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'../graphics/overall_grok_wp_cite_composition_{fsuffix}.pdf')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8717be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_charts(max_reliability_shift_df, title='Source Reliability Category Comparison (all articles)')\n",
    "plot_reliability_charts(max_reliability_shift_df_w_license, fsuffix='_w_license', title='Source Reliability Category Comparison (all articles with CC-license)')\n",
    "plot_reliability_charts(max_reliability_shift_df_wo_license, fsuffix='_wo_license', title='Source Reliability Category Comparison (all articles without CC-license)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e782c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_reliability_shift_df.to_csv(f'{RESULT_DIR}/reliability_citation_diff.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ee00cc",
   "metadata": {},
   "source": [
    "### Using Lin et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82212332",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reliability = pd.read_csv(f'../supplemental_data/news_reliability/LinRating_Join.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c31b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reliability['reliability_score'] = lin_reliability['pc1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bdc9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reliability_shift_by_buckets(lin_reliability, result_dir='../results/overall/domains', fsuffix='_domains.json'):\n",
    "    \"\"\"\n",
    "    Find articles where reliability shifts occur, grouped by reliability score buckets (0.2-sized buckets).\n",
    "    Uses lin_reliability DataFrame with reliability_score instead of discrete status categories.\n",
    "    \n",
    "    Args:\n",
    "        lin_reliability: DataFrame with 'domain' and 'reliability_score' columns\n",
    "        result_dir: Directory containing wp_domains.json and grok_domains.json\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples with article shift data grouped by reliability score buckets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define buckets: 0-0.2, 0.2-0.4, 0.4-0.6, 0.6-0.8, 0.8-1.0\n",
    "    bucket_size = 0.2\n",
    "    buckets = [(i * bucket_size, (i + 1) * bucket_size) for i in range(5)]\n",
    "    bucket_labels = [f\"{i * bucket_size:.1f}-{(i + 1) * bucket_size:.1f}\" for i in range(5)]\n",
    "    \n",
    "    def get_bucket(score):\n",
    "        \"\"\"Assign a reliability score to a bucket.\"\"\"\n",
    "        if pd.isna(score):\n",
    "            return None\n",
    "        for i, (low, high) in enumerate(buckets):\n",
    "            if low <= score < high:\n",
    "                return i\n",
    "        # Handle edge case: score == 1.0\n",
    "        if score == 1.0:\n",
    "            return 4\n",
    "        return None\n",
    "    \n",
    "    def get_article_reliability_bucket_counts(json_file, lin_reliability):\n",
    "        \"\"\"Load JSON and count citations by reliability score bucket per article.\"\"\"\n",
    "        # Create lookup dict for reliability score by domain\n",
    "        # Handle both 'reliability_score' and 'pc1' column names\n",
    "        score_col = 'reliability_score' if 'reliability_score' in lin_reliability.columns else 'pc1'\n",
    "        reliability_lookup = {}\n",
    "        for _, row in lin_reliability.iterrows():\n",
    "            domain = row['domain']\n",
    "            if domain and pd.notna(row.get(score_col)):\n",
    "                reliability_lookup[domain] = row[score_col]\n",
    "        \n",
    "        article_stats = []\n",
    "        \n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        for article_dict in data:\n",
    "            for article_title, domains in article_dict.items():\n",
    "                if isinstance(domains, dict):\n",
    "                    # Initialize counts for each bucket\n",
    "                    bucket_counts = {i: 0 for i in range(5)}\n",
    "                    other_count = 0\n",
    "                    total_count = 0\n",
    "                    \n",
    "                    for domain, count in domains.items():\n",
    "                        total_count += count\n",
    "                        if domain and domain in reliability_lookup:\n",
    "                            score = reliability_lookup[domain]\n",
    "                            bucket_idx = get_bucket(score)\n",
    "                            if bucket_idx is not None:\n",
    "                                bucket_counts[bucket_idx] += count\n",
    "                            else:\n",
    "                                other_count += count\n",
    "                        else:\n",
    "                            # Domain not in reliability lookup (unknown/other sources)\n",
    "                            other_count += count\n",
    "                    \n",
    "                    # Return tuple: (title, bucket_0, bucket_1, bucket_2, bucket_3, bucket_4, other, total)\n",
    "                    article_stats.append((\n",
    "                        article_title,\n",
    "                        bucket_counts[0], bucket_counts[1], bucket_counts[2], \n",
    "                        bucket_counts[3], bucket_counts[4], other_count, total_count\n",
    "                    ))\n",
    "        \n",
    "        return article_stats\n",
    "    \n",
    "    # Get per-article reliability bucket counts\n",
    "    wp_articles = get_article_reliability_bucket_counts(f'{result_dir}/wp{fsuffix}', lin_reliability)\n",
    "    grok_articles = get_article_reliability_bucket_counts(f'{result_dir}/grok{fsuffix}', lin_reliability)\n",
    "    \n",
    "    print(f\"Loaded {len(wp_articles)} articles from Wikipedia\")\n",
    "    print(f\"Loaded {len(grok_articles)} articles from Grokipedia\\n\")\n",
    "    \n",
    "    # Create dictionaries for quick lookup\n",
    "    wp_dict = {art[0]: art for art in wp_articles}\n",
    "    grok_dict = {art[0]: art for art in grok_articles}\n",
    "    \n",
    "    # Get all unique articles\n",
    "    all_articles = set(wp_dict.keys()) | set(grok_dict.keys())\n",
    "    \n",
    "    # Calculate shifts for each article, grouped by buckets\n",
    "    article_bucket_shifts = []\n",
    "    for article in all_articles:\n",
    "        wp_data = wp_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        grok_data = grok_dict.get(article, (article, 0, 0, 0, 0, 0, 0, 0))\n",
    "        \n",
    "        _, wp_b0, wp_b1, wp_b2, wp_b3, wp_b4, wp_other, wp_total = wp_data\n",
    "        _, grok_b0, grok_b1, grok_b2, grok_b3, grok_b4, grok_other, grok_total = grok_data\n",
    "        \n",
    "        # Calculate shift for each bucket: WP_count - Grok_count\n",
    "        # Positive means WP has more, negative means Grok has more\n",
    "        shifts = [\n",
    "            wp_b0 - grok_b0,\n",
    "            wp_b1 - grok_b1,\n",
    "            wp_b2 - grok_b2,\n",
    "            wp_b3 - grok_b3,\n",
    "            wp_b4 - grok_b4\n",
    "        ]\n",
    "        \n",
    "        # Calculate overall shift metric: weighted sum (higher buckets weighted more)\n",
    "        # This gives more weight to shifts in higher reliability buckets\n",
    "        weighted_shift = sum(shifts[i] * (i + 1) for i in range(5))\n",
    "        \n",
    "        article_bucket_shifts.append((\n",
    "            article,\n",
    "            wp_b0, wp_b1, wp_b2, wp_b3, wp_b4, wp_other, wp_total,\n",
    "            grok_b0, grok_b1, grok_b2, grok_b3, grok_b4, grok_other, grok_total,\n",
    "            shifts[0], shifts[1], shifts[2], shifts[3], shifts[4],  # individual bucket shifts\n",
    "            weighted_shift  # overall weighted shift\n",
    "        ))\n",
    "    \n",
    "    return article_bucket_shifts\n",
    "\n",
    "bucket_columns = [\n",
    "    'title',\n",
    "    'wp_bucket_0_0.2', 'wp_bucket_0.2_0.4', 'wp_bucket_0.4_0.6', 'wp_bucket_0.6_0.8', 'wp_bucket_0.8_1.0', 'wp_other', 'wp_total',\n",
    "    'grok_bucket_0_0.2', 'grok_bucket_0.2_0.4', 'grok_bucket_0.4_0.6', 'grok_bucket_0.6_0.8', 'grok_bucket_0.8_1.0', 'grok_other', 'grok_total',\n",
    "    'shift_bucket_0_0.2', 'shift_bucket_0.2_0.4', 'shift_bucket_0.4_0.6', 'shift_bucket_0.6_0.8', 'shift_bucket_0.8_1.0',\n",
    "    'weighted_shift'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_maximals_lin = find_reliability_shift_by_buckets(lin_reliability)\n",
    "max_reliability_shift_df_lin = pd.DataFrame(article_maximals_lin, columns=bucket_columns)\n",
    "article_maximals_w_license_lin = find_reliability_shift_by_buckets(lin_reliability, fsuffix='_domains_w_license.json')\n",
    "max_reliability_shift_df_w_license_lin = pd.DataFrame(article_maximals_w_license_lin, columns=bucket_columns)\n",
    "article_maximals_wo_license_lin = find_reliability_shift_by_buckets(lin_reliability, fsuffix='_domains_wo_license.json')\n",
    "max_reliability_shift_df_wo_license_lin = pd.DataFrame(article_maximals_wo_license_lin, columns=bucket_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ce934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reliability_bucket_charts(filtered_df, fsuffix='', show=True, title=None):\n",
    "    \"\"\"\n",
    "    Plots stacked bar + diagonal comparison charts using reliability score buckets:\n",
    "    1. Stacked bar and overlay: Proportion of sources in each reliability bucket for Wikipedia and Grokipedia, with diagonal fills illustrating change.\n",
    "    2. Bar chart: Percentage of articles containing at least one source in each bucket for Wikipedia and Grokipedia.\n",
    "\n",
    "    Parameters:\n",
    "        filtered_df (pd.DataFrame): DataFrame with bucket columns (from find_reliability_shift_by_buckets)\n",
    "        fsuffix (str): Suffix for output filename\n",
    "        show (bool): If True, calls plt.show() at end.\n",
    "        title (str): Optional custom title for the whole figure. If None, uses default title.\n",
    "    \"\"\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    from matplotlib.patches import Patch\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    # --- Setup buckets, labels, colors ---\n",
    "    bucket_labels = ['0.0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0', 'other']\n",
    "    display_names = {\n",
    "        '0.0-0.2': '0.0-0.2',\n",
    "        '0.2-0.4': '0.2-0.4',\n",
    "        '0.4-0.6': '0.4-0.6',\n",
    "        '0.6-0.8': '0.6-0.8',\n",
    "        '0.8-1.0': '0.8-1.0',\n",
    "        'other': 'No score'\n",
    "    }\n",
    "    \n",
    "    # Create color map: green (for 1.0), yellow (for 0.5), red (for 0.0), gray for 'other'\n",
    "    from matplotlib.colors import to_hex, LinearSegmentedColormap\n",
    "    \n",
    "    # Create a green-yellow-red colormap, where 1.0 is green, 0.5 is yellow, 0.0 is red\n",
    "    spect_cmap = LinearSegmentedColormap.from_list(\n",
    "        \"green_yellow_red\", [(0.0, \"#D73027\"), (0.5, \"#FEE08B\"), (1.0, \"#1A9850\")]  # red, yellow, green\n",
    "    )\n",
    "    \n",
    "    # Map bucket labels to their midpoint values for colormap\n",
    "    bucket_midpoints = {\n",
    "        '0.0-0.2': 0.1,  # Red end\n",
    "        '0.2-0.4': 0.3,  # Red-yellow transition\n",
    "        '0.4-0.6': 0.5,  # Yellow (middle)\n",
    "        '0.6-0.8': 0.7,  # Yellow-green transition\n",
    "        '0.8-1.0': 0.9,  # Green end\n",
    "    }\n",
    "    \n",
    "    # Generate colors for each bucket using the colormap\n",
    "    color_map = {}\n",
    "    for bucket_label in bucket_labels:\n",
    "        if bucket_label == 'other':\n",
    "            color_map[bucket_label] = 'grey'\n",
    "        else:\n",
    "            midpoint = bucket_midpoints[bucket_label]\n",
    "            color_map[bucket_label] = to_hex(spect_cmap(midpoint))\n",
    "\n",
    "    # --- Aggregate counts as \"wp\" and \"grok\" bucket table ---\n",
    "    # Map bucket labels to actual column name suffixes\n",
    "    bucket_to_col = {\n",
    "        '0.0-0.2': '0_0.2',\n",
    "        '0.2-0.4': '0.2_0.4',\n",
    "        '0.4-0.6': '0.4_0.6',\n",
    "        '0.6-0.8': '0.6_0.8',\n",
    "        '0.8-1.0': '0.8_1.0'\n",
    "    }\n",
    "    \n",
    "    agg = {}\n",
    "    for bucket_label in bucket_labels:\n",
    "        if bucket_label == 'other':\n",
    "            agg[f'wp_{bucket_label}'] = filtered_df['wp_other'].sum()\n",
    "            agg[f'grok_{bucket_label}'] = filtered_df['grok_other'].sum()\n",
    "        else:\n",
    "            # Use the correct column name format\n",
    "            col_suffix = bucket_to_col[bucket_label]\n",
    "            agg[f'wp_{bucket_label}'] = filtered_df[f'wp_bucket_{col_suffix}'].sum()\n",
    "            agg[f'grok_{bucket_label}'] = filtered_df[f'grok_bucket_{col_suffix}'].sum()\n",
    "\n",
    "    # Make DF of shape: index=['Wikipedia', 'Grokipedia'], columns=bucket_labels\n",
    "    wp_row = [agg[f'wp_{k}'] for k in bucket_labels]\n",
    "    grok_row = [agg[f'grok_{k}'] for k in bucket_labels]\n",
    "    prop_df = pd.DataFrame(\n",
    "        [wp_row, grok_row],\n",
    "        columns=bucket_labels,\n",
    "        index=['Wikipedia', 'Grokipedia']\n",
    "    )\n",
    "    prop_df_norm = prop_df.div(prop_df.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    # --- Plotting stacked bars with diagonal overlays ---\n",
    "    labels = ['Wikipedia', 'Grokipedia']\n",
    "    x = np.arange(len(labels))\n",
    "    bar_sep = 0.09\n",
    "    width = 0.18\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 7), gridspec_kw={'width_ratios': [1, 1.7]})\n",
    "    ax = axs[0]\n",
    "\n",
    "    # Set up stacking\n",
    "    bottoms = [0, 0]\n",
    "    bars_wp = []\n",
    "    bars_grok = []\n",
    "\n",
    "    # For synchronized stacking, process in bucket order:\n",
    "    for j, bucket_label in enumerate(bucket_labels):\n",
    "        color = color_map.get(bucket_label, 'grey')\n",
    "        # WP bar proportions\n",
    "        wp_prop = prop_df_norm.loc['Wikipedia', bucket_label]\n",
    "        grok_prop = prop_df_norm.loc['Grokipedia', bucket_label]\n",
    "        bar_wp = ax.bar(x[0] - width/2 - bar_sep/2, wp_prop, width=width,\n",
    "                        bottom=bottoms[0], color=color, edgecolor='none', zorder=2)\n",
    "        bar_grok = ax.bar(x[1] + width/2 + bar_sep/2, grok_prop, width=width,\n",
    "                          bottom=bottoms[1], color=color, edgecolor='none', zorder=2)\n",
    "\n",
    "        # Diagonal change fill\n",
    "        wp_top = bottoms[0] + wp_prop\n",
    "        grok_top = bottoms[1] + grok_prop\n",
    "        ax.fill_between(\n",
    "            [x[0] - width/2, x[1] + width/2],\n",
    "            [wp_top, grok_top],\n",
    "            [bottoms[0], bottoms[1]],\n",
    "            color=color, alpha=0.25, zorder=1, linewidth=0\n",
    "        )\n",
    "        bars_wp.append(bar_wp)\n",
    "        bars_grok.append(bar_grok)\n",
    "        bottoms[0] += wp_prop\n",
    "        bottoms[1] += grok_prop\n",
    "\n",
    "    # Set axis ticks and labels\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, fontsize=16)\n",
    "    ax.set_ylabel(\"Proportion of Citations\", fontsize=16)\n",
    "    ax.set_title(\"Source Reliability Score Proportion: Wikipedia vs Grokipedia\", fontsize=18)\n",
    "\n",
    "    # Make axis tight with bars\n",
    "    ax.set_xlim(-0.23, 1.23)\n",
    "    ax.set_ylim(bottom=0, top=1.01)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "\n",
    "    # Custom legend patch (color by bucket) - reversed order - move inside plot to reduce whitespace\n",
    "    legend_elements = [Patch(facecolor=color_map.get(bucket_label, 'grey'), label=display_names.get(bucket_label, bucket_label), alpha=0.55) for bucket_label in reversed(bucket_labels)]\n",
    "    ax.legend(handles=legend_elements, title='Reliability Score', loc='upper center', framealpha=0.9, fontsize=16)\n",
    "\n",
    "    # Set figure title (for whole figure)\n",
    "    plot_title = title if title is not None else \"Source Reliability Score Proportion: Wikipedia vs Grokipedia\"\n",
    "    fig.suptitle(plot_title, fontsize=16, y=0.98)\n",
    "    \n",
    "    # Tighten subplot spacing to reduce whitespace (leave room for suptitle)\n",
    "    fig.subplots_adjust(wspace=0.15, left=0.05, right=0.97, top=0.88, bottom=0.1)\n",
    "\n",
    "    # ---- New plot: % of articles containing at least 1 in each bucket ----\n",
    "    ax2 = axs[1]\n",
    "    main_buckets = ['0.0-0.2', '0.2-0.4', '0.4-0.6', '0.6-0.8', '0.8-1.0']\n",
    "    bucket_display_labels = [display_names[b] for b in main_buckets]\n",
    "    bar_x = np.arange(len(main_buckets))\n",
    "    bar_width = 0.36\n",
    "\n",
    "    n_articles = len(filtered_df)\n",
    "    percentages = {'Wikipedia': [], 'Grokipedia': []}\n",
    "    # Print the number of articles containing at least one 0-0.2-reliability citation for Wikipedia and Grokipedia\n",
    "    wp_0_0_2_col = 'wp_bucket_0_0.2'\n",
    "    grok_0_0_2_col = 'grok_bucket_0_0.2'\n",
    "    wp_0_0_2_num = (filtered_df[wp_0_0_2_col] > 0).sum()\n",
    "    grok_0_0_2_num = (filtered_df[grok_0_0_2_col] > 0).sum()\n",
    "    print(f\"Number of articles with at least one 0-0.2 source: Wikipedia: {wp_0_0_2_num}, Grokipedia: {grok_0_0_2_num}\")\n",
    "\n",
    "    # Print the number of pages (rows) with a 0-0.2 source for Wikipedia and for Grokipedia\n",
    "    print(f\"Number of pages with a 0-0.2 source in Wikipedia: {wp_0_0_2_num}\")\n",
    "    print(f\"Number of pages with a 0-0.2 source in Grokipedia: {grok_0_0_2_num}\")\n",
    "\n",
    "    for bucket_label in main_buckets:\n",
    "        # Use the correct column name format\n",
    "        col_suffix = bucket_to_col[bucket_label]\n",
    "        wp_col = f'wp_bucket_{col_suffix}'\n",
    "        grok_col = f'grok_bucket_{col_suffix}'\n",
    "        wp_count = (filtered_df[wp_col] > 0).sum()\n",
    "        grok_count = (filtered_df[grok_col] > 0).sum()\n",
    "        percentages['Wikipedia'].append(wp_count / n_articles * 100)\n",
    "        percentages['Grokipedia'].append(grok_count / n_articles * 100)\n",
    "\n",
    "    # Plot as side-by-side bars with blue/red scheme (same as reliability chart)\n",
    "    wp_color = \"#4977bc\"  # Blue for Wikipedia\n",
    "    grok_color = \"#e86b54\"  # Red for Grokipedia\n",
    "    ax2.bar(bar_x - bar_width/2, percentages['Wikipedia'], bar_width,\n",
    "           label='Wikipedia', color=wp_color, edgecolor='black', alpha=0.7)\n",
    "    ax2.bar(bar_x + bar_width/2, percentages['Grokipedia'], bar_width,\n",
    "           label='Grokipedia', color=grok_color, edgecolor='black', alpha=0.7)\n",
    "\n",
    "    for i, (wp, gk) in enumerate(zip(percentages['Wikipedia'], percentages['Grokipedia'])):\n",
    "        ax2.text(i - bar_width/2, wp + 1, f\"{wp:.1f}%\", ha='center', va='bottom', fontsize=10, color=\"#10426b\")\n",
    "        ax2.text(i + bar_width/2, gk + 1, f\"{gk:.1f}%\", ha='center', va='bottom', fontsize=10, color=\"#7a230c\")\n",
    "\n",
    "    ax2.set_xticks(bar_x)\n",
    "    ax2.set_xticklabels(bucket_display_labels, rotation=14, fontsize=16)\n",
    "    ax2.set_ylim(0, 105)\n",
    "    ax2.set_ylabel(\"Percent of Articles\", fontsize=16)\n",
    "    ax2.set_title(\"% of Articles Citing Any Source in Reliability Bucket\", fontsize=18)\n",
    "    ax2.legend(loc='upper right', fontsize=16)\n",
    "    ax2.grid(axis='y', linestyle=':', alpha=0.4)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f'../graphics/overall_grok_wp_cite_composition_lin_{fsuffix}.pdf')\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b879c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reliability_bucket_charts(max_reliability_shift_df_lin, title='Source Reliability Score Comparison (all articles)')\n",
    "plot_reliability_bucket_charts(max_reliability_shift_df_w_license_lin, fsuffix='_w_license', title='Source Reliability Score Comparison (all articles with CC-license)')\n",
    "plot_reliability_bucket_charts(max_reliability_shift_df_wo_license_lin, fsuffix='_wo_license', title='Source Reliability Score Comparison (all articles without CC-license)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0298f40",
   "metadata": {},
   "source": [
    "## Examining Book Citation Patterns in Grokipedia Articles\n",
    "\n",
    "Finding articles with book citations and examining their reference structure.\n",
    "\n",
    "\n",
    "(tktk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f87adc6",
   "metadata": {},
   "source": [
    "## Getting Twitter / X usernames that were cited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a9a4a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_object_at_offset(path, offset):\n",
    "    if offset is None:\n",
    "        raise ValueError(\"Offset must not be None. Check the index lookup.\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        f.seek(offset)\n",
    "        line = f.readline()\n",
    "        return json.loads(line)\n",
    "\n",
    "# Patterns as before\n",
    "SITE_PATTERNS = {\n",
    "    \"twitter.com\": re.compile(r\"https?://(?:www\\.)?(?:twitter\\.com|x\\.com)/([A-Za-z0-9_]{1,15})(?:[/?#]|$)\"),\n",
    "    \"x.com\": re.compile(r\"https?://(?:www\\.)?x\\.com/([A-Za-z0-9_]{1,15})(?:[/?#]|$)\"),\n",
    "    \"facebook.com\": re.compile(r\"https?://(?:www\\.)?facebook\\.com/(?!(pages|groups|events|marketplace|watch|gaming|live|photo\\.php)(?:/|$))([A-Za-z0-9_.]{5,})(?:[/?#]|$)\"),\n",
    "    \"youtube.com\": re.compile(\n",
    "        r\"https?://(?:www\\.)?youtube\\.com/(?:(?:user|c|channel)/([A-Za-z0-9._\\-]{1,64}))(?:[/?#]|$)\"\n",
    "    ),\n",
    "    \"youtu.be\": re.compile(r\"https?://youtu\\.be/([A-Za-z0-9_\\-]{6,})(?:[/?#]|$)\"),\n",
    "    \"reddit.com\": re.compile(r\"https?://(?:www\\.)?reddit\\.com/user/([A-Za-z0-9_\\-]{1,20})(?:[/?#]|$)\"),\n",
    "    \"instagram.com\": re.compile(r\"https?://(?:www\\.)?instagram\\.com/([A-Za-z0-9_.]{1,30})(?:[/?#]|$)\"),\n",
    "    \"tiktok.com\": re.compile(r\"https?://(?:www\\.)?tiktok\\.com/@([A-Za-z0-9_.]{1,30})(?:[/?#]|$)\"),\n",
    "    \"truthsocial.com\": re.compile(r\"https?://(?:www\\.)?truthsocial\\.com/@([A-Za-z0-9_.]{1,30})(?:[/?#]|$)\"),\n",
    "    \"bsky.app\": re.compile(r\"https?://(?:www\\.)?bsky\\.app/profile/([A-Za-z0-9_.\\-]+)(?:[/?#]|$)\"),\n",
    "    \"pinterest.com\": re.compile(r\"https?://(?:www\\.)?pinterest\\.com/([A-Za-z0-9_/.\\-]{3,})/?(?:[/?#]|$)\"),\n",
    "}\n",
    "ALL_PATTERNS = [(name, pat) for name, pat in SITE_PATTERNS.items()]\n",
    "\n",
    "# Add patterns for shareable AI links\n",
    "SHAREABLE_AI_PATTERNS = {\n",
    "    # ChatGPT links look like: https://chatgpt.com/share/<uuid>\n",
    "    \"chatgpt.com\": re.compile(\n",
    "        r\"(?:https?://)?(?:www\\.)?chatgpt\\.com/share/([0-9a-fA-F\\-]{10,})\"\n",
    "    ),\n",
    "    # Claude links look like: https://claude.ai/share/<uuid>\n",
    "    \"claude.ai\": re.compile(\n",
    "        r\"(?:https?://)?(?:www\\.)?claude\\.ai/share/([0-9a-fA-F\\-]{10,})\"\n",
    "    ),\n",
    "    # You can add more patterns for other generative AI share links here\n",
    "    \"chat.deepseek.com\": re.compile(\n",
    "        r\"(?:https?://)?(?:www\\.)?chat\\.deepseek\\.com/share/([0-9a-fA-F\\-]{10,})\"\n",
    "    ),\n",
    "    \"twitter.com\": re.compile(\n",
    "        r\"(?:https?://)?(?:www\\.)?twitter\\.com/i/grok/share/([0-9a-fA-F\\-]{10,})\"\n",
    "    ),\n",
    "    \"x.com\": re.compile(\n",
    "        r\"(?:https?://)?(?:www\\.)?x\\.com/i/grok/share/([0-9a-fA-F\\-]{10,})\"\n",
    "    ),\n",
    "    \"perplexity.ai\": re.compile(\n",
    "        r\"(?:https?://)?(?:www\\.)?perplexity\\.ai/search/([0-9a-fA-F\\-]{10,})\"\n",
    "    ),\n",
    "    \"copilot.microsoft.com\": re.compile(\n",
    "        r\"(?:https?://)?(?:www\\.)?copilot\\.microsoft\\.com/shares/([0-9a-fA-F\\-]{10,})\"\n",
    "    )\n",
    "}\n",
    "ALL_SHARE_PATTERNS = [(name, pat) for name, pat in SHAREABLE_AI_PATTERNS.items()]\n",
    "\n",
    "def extract_ai_share_links(urls, print_debug=False):\n",
    "    \"\"\"\n",
    "    Extract AI share links from a list of URLs.\n",
    "    Also extracts Twitter/X links where the username is \"grok\".\n",
    "    \n",
    "    Args:\n",
    "        urls: List of URL strings\n",
    "        print_debug: If True, print debug messages\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples: [(ai_site, share_id, url), ...]\n",
    "        For Twitter/X grok username links: (\"twitter.com/grok\", username, url) or (\"x.com/grok\", username, url)\n",
    "    \"\"\"\n",
    "    ai_shares = []\n",
    "    # Pattern for Twitter/X usernames (for profile links, not share links)\n",
    "    twitter_x_pattern = re.compile(r\"https?://(?:www\\.)?(?:twitter\\.com|x\\.com)/([A-Za-z0-9_]{1,15})(?:[/?#]|$)\")\n",
    "    \n",
    "    for url in urls:\n",
    "        if not url:\n",
    "            continue\n",
    "        \n",
    "        # Track if we found a share link for this URL\n",
    "        found_share_link = False\n",
    "        \n",
    "        # Check for AI share links first (prioritize share links over profile links)\n",
    "        for ai_site, ai_pat in ALL_SHARE_PATTERNS:\n",
    "            # Extract the domain from the ai_site key (e.g., \"chatgpt.com\" -> \"chatgpt.com\")\n",
    "            domain = ai_site.split('/')[0]\n",
    "            # Check if the domain is in the URL and try to match the pattern\n",
    "            if domain in url:\n",
    "                ai_match = ai_pat.search(url)\n",
    "                if ai_match:\n",
    "                    share_id = ai_match.group(1)\n",
    "                    ai_shares.append((ai_site, share_id, url))\n",
    "                    found_share_link = True\n",
    "                    if print_debug:\n",
    "                        print(f\"    [DEBUG] Found AI share: {ai_site}, id: {share_id}, url: {url}\")\n",
    "                    # Break after finding a match to avoid duplicate entries\n",
    "                    break\n",
    "        \n",
    "        # Only check for Twitter/X profile links where username is \"grok\" if we didn't find a share link\n",
    "        if not found_share_link and (\"twitter.com\" in url or \"x.com\" in url):\n",
    "            # Make sure it's not a share link path\n",
    "            if \"/i/grok/share/\" not in url and \"/i/share/\" not in url:\n",
    "                match = twitter_x_pattern.search(url)\n",
    "                if match:\n",
    "                    username = match.group(1)\n",
    "                    if username.lower() == \"grok\":\n",
    "                        # Determine which site (twitter.com or x.com)\n",
    "                        site_key = \"twitter.com/grok\" if \"twitter.com\" in url else \"x.com/grok\"\n",
    "                        ai_shares.append((site_key, username, url))\n",
    "                        if print_debug:\n",
    "                            print(f\"    [DEBUG] Found Grok username link: {site_key}, username: {username}, url: {url}\")\n",
    "    \n",
    "    return ai_shares\n",
    "\n",
    "def get_ai_share_links_only(domains_data, idx_path, data_path, test_mode=False, print_debug=False):\n",
    "    \"\"\"\n",
    "    Extract only AI share links from articles, without extracting social usernames.\n",
    "    \n",
    "    Args:\n",
    "        domains_data: loaded from *_domains.json [{title: {...}}, ...]\n",
    "        idx_path: path to cached index\n",
    "        data_path: article data file (either grokipedia or wikipedia)\n",
    "        test_mode: if True, only process the first 1000 articles for a quick test.\n",
    "        print_debug: if True, print debug messages\n",
    "    \n",
    "    Returns:\n",
    "        dict mapping titles to [(ai_site, share_id, url), ...] for AI share links.\n",
    "    \"\"\"\n",
    "    print(f\"Opening index file: {idx_path}\")\n",
    "    with open(idx_path, \"rb\") as f:\n",
    "        idx_data = pickle.load(f)\n",
    "    print(f\"Index loaded. Number of entries: {len(idx_data)}\")\n",
    "\n",
    "    # Flatten input to [(title, domains)] list\n",
    "    if isinstance(domains_data, dict):\n",
    "        items_list = list(domains_data.items())\n",
    "    else:\n",
    "        items_list = []\n",
    "        for d in domains_data:\n",
    "            for title, domains in d.items():\n",
    "                items_list.append((title, domains))\n",
    "    print(f\"items_list created with {len(items_list)} articles\")\n",
    "\n",
    "    ai_shares_by_title = defaultdict(list)  # {title: [(ai_site, share_id, url), ...]}\n",
    "\n",
    "    iter_items = items_list\n",
    "    bar_length = min(1000, len(items_list)) if test_mode else len(items_list)\n",
    "    with tqdm(total=bar_length, desc=\"Extracting AI share links\", unit=\"article\") as pbar:\n",
    "        for i, (title_orig, domains) in enumerate(iter_items):\n",
    "            if test_mode and i >= 1000:\n",
    "                print(\"Test mode enabled. Breaking after 1000 records.\")\n",
    "                break\n",
    "\n",
    "            title_lookup = title_orig.replace(\"_\", \" \").lower()\n",
    "            title_for_result = title_lookup\n",
    "\n",
    "            # Only check for AI share domains, not social domains\n",
    "            relevant = any(any(ai_dom in dom for ai_dom in SHAREABLE_AI_PATTERNS.keys()) for dom in domains)\n",
    "            if not relevant:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            if i % 10000 == 0 and i != 0:\n",
    "                print(f\"Processing index {i} / {len(items_list)}: '{title_for_result}'...\")\n",
    "\n",
    "            line_idx = idx_data.get(title_orig)\n",
    "            if line_idx is None:\n",
    "                line_idx = idx_data.get(title_lookup)\n",
    "            if line_idx is None:\n",
    "                if print_debug:\n",
    "                    print(f\"  WARNING: No index for title '{title_for_result}'\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ref = get_object_at_offset(data_path, line_idx)\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not fetch article for '{title_for_result}' (offset: {line_idx}): {e}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            urls = []\n",
    "\n",
    "            # This follows the extraction logic in 05_preprocessing.ipynb\n",
    "            if 'wikipedia' in data_path:\n",
    "                references = ref.get(\"references\", [])\n",
    "                if i < 5 and print_debug:\n",
    "                    print(f\"  [DEBUG] First refs for '{title_for_result}': {references[:2]}\")\n",
    "                for single_ref in references:\n",
    "                    metadata = single_ref.get(\"metadata\") if isinstance(single_ref, dict) else None\n",
    "                    url = metadata.get(\"url\") if isinstance(metadata, dict) else None\n",
    "                    if url:\n",
    "                        urls.append(url)\n",
    "            else:\n",
    "                references = ref.get(\"data\", {}).get(\"references\", [])\n",
    "                if i < 5 and print_debug:\n",
    "                    print(f\"  [DEBUG] First grok refs for '{title_for_result}': {references[:2]}\")\n",
    "                for single_ref in references:\n",
    "                    link = single_ref.get(\"link\") if isinstance(single_ref, dict) else None\n",
    "                    href = link.get(\"href\") if isinstance(link, dict) else None\n",
    "                    if href:\n",
    "                        urls.append(href)\n",
    "\n",
    "            if i < 5 and print_debug:\n",
    "                print(f\"  [DEBUG] Extracted URLs for '{title_for_result}': {urls[:5]}\")\n",
    "\n",
    "            # Extract AI share links only\n",
    "            ai_shares = extract_ai_share_links(urls, print_debug=print_debug)\n",
    "            for ai_site, share_id, url in ai_shares:\n",
    "                ai_shares_by_title[title_for_result].append((ai_site, share_id, url))\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Total articles with any AI share links: {len(ai_shares_by_title)}\")\n",
    "    return dict(ai_shares_by_title)\n",
    "\n",
    "# Usernames known to be spurious, e.g. from X/Twitter link shortener\n",
    "SPURIOUS_X_USERNAMES = {\n",
    "    \"PRODU\", 'search'\n",
    "}\n",
    "\n",
    "# Values considered NOT real usernames -- skip these by site (case-sensitive)\n",
    "SKIP_FACEBOOK_USERNAMES = set([\n",
    "    \"photo.php\", 'story.php', 'photo', 'media', 'notes', 'business', 'permalink.php', 'search'\n",
    "])\n",
    "# Only numbers (facebook.com/123...), very likely not a real username\n",
    "FACEBOOK_NUMERIC_RE = re.compile(r'^\\d+$')\n",
    "\n",
    "SKIP_INSTAGRAM_USERNAMES = set([\n",
    "    \"reel\", 'tv'\n",
    "])\n",
    "# Skip youtube channel IDs like \"UCtWQDzuH1e84SebEyZN_aXw\": channel IDs always start with \"UC\" and are 24 chars\n",
    "YOUTUBE_CHANNEL_ID_RE = re.compile(r\"^UC[\\w-]{22}$\")\n",
    "\n",
    "# X.com \"i\" pseudo-username (used for i/grok/share/ and others: always skip these, and possibly log for debug)\n",
    "SKIP_X_USERNAMES = set([\n",
    "    \"i\",\n",
    "])\n",
    "\n",
    "def get_social_usernames_with_counts(domains_data, idx_path, data_path, test_mode=False, print_debug=False):\n",
    "    \"\"\"\n",
    "    Given:\n",
    "        domains_data: loaded from *_domains.json [{title: {...}}, ...]\n",
    "        idx_path: path to cached index\n",
    "        data_path: article data file (either grokipedia or wikipedia)\n",
    "        test_mode: if True, only process the first 1000 articles for a quick test.\n",
    "    Returns:\n",
    "        dict mapping titles to {site: Counter({username: count, ...}), ...} for user/social platforms.\n",
    "        Also returns (ai_shares_by_title, skipped_x_i): ai_shares_by_title is {title: [(ai_site, share_id, url), ...]}\n",
    "    \"\"\"\n",
    "    print(f\"Opening index file: {idx_path}\")\n",
    "    with open(idx_path, \"rb\") as f:\n",
    "        idx_data = pickle.load(f)\n",
    "    print(f\"Index loaded. Number of entries: {len(idx_data)}\")\n",
    "\n",
    "    # Flatten input to [(title, domains)] list\n",
    "    if isinstance(domains_data, dict):\n",
    "        items_list = list(domains_data.items())\n",
    "    else:\n",
    "        items_list = []\n",
    "        for d in domains_data:\n",
    "            for title, domains in d.items():\n",
    "                items_list.append((title, domains))\n",
    "    print(f\"items_list created with {len(items_list)} articles\")\n",
    "\n",
    "    results = {}\n",
    "    skipped_x_i = []  # For optional debug reporting\n",
    "    ai_shares_by_title = defaultdict(list)  # New: {title: [(ai_site, share_id, url), ...]}\n",
    "\n",
    "    iter_items = items_list\n",
    "    bar_length = min(1000, len(items_list)) if test_mode else len(items_list)\n",
    "    with tqdm(total=bar_length, desc=\"Extracting social usernames\", unit=\"article\") as pbar:\n",
    "        for i, (title_orig, domains) in enumerate(iter_items):\n",
    "            if test_mode and i >= 1000:\n",
    "                print(\"Test mode enabled. Breaking after 1000 records.\")\n",
    "                break\n",
    "\n",
    "            title_lookup = title_orig.replace(\"_\", \" \").lower()\n",
    "            title_for_result = title_lookup\n",
    "\n",
    "            relevant = any(dom in SITE_PATTERNS or any(ai_dom in dom for ai_dom in SHAREABLE_AI_PATTERNS.keys()) for dom in domains)\n",
    "            if not relevant:\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            if i % 10000 == 0 and i != 0:\n",
    "                print(f\"Processing index {i} / {len(items_list)}: '{title_for_result}'...\")\n",
    "\n",
    "            line_idx = idx_data.get(title_orig)\n",
    "            if line_idx is None:\n",
    "                line_idx = idx_data.get(title_lookup)\n",
    "            if line_idx is None:\n",
    "                if print_debug:\n",
    "                    print(f\"  WARNING: No index for title '{title_for_result}'\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ref = get_object_at_offset(data_path, line_idx)\n",
    "            except Exception as e:\n",
    "                print(f\"  ERROR: Could not fetch article for '{title_for_result}' (offset: {line_idx}): {e}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "\n",
    "            urls = []\n",
    "\n",
    "            # This follows the extraction logic in 05_preprocessing.ipynb\n",
    "            if 'wikipedia' in data_path:\n",
    "                references = ref.get(\"references\", [])\n",
    "                if i < 5 and print_debug:\n",
    "                    print(f\"  [DEBUG] First refs for '{title_for_result}': {references[:2]}\")\n",
    "                for single_ref in references:\n",
    "                    metadata = single_ref.get(\"metadata\") if isinstance(single_ref, dict) else None\n",
    "                    url = metadata.get(\"url\") if isinstance(metadata, dict) else None\n",
    "                    if url:\n",
    "                        urls.append(url)\n",
    "            else:\n",
    "                references = ref.get(\"data\", {}).get(\"references\", [])\n",
    "                if i < 5 and print_debug:\n",
    "                    print(f\"  [DEBUG] First grok refs for '{title_for_result}': {references[:2]}\")\n",
    "                for single_ref in references:\n",
    "                    link = single_ref.get(\"link\") if isinstance(single_ref, dict) else None\n",
    "                    href = link.get(\"href\") if isinstance(link, dict) else None\n",
    "                    if href:\n",
    "                        urls.append(href)\n",
    "\n",
    "            if i < 5 and print_debug:\n",
    "                print(f\"  [DEBUG] Extracted URLs for '{title_for_result}': {urls[:5]}\")\n",
    "\n",
    "            usernames_by_site = defaultdict(Counter)\n",
    "            # === Extract usernames from general social patterns ===\n",
    "            for url in urls:\n",
    "                if not url:\n",
    "                    continue\n",
    "                # Social sites block\n",
    "                for site, pat in ALL_PATTERNS:\n",
    "                    if site in url:\n",
    "                        match = pat.search(url)\n",
    "                        if match:\n",
    "                            # Disallow youtube.com/watch etc. as username\n",
    "                            if site == \"youtube.com\" and \"watch\" in url and not \"/user/\" in url and not \"/c/\" in url and not \"/channel/\" in url:\n",
    "                                continue\n",
    "                            # Disallow instagram.com/p/ as usernames\n",
    "                            if site == \"instagram.com\" and re.search(r\"/p/[^/?#]+\", url):\n",
    "                                continue\n",
    "                            # facebook.com pattern has group 2, rest have group 1\n",
    "                            if site == \"facebook.com\" and match.lastindex and match.lastindex > 1:\n",
    "                                username = match.group(2)\n",
    "                            else:\n",
    "                                username = match.group(1)\n",
    "\n",
    "                            # Skip site-specific username types\n",
    "                            if site == \"facebook.com\":\n",
    "                                if username in SKIP_FACEBOOK_USERNAMES:\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped facebook.com username '{username}' in URL: {url}\")\n",
    "                                    continue\n",
    "                                if FACEBOOK_NUMERIC_RE.match(username):\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped numeric facebook.com username '{username}' in URL: {url}\")\n",
    "                                    continue\n",
    "                            elif site == \"instagram.com\":\n",
    "                                if username in SKIP_INSTAGRAM_USERNAMES:\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped instagram.com username '{username}' in URL: {url}\")\n",
    "                                    continue\n",
    "                            elif site == \"youtube.com\":\n",
    "                                if YOUTUBE_CHANNEL_ID_RE.match(username):\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped youtube.com channel ID '{username}' in URL: {url}\")\n",
    "                                    continue\n",
    "                            elif site in {\"x.com\", \"twitter.com\"}:\n",
    "                                if username in SKIP_X_USERNAMES:\n",
    "                                    skipped_x_i.append((username, url, title_for_result))\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped x.com username '{username}' in URL: {url}\")\n",
    "                                    continue\n",
    "                                if username in SPURIOUS_X_USERNAMES:\n",
    "                                    if print_debug:\n",
    "                                        print(f\"    [DEBUG] Skipped spurious {site} username '{username}' from URL: {url}\")\n",
    "                                    continue\n",
    "\n",
    "                            if username:\n",
    "                                usernames_by_site[site][username] += 1\n",
    "                                if print_debug:\n",
    "                                    print(f\"    [DEBUG] Matched {site} username '{username}' in URL: {url}\")\n",
    "\n",
    "            # Extract AI share links\n",
    "            ai_shares = extract_ai_share_links(urls, print_debug=print_debug)\n",
    "            for ai_site, share_id, url in ai_shares:\n",
    "                ai_shares_by_title[title_for_result].append((ai_site, share_id, url))\n",
    "\n",
    "            # Only save in results if found any usernames\n",
    "            if usernames_by_site and print_debug:\n",
    "                print(f\"  Found usernames for {title_for_result}: {[ (site, dict(counter)) for site, counter in usernames_by_site.items() if counter ]}\")\n",
    "                results[title_for_result] = {site: dict(counter) for site, counter in usernames_by_site.items() if counter}\n",
    "            elif usernames_by_site:\n",
    "                results[title_for_result] = {site: dict(counter) for site, counter in usernames_by_site.items() if counter}\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Total articles with any matching usernames: {len(results)}\")\n",
    "    print(f\"Total articles with any AI share links: {len(ai_shares_by_title)}\")\n",
    "    return results, skipped_x_i, dict(ai_shares_by_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18f35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/overall/domains/wp_domains.json', 'r') as f:\n",
    "    wp_domains = json.load(f)\n",
    "\n",
    "with open('../results/overall/domains/grok_domains.json', 'r') as f:\n",
    "    grok_domains = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04018c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_usernames_by_title, wp_grok_convo_links, wp_ai_shares_by_title = get_social_usernames_with_counts(wp_domains, '../results/overall/cached_wiki_idx.pkl', '../grokipedia_wikipedia_articles.ndjson')\n",
    "grok_usernames_by_title, grok_grok_convo_links, grok_ai_shares_by_title = get_social_usernames_with_counts(grok_domains, '../results/overall/cached_grok_idx.pkl', '../grokipedia_scrape.ndjson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "822e71eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening index file: ../results/overall/cached_grok_idx.pkl\n",
      "Index loaded. Number of entries: 880623\n",
      "items_list created with 883673 articles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  14%|        | 120371/883673 [00:36<03:44, 3399.83article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 120000 / 883673: 'korean canadians'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  20%|        | 180572/883673 [00:55<03:10, 3699.57article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 180000 / 883673: 'thomas sadoski'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  22%|       | 190573/883673 [00:58<03:26, 3357.35article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 190000 / 883673: 'clan macfarlane'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  26%|       | 230521/883673 [01:12<03:38, 2992.46article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 230000 / 883673: 'mirpur district'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  28%|       | 250636/883673 [01:19<03:07, 3368.46article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 250000 / 883673: 'idiotest'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  29%|       | 260516/883673 [01:22<03:02, 3411.03article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 260000 / 883673: 'she (tyler, the creator song)'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  34%|      | 300477/883673 [01:34<04:04, 2385.58article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 300000 / 883673: 'k. m. sachin dev'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  36%|      | 320539/883673 [01:40<02:33, 3668.73article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 320000 / 883673: 'roscoe dash'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  37%|      | 330435/883673 [01:44<03:26, 2676.70article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 330000 / 883673: 'megan davis'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  39%|      | 340446/883673 [01:47<02:36, 3476.04article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 340000 / 883673: 'incidents at six flags parks'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  51%|     | 450557/883673 [02:19<02:09, 3346.55article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 450000 / 883673: 'don't call me angel'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  58%|    | 510706/883673 [02:37<01:43, 3601.49article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 510000 / 883673: 'faky'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  63%|   | 560314/883673 [02:51<02:00, 2672.39article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 560000 / 883673: 'santiago creel'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  75%|  | 660304/883673 [03:19<01:07, 3296.31article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 660000 / 883673: 'groups rally'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links:  95%|| 840341/883673 [04:12<00:12, 3346.21article/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index 840000 / 883673: 'baby driver'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting AI share links: 100%|| 883673/883673 [04:24<00:00, 3336.40article/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles with any AI share links: 220\n"
     ]
    }
   ],
   "source": [
    "grok_ai_share_links = get_ai_share_links_only(grok_domains, '../results/overall/cached_grok_idx.pkl', '../grokipedia_scrape.ndjson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5411180f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ganesh chaturthi': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1959516819200659871')],\n",
       " 'grok (chatbot)': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1982383601292558751'),\n",
       "  ('x.com/grok', 'grok', 'https://x.com/grok/status/1978557968548581830'),\n",
       "  ('x.com/grok', 'grok', 'https://x.com/grok/status/1982454130246652232')],\n",
       " 'kamal haasan': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1934957329478566040')],\n",
       " 'barlas': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1971732953303847168')],\n",
       " 'the twelfth': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1939746368580637071')],\n",
       " 'eastern orthodoxy by country': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1978462021470552229')],\n",
       " 'nick fuentes': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1969964316276449649')],\n",
       " 'persecution of christians in the eastern bloc': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1951197167718461852')],\n",
       " 'mia khalifa': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1928097424834449510')],\n",
       " 'hakirya': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1933965769496563976'),\n",
       "  ('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1934314219081760892')],\n",
       " 'religion in germany': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1953731434159681984')],\n",
       " 'religion in the soviet union': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1951197167718461852')],\n",
       " 'krystal ball': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1967965110120653204')],\n",
       " 'ku klux klan members in united states politics': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1967142789222985741')],\n",
       " 'x.com': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1978120131751665841')],\n",
       " 'governor of abia state': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1908604716944158967')],\n",
       " 'governor of lagos state': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1905541603894100448')],\n",
       " 'anton lavey': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1967022117834420719')],\n",
       " 'abdirahman saylici': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1958541771862090051')],\n",
       " \"abdirahman bin isma'il al-jabarti\": [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1955482688305106992')],\n",
       " 'matt furie': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1913014594001604913')],\n",
       " 'mcfarlane toys': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1938694211165135177')],\n",
       " 'owen benjamin': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1973825520908607936')],\n",
       " 'kabaw valley': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1900603567557251437')],\n",
       " 'bbc scotland': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1953428030044725600')],\n",
       " 'sarocha chankimha': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1951167525619900631?lang=en'),\n",
       "  ('x.com/grok', 'grok', 'https://x.com/grok/status/1968648815424225543')],\n",
       " 'theories of jewish responsibility for wars': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1952236680192348583')],\n",
       " 'harvey milk day': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1930029847327453544?lang=en')],\n",
       " 'freedom, georgia': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1949835489927913820')],\n",
       " 'marileidy paulino': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1969111991219601655')],\n",
       " 'list of space programs of the united states': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1982510366946189704')],\n",
       " \"wesley girls' senior high school\": [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1953869312818876758')],\n",
       " 'east asia': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1970110806940635311')],\n",
       " 'list of terrorist incidents in 2023': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1949230946055762138')],\n",
       " 'akhtar raza khan': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1919357692902253057')],\n",
       " 'akinwunmi rhodes-vivour': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1965419251298443503')],\n",
       " 'ryan garcia': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1972461265365942406')],\n",
       " 'fc schalke 04': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1911822131912159334')],\n",
       " 'private spaceflight': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1912018017707528671')],\n",
       " 'azur lane': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1970646050093629485')],\n",
       " 'tutnese': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1961458828140319163')],\n",
       " 'popasna': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1978462966279532769')],\n",
       " 'dw news': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1963993056832889217')],\n",
       " 'david freiheit': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1904979934868619331')],\n",
       " 'jay leno': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1945997046231785862')],\n",
       " 'zyklon b': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1941081668259127662')],\n",
       " 'fbi criminal, cyber, response, and services branch': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1968180757664604176')],\n",
       " 'transgender pornography': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1906763975091425778')],\n",
       " 'tasmac': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1964719049486868494')],\n",
       " 'list of jewish american businesspeople in media': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1968623241641214086')],\n",
       " \"fred matiang'i\": [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1913903897175069071')],\n",
       " 'fred rwigyema': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1948629630061260975')],\n",
       " 'rotimi amaechi': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1954286566224076941')],\n",
       " 'n. chandrababu naidu': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1910977397098500170')],\n",
       " 'n. t. rama rao jr.': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1900958244694286523')],\n",
       " 'yorubaland': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1963630216829702501')],\n",
       " 'rui costa': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1967529670657884639')],\n",
       " 'jaipur metro': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1977927673839321109')],\n",
       " 'joshua ryne goldberg': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1955093356796920000')],\n",
       " 'tifariti': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1954867447376560580')],\n",
       " 'ikwerre people': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1900154184587997225')],\n",
       " 'list of current heads of state and government': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1954867447376560580')],\n",
       " 'bhutto family': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1929067102331351162')],\n",
       " 'adedeji adeleke': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1954260418408689940')],\n",
       " 'gleeden': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1903341193087570232')],\n",
       " 'destiny (streamer)': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1965210240854671552')],\n",
       " 'take-two interactive': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1969193578305110210')],\n",
       " 'demographics of zimbabwe': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1978698972287136143')],\n",
       " 'q into the storm': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1905426843353620851')],\n",
       " 'kenya revenue authority': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1979882553705369774')],\n",
       " 'kerala police': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1979839646252364142')],\n",
       " 'legal education': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1952348136703479995')],\n",
       " 'paganism': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1941185402179813593')],\n",
       " 'jeremy bertino': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1960720161075499236')],\n",
       " 'hasan piker': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1898550148474585573')],\n",
       " 'bimaal': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1959219668809601533')],\n",
       " 'israelitaly relations': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1968687915527192605')],\n",
       " 'free energy suppression claims': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1920463749917651043')],\n",
       " 'maya jayapal': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1974588553020760203')],\n",
       " 'vitaly zdorovetskiy': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1916717442631651363')],\n",
       " 'gore': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1949900771962876175')],\n",
       " 'pornography in japan': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1978855033962926388')],\n",
       " 'history of san diego': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1939170708472635766')],\n",
       " 'rabri devi': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1902166636649705733')],\n",
       " 'radhanite': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1949212429562642523')],\n",
       " 'maria farmer': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1963147387800789191')],\n",
       " 'm. k. stalin ministry': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1971456529867174156')],\n",
       " 'shilpi-gautam murder': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1979778066684797229')],\n",
       " 'steven pruitt': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1973563519154135198')],\n",
       " 'rapelay': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1980634826039496785')],\n",
       " 'indians in japan': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1953194492154257561')],\n",
       " 'mikoyan mig-29k': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1909902896839172172')],\n",
       " 'list of serving senior officers of the royal navy': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1944011261978763658')],\n",
       " 'ghana police service': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1977359676623958125')],\n",
       " 'cinema of malaysia': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1963311839372304887')],\n",
       " 'podhuvaga emmanasu thangam': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1960605116266111122')],\n",
       " 'pornographic film actor': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1960374644852322791')],\n",
       " 'ashir azeem': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1967128850955202602')],\n",
       " 'asaram': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1964298841508946003')],\n",
       " 'records of prime ministers of the united kingdom': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1968973259401199976')],\n",
       " 'tariq nasheed': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1952918628212593077')],\n",
       " 'colombian civil war': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1937308747497677133')],\n",
       " 'arab soldiers in the israel defense forces': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1940255658936553981')],\n",
       " 'death and state funeral of george h. w. bush': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1929338193650057671')],\n",
       " 'david caprio': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1950726135144427865')],\n",
       " 'david cole (journalist)': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1970045853202272492')],\n",
       " 'chatham house': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1968628466955469257')],\n",
       " 'che guevara mausoleum': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1964350615766888484')],\n",
       " 'james woods': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1949161834935996521'),\n",
       "  ('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1968918611088847006')],\n",
       " 'all progressives grand alliance': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1916757862925389901')],\n",
       " \"shrine of abu lu'lu'a\": [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1936907088963547591')],\n",
       " 'catturd': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1949175006405317116'),\n",
       "  ('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1980916074662265231')],\n",
       " 'toby turner': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1920665615729389758')],\n",
       " 'rubn rocha moya': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1970709472961274345')],\n",
       " 'robert j. shillman': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1970461459613393395')],\n",
       " 'habr awal': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1963463844162146673')],\n",
       " 'scottish nationalism': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1982453181130154374')],\n",
       " 'japanese people in germany': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1973360395692945649')],\n",
       " 'pericles abbasi': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1904679910406307946')],\n",
       " 'babu owino': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1956074680701628921')],\n",
       " 'the israel lobby and u.s. foreign policy': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1939772746420167141')],\n",
       " 'pakistanis': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1903820299251966300')],\n",
       " 'apoel fc': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1905358968462979583')],\n",
       " 'list of jewish heads of state and government': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1951912415144415435')],\n",
       " 'governor of rivers state': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1954286566224076941')],\n",
       " 'gordon g. chang': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1948352314894602442')],\n",
       " 'nigel farage': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1949142859237728627')],\n",
       " 'peter obi': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1916990308870672812'),\n",
       "  ('x.com/grok', 'grok', 'https://x.com/grok/status/1900279272268333294')],\n",
       " 'nematollah nassiri': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1937149910098604403')],\n",
       " 'eagle of saladin': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1936492956799943066')],\n",
       " 'itamar ben-gvir': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1950564760510218382')],\n",
       " 'height restriction laws': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1971578200376643623')],\n",
       " 'heiko von der leyen': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1967858369248755904')],\n",
       " 'arsenal f.c. supporters': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1949482244806226430')],\n",
       " 'nigerian youth movement': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1968065345559974067')],\n",
       " 'order of the niger': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1950223632640463137')],\n",
       " 'chilevisin': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1949915337497219263')],\n",
       " 'davido': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1967555637187162597')],\n",
       " 'indecent assault': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1953912766072586715')],\n",
       " 'james parry': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1913946765801431501')],\n",
       " 'anti-rape device': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1927797765901537425')],\n",
       " 'brian and ed krassenstein': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1977795089880715502')],\n",
       " \"maclean's\": [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1929317120179122211')],\n",
       " 'sardar vallabhbhai patel sports enclave': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1964906832125944154')],\n",
       " 'jamar k. walker': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1976678459289280705')],\n",
       " 'gulab raghunath patil': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1904604880934748591')],\n",
       " 'orji uzor kalu': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1957932424538780117')],\n",
       " 'murat boz': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1957395062171857196')],\n",
       " 'aston': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1976710318245155050')],\n",
       " 'clinton body count': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1946180154868555812')],\n",
       " 'nitish bharadwaj': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1970494548817387597')],\n",
       " 'transgender erotica awards': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1951373756619956357')],\n",
       " 'mai mala buni': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1970867001368432976')],\n",
       " 'sodomy': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1968518319507710060')],\n",
       " 'leafyishere': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1931781318431711669')],\n",
       " 'ladakh': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1973719603659542613')],\n",
       " 'ladakhis': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1973719603659542613')],\n",
       " 'transportation in mexico': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1920494021379829783')],\n",
       " 'tiong hiew king': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1980842095402447252')],\n",
       " 'ava max': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1941371937600700439')],\n",
       " 'bla bollobs': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1954693433324618199')],\n",
       " 'banu hanifa': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1911144784103903296')],\n",
       " 'pasupathy pandian': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1979460175674380580')],\n",
       " 'contrapoints': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1957718124557660562')],\n",
       " 'vanderbilt commodores': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1971714030156108115')],\n",
       " 'c. i. scofield': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1935641853644390787')],\n",
       " 'sachin pilot': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1921074954000625876')],\n",
       " 'stephanie vaquer': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1972073656319947092')],\n",
       " 'list of largest empires': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1963279910908338664')],\n",
       " 'shyam singha roy': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1964214537500184744')],\n",
       " 'little saint james, u.s. virgin islands': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1975583294243938565')],\n",
       " 'foreign fighters in the bosnian war': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1943979047186575841')],\n",
       " 'st. louis': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1959729175625089237')],\n",
       " 'avn award for male performer of the year': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1947357486853591094')],\n",
       " 'rajdeep sardesai': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1901254762198335816')],\n",
       " 'jadavpur lok sabha constituency': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1902973036812439878')],\n",
       " 'list of bars': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1958816526162866234')],\n",
       " 'ali zafar': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1977214026817122650')],\n",
       " 'sheng thao': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1948518393856098430')],\n",
       " 'zee news controversies': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1902346835680755820')],\n",
       " 'type 08': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1968406626194571748')],\n",
       " 'lauren witzke': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1973738905049665838')],\n",
       " 'nangeli': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1905438746872316154')],\n",
       " 'nadia marcinko': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1946596693627314533?lang=en')],\n",
       " 'race and crime': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1966828159950794791')],\n",
       " 'blue movie': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1938290692222324951'),\n",
       "  ('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1938290692222324951')],\n",
       " 'catherine kamau': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1975626927798112448')],\n",
       " 'ayla malik': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1941790436722643122')],\n",
       " 'list of mosques in the united kingdom': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1962475521385881982')],\n",
       " 'jewish american heritage month': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1932316161947165170')],\n",
       " 'jewish leadership': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1904401307852788076')],\n",
       " 'demographics of brussels': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1919803452718514517')],\n",
       " 'political communication': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1969238328211095828')],\n",
       " 'anonymous and the russo-ukrainian war': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1910417042068619384')],\n",
       " '1995 kodiyankulam violence': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1900121701196664972')],\n",
       " 'omar koshan': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1938152249236377615')],\n",
       " 'william vivour': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1967337638882652515'),\n",
       "  ('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1968310620962611333')],\n",
       " 'northsouth and eastwest corridor': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1978051586166579687')],\n",
       " 'harvey milk': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1930029847327453544?lang=en'),\n",
       "  ('x.com/grok', 'grok', 'https://x.com/grok/status/1930397885843288084')],\n",
       " 'makoto sakurai': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1964883617987108889')],\n",
       " 'gypsycrusader': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1965178052901789740'),\n",
       "  ('x.com/grok', 'grok', 'https://x.com/grok/status/1948494900791755263'),\n",
       "  ('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1957176299761606733'),\n",
       "  ('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1974553719334416802')],\n",
       " 'sadiq khan': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1971095973767393284')],\n",
       " 'michael rectenwald': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1966840807509684568')],\n",
       " 'jennifer carroll macneill': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1966953064218456429')],\n",
       " 'erika hilton': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1948184492218429877')],\n",
       " 'vkusno i tochka': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1962148910820122874')],\n",
       " 'alex rosen': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1898071163957395918')],\n",
       " 'dov s. zakheim': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1956406022723236109')],\n",
       " 'labour party (nigeria)': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1900909261653254418')],\n",
       " 'polcia de segurana pblica': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1977383249245798711')],\n",
       " 'china lobby in the united states': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1972456122071285927')],\n",
       " 'port': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1953035435858702638')],\n",
       " '8chan': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1958999145110544502')],\n",
       " 'president of kenya': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1980207621962678587')],\n",
       " 'bebe cool': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1951371876930035947')],\n",
       " 'rice christian': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1966482423774171498')],\n",
       " 'heathenry (new religious movement)': [('twitter.com/grok',\n",
       "   'grok',\n",
       "   'https://twitter.com/grok/status/1941185402179813593')],\n",
       " 'lateef jakande': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1954480051615060391')],\n",
       " 'russell martin (footballer)': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1961773042952265941')],\n",
       " 'obi cubana': [('x.com/grok',\n",
       "   'grok',\n",
       "   'https://x.com/grok/status/1931360143062778325')]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grok_ai_share_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bd2ff5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/overall/usernames/grok_twitter_convo_links_w_article.csv', 'w') as f:\n",
    "    for title, link in grok_ai_share_links.items():\n",
    "        for l in link:\n",
    "            f.write(f'{title},{l[-1]}\\n') if ',' not in title else f.write(f'\"{title}\",{l[-1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b263de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/wp_usernames_by_title.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"title\", \"site\", \"username\", \"count\"])\n",
    "    for title, site_dict in wp_usernames_by_title.items():\n",
    "        for site, usernames in site_dict.items():\n",
    "            for username, count in usernames.items():\n",
    "                writer.writerow([title, site, username, count])\n",
    "\n",
    "with open('../results/grok_usernames_by_title.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"title\", \"site\", \"username\", \"count\"])\n",
    "    for title, site_dict in grok_usernames_by_title.items():\n",
    "        for site, usernames in site_dict.items():\n",
    "            for username, count in usernames.items():\n",
    "                writer.writerow([title, site, username, count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e946cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = {}\n",
    "\n",
    "for (_, link, title) in grok_grok_convo_links:\n",
    "    if 'i/grok/share' in link:\n",
    "        if title not in pages:\n",
    "            pages[title] = set()\n",
    "\n",
    "        link_id = link.strip().split('/')[-1]\n",
    "        # Add link only if there is no other link in pages[title] with the same final part\n",
    "        if link_id not in {l.strip().split('/')[-1] for l in pages[title]}:\n",
    "            pages[title].add(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb052de",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/grok_grok_convo_links.txt', 'w') as f:\n",
    "    for page, links in pages.items():\n",
    "        for link in links:\n",
    "            f.write(f'{page},{link}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ff1661",
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_username_df = pd.read_csv('../results/wp_usernames_by_title.csv')\n",
    "grok_username_df = pd.read_csv('../results/grok_usernames_by_title.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c552a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i[1] for i in wp_grok_convo_links if 'i/grok/share' in i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a581c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "[i[1] for i in grok_grok_convo_links if 'i/communitynotes/share' in i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35097e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/grok_grok_convo_links.txt', 'w') as f:\n",
    "    for i in grok_grok_convo_links:\n",
    "        if 'i/grok/share' in i[1]:\n",
    "            f.write(f'{i[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_username_df = pd.merge(wp_username_df, grok_username_df, on=['title', 'site', 'username'], suffixes=('_wp', '_grok'), how='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21633b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_username_df['diff'] = combined_username_df['count_grok'] - combined_username_df['count_wp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee76244",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_username_df[combined_username_df['title'] == 'elon musk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f4d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_username_df.sort_values(by='diff', ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames_by_site_df = combined_username_df[['site', 'username', 'count_wp', 'count_grok', 'diff']].groupby(['site', 'username']).sum().reset_index().sort_values(by='count_grok', ascending=False)#.sort_values(by='diff', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acc73af",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames_by_site_df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a884538",
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames_by_site_df[usernames_by_site_df['site'].isin(['twitter.com', 'x.com'])][['username', 'count_wp', 'count_grok', 'diff']].groupby('username').sum().sort_values(by='diff', ascending=False)[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425807c6",
   "metadata": {},
   "source": [
    "### Analysis of fringe sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fringe_sites = {\n",
    "    'unz.com',\n",
    "    'vdare.com',\n",
    "    'frontpagemag.com',\n",
    "    'jihadwatch.org',\n",
    "    'lifesitenews.com',\n",
    "    'thegatewaypundit.com',\n",
    "    'globalresearch.ca',\n",
    "    'voltairenet.org',\n",
    "    'infowars.com',\n",
    "    'stormfront.org',\n",
    "}\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "fringe_rows = []\n",
    "for page in grok_domains:\n",
    "    title = list(page.keys())[0]\n",
    "    domains = list(page.values())[0]\n",
    "    for site in fringe_sites:\n",
    "        if site in domains:\n",
    "            fringe_rows.append({\n",
    "                'title': title,\n",
    "                'site': site,\n",
    "                'citations': domains[site]\n",
    "            })\n",
    "\n",
    "fringe_df = pd.DataFrame(fringe_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79011a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fringe_df.sort_values(by='citations', ascending=False).to_csv('../results/fringe_citations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe851d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_sites = {\n",
    "    'wikipedia.org',\n",
    "    'wiktionary.org',\n",
    "    'wikiquote.org',\n",
    "    'wikisource.org',\n",
    "    'wikiversity.org',\n",
    "    'wikivoyage.org',\n",
    "    'wiktionary.org',\n",
    "    'wikidata.org',\n",
    "    'wikibooks.org',\n",
    "    'wikinews.org',\n",
    "    'wikispecies.org',\n",
    "    'wikiversity.org',\n",
    "    'wikivoyage.org',\n",
    "}\n",
    "\n",
    "wikipedia_rows = []\n",
    "for page in grok_domains:\n",
    "    title = list(page.keys())[0]\n",
    "    domains = list(page.values())[0]\n",
    "    for site in wikipedia_sites:\n",
    "        for domain in domains:\n",
    "            if site in domain:\n",
    "                wikipedia_rows.append({\n",
    "                    'title': title,\n",
    "                    'site': domain,\n",
    "                    'citations': domains[domain]\n",
    "                })\n",
    "\n",
    "wikipedia_df = pd.DataFrame(wikipedia_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543111c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_df.sort_values(by='citations', ascending=False).to_csv('../results/wikipedia_citations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69947487",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_df.site.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37541d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_citations = 0\n",
    "for page in grok_domains:\n",
    "    domains = list(page.values())[0]\n",
    "    total_citations += sum(domains.values())\n",
    "total_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6111c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
