{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93db0726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 11:14:22,949 - INFO - BrightData proxy configured\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from aiolimiter import AsyncLimiter\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# BrightData Proxy Configuration\n",
    "BRIGHTDATA_USERNAME = os.getenv('BRIGHTDATA_USERNAME')\n",
    "BRIGHTDATA_PASSWORD = os.getenv('BRIGHTDATA_PASSWORD')\n",
    "\n",
    "if BRIGHTDATA_USERNAME and BRIGHTDATA_PASSWORD:\n",
    "    PROXY_URL = f'http://{BRIGHTDATA_USERNAME}:{BRIGHTDATA_PASSWORD}@brd.superproxy.io:33335'\n",
    "    logger.info('BrightData proxy configured')\n",
    "else:\n",
    "    PROXY_URL = None\n",
    "    logger.warning('BrightData credentials not found - running without proxy')\n",
    "\n",
    "# Configuration - optimized for BrightData proxy\n",
    "DISCOVERY_START_INDEX = 250000  # Resume from 180k\n",
    "DISCOVERY_BATCH_SIZE = 5000\n",
    "DISCOVERY_TIMEOUT = 15  # Increased timeout\n",
    "SCRAPING_START_INDEX = 355900\n",
    "SCRAPING_BATCH_SIZE = 300\n",
    "SCRAPING_SKIP_ON_ERROR = True\n",
    "SCRAPING_TIMEOUT = 60  # Increased timeout for reliability\n",
    "MAX_CONCURRENT = 200  # Keep high with proxy\n",
    "RATE_LIMIT = 300  # Aggressive with proxy\n",
    "BATCH_DELAY = 0.5  # Reduced delay\n",
    "\n",
    "SCRAPED_DATA_DIR = \"../scraped_data\"\n",
    "DISCOVERED_TITLES_DIR = \"../discovered_titles\"\n",
    "\n",
    "# Create output directories\n",
    "Path(DISCOVERED_TITLES_DIR).mkdir(exist_ok=True)\n",
    "Path(SCRAPED_DATA_DIR).mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a75c9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 11:14:22,958 - INFO - Using proxy: brd.superproxy.io:33335\n"
     ]
    }
   ],
   "source": [
    "# Configure aiohttp with BrightData proxy if available\n",
    "if PROXY_URL:\n",
    "    logger.info(f\"Using proxy: {PROXY_URL.split('@')[1]}\")  # Don't log credentials\n",
    "    proxy_auth = aiohttp.BasicAuth(BRIGHTDATA_USERNAME, BRIGHTDATA_PASSWORD) if BRIGHTDATA_USERNAME else None\n",
    "    \n",
    "    # Proxy configuration for aiohttp\n",
    "    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT)\n",
    "    proxy_config = {\n",
    "        'http': f'http://brd.superproxy.io:33335',\n",
    "        'https': f'http://brd.superproxy.io:33335'\n",
    "    }\n",
    "else:\n",
    "    proxy_config = None\n",
    "    logger.info(\"Running without proxy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56683ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse grokipedia HTML\n",
    "def parse_grokipedia_html(html_content, url, title=None):\n",
    "    \"\"\"Parse grokipedia HTML and extract structured data\"\"\"\n",
    "    if title is None:\n",
    "        title = url.split('/page/')[-1]\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    data = {\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'main_title': None,\n",
    "        'sections': [],\n",
    "        'paragraphs': [],\n",
    "        'tables': [],\n",
    "        'references': [],\n",
    "    }\n",
    "    \n",
    "    # Find article container\n",
    "    article = soup.find('div', class_='mx-auto max-w-[850px]')\n",
    "    if not article:\n",
    "        return data\n",
    "    \n",
    "    # Extract main title (h1)\n",
    "    h1 = article.find('h1')\n",
    "    if h1:\n",
    "        data['main_title'] = h1.get_text(strip=True)\n",
    "    \n",
    "    # Extract sections with proper content\n",
    "    for heading in article.find_all(['h1', 'h2', 'h3'], id=True):\n",
    "        section_data = {\n",
    "            'level': heading.name,\n",
    "            'id': heading.get('id'),\n",
    "            'title': heading.get_text(strip=True),\n",
    "            'content': []\n",
    "        }\n",
    "        \n",
    "        # Walk through siblings after heading\n",
    "        current = heading.next_sibling\n",
    "        while current:\n",
    "            if hasattr(current, 'name') and current.name in ['h1', 'h2', 'h3']:\n",
    "                if current.name <= heading.name:\n",
    "                    break\n",
    "            \n",
    "            if hasattr(current, 'name'):\n",
    "                if current.name == 'span' and 'mb-4' in (current.get('class') or []):\n",
    "                    text = current.get_text(strip=True)\n",
    "                    if text:\n",
    "                        # Join sentences with proper spacing\n",
    "                        section_data['content'].append({'type': 'paragraph', 'text': ' '.join(text.split())})\n",
    "                elif current.name == 'ul':\n",
    "                    items = [li.get_text(strip=True) for li in current.find_all('li')]\n",
    "                    if items:\n",
    "                        section_data['content'].append({'type': 'list', 'items': items})\n",
    "                elif current.name == 'ol':\n",
    "                    items = [li.get_text(strip=True) for li in current.find_all('li')]\n",
    "                    if items:\n",
    "                        section_data['content'].append({'type': 'ordered_list', 'items': items})\n",
    "            \n",
    "            current = current.next_sibling\n",
    "        \n",
    "        data['sections'].append(section_data)\n",
    "    \n",
    "    # Extract paragraphs with proper spacing\n",
    "    for span in article.find_all('span', class_='mb-4'):\n",
    "        text = span.get_text(strip=True)\n",
    "        # Normalize whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        if text and text not in data['paragraphs']:\n",
    "            data['paragraphs'].append(text)\n",
    "    \n",
    "    # Extract tables\n",
    "    for table in article.find_all('table'):\n",
    "        table_data = []\n",
    "        headers = []\n",
    "        \n",
    "        if table.find('thead'):\n",
    "            for th in table.find('thead').find_all('th'):\n",
    "                headers.append(th.get_text(strip=True))\n",
    "        \n",
    "        if table.find('tbody'):\n",
    "            for tr in table.find('tbody').find_all('tr'):\n",
    "                row = []\n",
    "                for td in tr.find_all('td'):\n",
    "                    row.append(td.get_text(strip=True))\n",
    "                if row:\n",
    "                    table_data.append(row)\n",
    "        \n",
    "        if headers or table_data:\n",
    "            data['tables'].append({'headers': headers, 'rows': table_data})\n",
    "    \n",
    "    # Extract references WITH links\n",
    "    references_section = soup.find('div', id='references')\n",
    "    if references_section:\n",
    "        for li in references_section.find_all('li'):\n",
    "            ref_text = li.get_text(strip=True)\n",
    "            ref_link = None\n",
    "            \n",
    "            link = li.find('a')\n",
    "            if link and link.get('href'):\n",
    "                ref_link = {'href': link.get('href'), 'text': link.get_text(strip=True)}\n",
    "            \n",
    "            if ref_text:\n",
    "                data['references'].append({'text': ref_text, 'link': ref_link})\n",
    "    \n",
    "    # Remove references from paragraphs\n",
    "    data['paragraphs'] = [p for p in data['paragraphs'] \n",
    "                          if not any(ref['text'].split()[0:3] == p.split()[0:3] for ref in data['references'])]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5582f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def discover_page_exists(session, limiter, title):\n",
    "    \"\"\"Check if a grokipedia page exists using HEAD request\"\"\"\n",
    "    url = f\"https://grokipedia.com/page/{quote(title)}\"\n",
    "    try:\n",
    "        async with limiter:\n",
    "            async with session.head(\n",
    "                url,\n",
    "                timeout=DISCOVERY_TIMEOUT,\n",
    "                headers={'Accept-Encoding': 'gzip, deflate'},\n",
    "                proxy=PROXY_URL,\n",
    "            ) as response:\n",
    "                status = response.status\n",
    "                if status == 200:\n",
    "                    return {'title': title, 'url': url, 'status': 'exists', 'checked_at': datetime.now().isoformat()}\n",
    "                elif status == 404:\n",
    "                    return {'title': title, 'url': url, 'status': 'not_found', 'checked_at': datetime.now().isoformat()}\n",
    "                elif status == 429:\n",
    "                    await asyncio.sleep(5)\n",
    "                    return {'title': title, 'url': url, 'status': 'rate_limited', 'checked_at': datetime.now().isoformat()}\n",
    "                else:\n",
    "                    return {'title': title, 'url': url, 'status': f'error_{status}', 'checked_at': datetime.now().isoformat()}\n",
    "    except Exception as e:\n",
    "        return {'title': title, 'url': url, 'status': 'error', 'error': str(e), 'checked_at': datetime.now().isoformat()}\n",
    "\n",
    "async def discovery_phase(titles, start_index=0, batch_size=10000):\n",
    "    \"\"\"Run discovery phase to find which pages exist\"\"\"\n",
    "    limiter = AsyncLimiter(max_rate=RATE_LIMIT, time_period=DISCOVERY_TIMEOUT)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        discovered_count = 0\n",
    "        not_found_count = 0\n",
    "        error_count = 0\n",
    "        results = []\n",
    "\n",
    "        pbar = tqdm(total=len(titles), desc=\"Discovery\", initial=start_index)\n",
    "\n",
    "        for i in range(start_index, len(titles), MAX_CONCURRENT):\n",
    "            batch = titles[i:i + MAX_CONCURRENT]\n",
    "\n",
    "            tasks = [discover_page_exists(session, limiter, title) for title in batch]\n",
    "            batch_results = await asyncio.gather(*tasks)\n",
    "\n",
    "            for result in batch_results:\n",
    "                if result['status'] == 'exists':\n",
    "                    discovered_count += 1\n",
    "                elif result['status'] == 'not_found':\n",
    "                    not_found_count += 1\n",
    "                else:\n",
    "                    error_count += 1\n",
    "                results.append(result)\n",
    "\n",
    "            pbar.update(len(batch))\n",
    "            pbar.set_postfix({'found': discovered_count, 'not_found': not_found_count, 'errors': error_count})\n",
    "\n",
    "            if i % MAX_CONCURRENT < MAX_CONCURRENT - 1:\n",
    "                await asyncio.sleep(BATCH_DELAY)\n",
    "\n",
    "            if len(results) >= batch_size:\n",
    "                batch_num = (i // batch_size) + 1\n",
    "                batch_start = (batch_num - 1) * batch_size\n",
    "                batch_end = batch_start + len(results)\n",
    "                with open(f'discovered_titles/batch_{batch_start}_{batch_end}.jsonl', 'w') as f:\n",
    "                    for result in results:\n",
    "                        f.write(json.dumps(result) + '\\n')\n",
    "                checkpoint = {\n",
    "                    'last_processed_index': i,\n",
    "                    'discovered_count': discovered_count,\n",
    "                    'not_found_count': not_found_count,\n",
    "                    'error_count': error_count,\n",
    "                    'total_processed': i + len(batch)\n",
    "                }\n",
    "                with open('discovery_checkpoint.json', 'w') as f:\n",
    "                    json.dump(checkpoint, f)\n",
    "                results = []\n",
    "\n",
    "        if results:\n",
    "            batch_num = (len(titles) // batch_size)\n",
    "            batch_start = batch_num * batch_size\n",
    "            batch_end = batch_start + len(results)\n",
    "            with open(f'discovered_titles/batch_{batch_start}_{batch_end}.jsonl', 'w') as f:\n",
    "                for result in results:\n",
    "                    f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        stats = {\n",
    "            'total_checked': len(titles),\n",
    "            'discovered': discovered_count,\n",
    "            'not_found': not_found_count,\n",
    "            'errors': error_count,\n",
    "            'completed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        with open('discovery_stats.json', 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Discovery complete: Found {discovered_count} existing pages out of {len(titles)} checked\")\n",
    "        return discovered_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38f16e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_page(session, limiter, url, skip_on_error=True):\n",
    "    \"\"\"Scrape a single grokipedia page\"\"\"\n",
    "    try:\n",
    "        title = url.split('/page/')[-1]\n",
    "        async with limiter:\n",
    "            async with session.get(\n",
    "                url,\n",
    "                timeout=60,\n",
    "                headers={'Accept-Encoding': 'gzip, deflate'},\n",
    "                proxy=PROXY_URL,\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    html = await response.text()\n",
    "                    data = parse_grokipedia_html(html, url)\n",
    "                    return {'success': True, 'data': data}\n",
    "                elif response.status == 404:\n",
    "                    return {'success': False, 'error': 'not_found', 'title': title}\n",
    "                else:\n",
    "                    return {'success': False, 'error': f'status_{response.status}', 'title': title}\n",
    "    except asyncio.TimeoutError:\n",
    "        if skip_on_error:\n",
    "            return {'success': False, 'error': 'timeout', 'title': title}\n",
    "        else:\n",
    "            for delay in [2, 4, 8]:\n",
    "                await asyncio.sleep(delay)\n",
    "                try:\n",
    "                    async with limiter:\n",
    "                        async with session.get(\n",
    "                            url,\n",
    "                            timeout=45,\n",
    "                            headers={'Accept-Encoding': 'gzip, deflate'},\n",
    "                            proxy=PROXY_URL,\n",
    "                        ) as response:\n",
    "                            if response.status == 200:\n",
    "                                html = await response.text()\n",
    "                                data = parse_grokipedia_html(html, url, title)\n",
    "                                return {'success': True, 'data': data}\n",
    "                except:\n",
    "                    continue\n",
    "            return {'success': False, 'error': 'timeout_retries_exhausted', 'title': title}\n",
    "    except Exception as e:\n",
    "        return {'success': False, 'error': str(e), 'title': title}\n",
    "\n",
    "async def scraping_phase(urls, start_index=0, batch_size=1000, skip_on_error=True):\n",
    "    \"\"\"Run scraping phase to extract data from discovered pages\"\"\"\n",
    "    limiter = AsyncLimiter(max_rate=RATE_LIMIT, time_period=60)  # Fixed: use 60 second window\n",
    "    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT, force_close=True, enable_cleanup_closed=True)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    Path(SCRAPED_DATA_DIR).mkdir(exist_ok=True)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=aiohttp.ClientTimeout(total=SCRAPING_TIMEOUT)) as session:\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        scraped_data = []\n",
    "        failed_pages = []\n",
    "        batch_count = 0  # Track batch number\n",
    "        last_save_index = 0  # Track what's been saved\n",
    "        \n",
    "        # Create progress bar\n",
    "        pbar = tqdm(total=len(urls), desc=\"Scraping\", initial=start_index)\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(start_index, len(urls), MAX_CONCURRENT):\n",
    "            batch = urls[i:i + MAX_CONCURRENT]\n",
    "            \n",
    "            # Create tasks for concurrent requests\n",
    "            tasks = [scrape_page(session, limiter, url, skip_on_error) for url in batch]\n",
    "            \n",
    "            # Use gather with return_exceptions to handle individual failures\n",
    "            try:\n",
    "                batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Batch gather failed: {e}\")\n",
    "                batch_results = [{'success': False, 'error': str(e), 'title': 'batch_error'} for _ in batch]\n",
    "            \n",
    "            # Process results\n",
    "            for j, result in enumerate(batch_results):\n",
    "                # Handle exceptions\n",
    "                if isinstance(result, Exception):\n",
    "                    fail_count += 1\n",
    "                    failed_pages.append({\n",
    "                        'title': batch[j] if j < len(batch) else 'unknown',\n",
    "                        'url': batch[j] if j < len(batch) else 'unknown',\n",
    "                        'error': str(result),\n",
    "                        'failed_at': datetime.now().isoformat()\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                if result['success']:\n",
    "                    success_count += 1\n",
    "                    scraped_data.append({\n",
    "                        'title': result['data']['title'],\n",
    "                        'url': result['data']['url'],\n",
    "                        'data': result['data'],\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    })\n",
    "                else:\n",
    "                    fail_count += 1\n",
    "                    failed_pages.append({\n",
    "                        'title': result.get('title', 'unknown'),\n",
    "                        'url': result.get('url', 'unknown'),\n",
    "                        'error': result.get('error', 'unknown'),\n",
    "                        'failed_at': datetime.now().isoformat()\n",
    "                    })\n",
    "            \n",
    "            pbar.update(len(batch))\n",
    "            pbar.set_postfix({\n",
    "                'success': success_count,\n",
    "                'failed': fail_count,\n",
    "                'fail_rate': f'{fail_count/(success_count+fail_count)*100:.1f}%' if (success_count+fail_count) > 0 else '0%'\n",
    "            })\n",
    "            \n",
    "            # Save batch when we hit the batch_size limit OR every 100 successful items\n",
    "            should_save = False\n",
    "            save_reason = \"\"\n",
    "            \n",
    "            if len(scraped_data) >= batch_size:\n",
    "                should_save = True\n",
    "                save_reason = \"batch_size\"\n",
    "            elif len(scraped_data) >= 100 and (i - last_save_index) >= 5000:  # Save every 5000 processed pages\n",
    "                should_save = True\n",
    "                save_reason = \"progress\"\n",
    "            \n",
    "            if should_save and scraped_data:\n",
    "                batch_start = start_index + (batch_count * batch_size)\n",
    "                batch_end = batch_start + len(scraped_data)\n",
    "                \n",
    "                batch_file = Path(f'{SCRAPED_DATA_DIR}/batch_{batch_start}_{batch_end}.jsonl')\n",
    "                \n",
    "                with open(batch_file, 'w') as f:\n",
    "                    for item in scraped_data:\n",
    "                        f.write(json.dumps(item) + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                checkpoint = {\n",
    "                    'last_processed_index': i,\n",
    "                    'success_count': success_count,\n",
    "                    'fail_count': fail_count,\n",
    "                    'total_processed': i + len(batch),\n",
    "                    'last_save_index': batch_end,\n",
    "                    'batch_count': batch_count,\n",
    "                    'save_reason': save_reason\n",
    "                }\n",
    "                \n",
    "                with open('scraping_checkpoint.json', 'w') as f:\n",
    "                    json.dump(checkpoint, f)\n",
    "                \n",
    "                logger.info(f\"Saved batch: {batch_start} to {batch_end} ({len(scraped_data)} items) - Reason: {save_reason}\")\n",
    "                last_save_index = i\n",
    "                batch_count += 1\n",
    "                scraped_data = []\n",
    "            \n",
    "            # Save failed pages periodically too\n",
    "            if len(failed_pages) >= 1000:\n",
    "                failed_file = Path('scraping_failed_partial.jsonl')\n",
    "                with open(failed_file, 'a') as f:\n",
    "                    for item in failed_pages:\n",
    "                        f.write(json.dumps(item) + '\\n')\n",
    "                logger.info(f\"Saved {len(failed_pages)} failed pages to partial file\")\n",
    "                failed_pages = []\n",
    "            \n",
    "            # Adaptive delay - increase if failure rate is high\n",
    "            failure_rate = fail_count / (success_count + fail_count) if (success_count + fail_count) > 0 else 0\n",
    "            if failure_rate > 0.3:  # More than 30% failure rate\n",
    "                delay = 1.0\n",
    "                logger.warning(f\"High failure rate ({failure_rate*100:.1f}%), increasing delay to {delay}s\")\n",
    "            else:\n",
    "                delay = 0.1\n",
    "            \n",
    "            await asyncio.sleep(delay)\n",
    "        \n",
    "        # Save any remaining results\n",
    "        if scraped_data:\n",
    "            batch_start = start_index + (batch_count * batch_size)\n",
    "            batch_end = batch_start + len(scraped_data)\n",
    "            \n",
    "            batch_file = Path(f'{SCRAPED_DATA_DIR}/batch_{batch_start}_{batch_end}.jsonl')\n",
    "            \n",
    "            with open(batch_file, 'w') as f:\n",
    "                for item in scraped_data:\n",
    "                    f.write(json.dumps(item) + '\\n')\n",
    "            \n",
    "            logger.info(f\"Saved final batch: {batch_start} to {batch_end} ({len(scraped_data)} items)\")\n",
    "        \n",
    "        # Save all failed pages\n",
    "        if failed_pages:\n",
    "            with open('scraping_failed.jsonl', 'w') as f:\n",
    "                for item in failed_pages:\n",
    "                    f.write(json.dumps(item) + '\\n')\n",
    "            logger.info(f\"Saved {len(failed_pages)} failed pages\")\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Save final stats\n",
    "        stats = {\n",
    "            'total_scraped': len(urls),\n",
    "            'success': success_count,\n",
    "            'failed': fail_count,\n",
    "            'completed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open('scraping_stats.json', 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Scraping complete: Successfully scraped {success_count} pages out of {len(urls)} attempted\")\n",
    "        return success_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc0d2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def load_titles():\n",
    "    \"\"\"Load and sort enwiki titles from pandas DataFrame\"\"\"\n",
    "    df = pd.read_pickle('../enwiki_titles_20251027.pkl')\n",
    "    titles = df['page_title'].tolist()\n",
    "    titles_sorted = sorted(titles)\n",
    "    logger.info(f\"Loaded {len(titles_sorted)} titles from DataFrame\")\n",
    "    return titles_sorted\n",
    "\n",
    "def load_discovered_titles():\n",
    "    \"\"\"Load all discovered titles from batch files\"\"\"\n",
    "    discovered = []\n",
    "    for file in sorted(Path('../discovered_titles').glob('batch_*.jsonl')):\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                result = json.loads(line)\n",
    "                if result['status'] == 'exists':\n",
    "                    discovered.append(result['title'])\n",
    "    logger.info(f\"Loaded {len(discovered)} discovered titles\")\n",
    "    return discovered\n",
    "\n",
    "def load_scraped_data():\n",
    "    \"\"\"Load all scraped data from batch files\"\"\"\n",
    "    scraped = []\n",
    "    for file in sorted(Path('{SCRAPED_DATA_DIR}').glob('batch_*.jsonl')):\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                scraped.append(json.loads(line))\n",
    "    logger.info(f\"Loaded {len(scraped)} scraped pages\")\n",
    "    return scraped\n",
    "\n",
    "def retry_failed_pages():\n",
    "    \"\"\"Retry scraping failed pages\"\"\"\n",
    "    failed = []\n",
    "    if Path('scraping_failed.jsonl').exists():\n",
    "        with open('scraping_failed.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                failed.append(json.loads(line))\n",
    "    \n",
    "    if not failed:\n",
    "        logger.info(\"No failed pages to retry\")\n",
    "        return []\n",
    "    \n",
    "    titles = [item['title'] for item in failed]\n",
    "    logger.info(f\"Retrying {len(titles)} failed pages\")\n",
    "    return titles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 1: DISCOVERY\n",
    "# Load and sort all enwiki titles\n",
    "titles = load_titles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b70520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run discovery phase\n",
    "# This will check all 7M titles to find which ~885k exist on grokipedia\n",
    "discovered_count = await discovery_phase(\n",
    "    titles,\n",
    "    start_index=DISCOVERY_START_INDEX,\n",
    "    batch_size=DISCOVERY_BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"\\nDiscovery Summary:\")\n",
    "print(f\"Checked {len(titles)} titles\")\n",
    "print(f\"Found {discovered_count} existing pages on grokipedia\")\n",
    "print(f\"Success rate: {discovered_count/len(titles)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaa8e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 2: SCRAPING\n",
    "# Load discovered titles from Phase 1\n",
    "# discovered_titles = load_discovered_titles()\n",
    "df_urls = pd.read_json(\"hf://datasets/stefan-it/grokipedia-urls/urls.jsonl\", lines=True)\n",
    "urls = df_urls['url'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9464b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping:  40%|████      | 356100/885279 [00:23<17:07:28,  8.58it/s, success=191, failed=9, fail_rate=4.5%]2025-10-29 11:15:19,380 - INFO - Saved batch: 355900 to 356091 (191 items) - Reason: progress\n",
      "Scraping:  40%|████      | 356500/885279 [02:18<37:01:32,  3.97it/s, success=577, failed=23, fail_rate=3.8%]2025-10-29 11:17:14,910 - INFO - Saved batch: 356200 to 356586 (386 items) - Reason: batch_size\n",
      "Scraping:  40%|████      | 356900/885279 [04:20<41:45:50,  3.51it/s, success=960, failed=40, fail_rate=4.0%]2025-10-29 11:19:16,921 - INFO - Saved batch: 356500 to 356883 (383 items) - Reason: batch_size\n",
      "Scraping:  40%|████      | 357300/885279 [06:22<43:23:54,  3.38it/s, success=1338, failed=62, fail_rate=4.4%]2025-10-29 11:21:18,833 - INFO - Saved batch: 356800 to 357178 (378 items) - Reason: batch_size\n",
      "Scraping:  40%|████      | 357700/885279 [07:49<36:23:26,  4.03it/s, success=1733, failed=67, fail_rate=3.7%]2025-10-29 11:22:45,279 - INFO - Saved batch: 357100 to 357495 (395 items) - Reason: batch_size\n",
      "Scraping:  40%|████      | 358100/885279 [09:50<40:35:48,  3.61it/s, success=2112, failed=88, fail_rate=4.0%]2025-10-29 11:24:46,783 - INFO - Saved batch: 357400 to 357779 (379 items) - Reason: batch_size\n",
      "Scraping:  40%|████      | 358500/885279 [11:37<39:16:09,  3.73it/s, success=2474, failed=126, fail_rate=4.8%]2025-10-29 11:26:33,496 - INFO - Saved batch: 357700 to 358062 (362 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 358900/885279 [13:39<42:01:44,  3.48it/s, success=2844, failed=156, fail_rate=5.2%]2025-10-29 11:28:35,782 - INFO - Saved batch: 358000 to 358370 (370 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 359300/885279 [15:23<39:18:43,  3.72it/s, success=3233, failed=167, fail_rate=4.9%]2025-10-29 11:30:19,742 - INFO - Saved batch: 358300 to 358689 (389 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 359700/885279 [17:08<38:10:28,  3.82it/s, success=3629, failed=171, fail_rate=4.5%]2025-10-29 11:32:04,454 - INFO - Saved batch: 358600 to 358996 (396 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 360100/885279 [19:10<41:26:45,  3.52it/s, success=4011, failed=189, fail_rate=4.5%]2025-10-29 11:34:06,773 - INFO - Saved batch: 358900 to 359282 (382 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 360500/885279 [21:12<42:58:05,  3.39it/s, success=4401, failed=199, fail_rate=4.3%]2025-10-29 11:36:08,846 - INFO - Saved batch: 359200 to 359590 (390 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 360900/885279 [23:14<43:41:44,  3.33it/s, success=4788, failed=212, fail_rate=4.2%]2025-10-29 11:38:10,850 - INFO - Saved batch: 359500 to 359887 (387 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 361300/885279 [25:12<43:24:56,  3.35it/s, success=5096, failed=304, fail_rate=5.6%]2025-10-29 11:40:08,987 - INFO - Saved batch: 359800 to 360108 (308 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 361700/885279 [27:13<43:41:11,  3.33it/s, success=5487, failed=313, fail_rate=5.4%]2025-10-29 11:42:09,855 - INFO - Saved batch: 360100 to 360491 (391 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 362100/885279 [29:04<42:16:37,  3.44it/s, success=5883, failed=317, fail_rate=5.1%]2025-10-29 11:44:00,840 - INFO - Saved batch: 360400 to 360796 (396 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 362500/885279 [31:06<43:17:15,  3.35it/s, success=6274, failed=326, fail_rate=4.9%]2025-10-29 11:46:02,821 - INFO - Saved batch: 360700 to 361091 (391 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 362900/885279 [33:11<44:12:30,  3.28it/s, success=6652, failed=348, fail_rate=5.0%]2025-10-29 11:48:07,930 - INFO - Saved batch: 361000 to 361378 (378 items) - Reason: batch_size\n",
      "Scraping:  41%|████      | 363100/885279 [34:12<44:12:36,  3.28it/s, success=6847, failed=353, fail_rate=4.9%]"
     ]
    }
   ],
   "source": [
    "# Run scraping phase\n",
    "# This will extract structured data from the discovered pages\n",
    "scraped_count = await scraping_phase(\n",
    "    urls,\n",
    "    start_index=SCRAPING_START_INDEX,\n",
    "    batch_size=SCRAPING_BATCH_SIZE,\n",
    "    skip_on_error=SCRAPING_SKIP_ON_ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d846ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nScraping Summary:\")\n",
    "print(f\"Attempted to scrape {len(urls)} pages\")\n",
    "print(f\"Successfully scraped {scraped_count} pages\")\n",
    "print(f\"Success rate: {scraped_count/len(urls)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc01ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANALYSIS: Compare coverage\n",
    "titles = load_titles()\n",
    "discovered = load_discovered_titles()\n",
    "\n",
    "print(\"Coverage Analysis:\")\n",
    "print(f\"Total enwiki titles: {len(titles):,}\")\n",
    "print(f\"Grokipedia pages found: {len(discovered):,}\")\n",
    "print(f\"Coverage: {len(discovered)/len(titles)*100:.2f}%\")\n",
    "print(f\"Missing: {len(titles)-len(discovered):,} pages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a33a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRY FAILED PAGES (optional)\n",
    "# Uncomment to retry pages that failed during Phase 2\n",
    "# failed_titles = retry_failed_pages()\n",
    "# if failed_titles:\n",
    "#     await scraping_phase(\n",
    "#         failed_titles,\n",
    "#         start_index=0,\n",
    "#         batch_size=SCRAPING_BATCH_SIZE,\n",
    "#         skip_on_error=False  # Use exponential backoff for retries\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce8f521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD ALL SCRAPED DATA\n",
    "# Load all scraped data into a list for analysis\n",
    "scraped = load_scraped_data()\n",
    "\n",
    "# Example: View first scraped page\n",
    "if scraped:\n",
    "    print(f\"Total pages scraped: {len(scraped)}\")\n",
    "    print(\"\\nExample page structure:\")\n",
    "    import json\n",
    "    print(json.dumps(scraped[0], indent=2)[:500] + \"...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
