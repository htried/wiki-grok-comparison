{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d02e8a84",
   "metadata": {},
   "source": [
    "# Dedupe Grokipedia content\n",
    "\n",
    "I realized that I accidentally mis-parsed `<h1>` blocks in Grokipedia, leading to a great deal of duplicated content. This notebook is for deduping those pieces of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884c60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187b33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_page(page):\n",
    "    data = page.get(\"data\") or {}\n",
    "    sections = data.get(\"sections\") or []\n",
    "    if not sections:\n",
    "        return page\n",
    "\n",
    "    # Precompute content signatures for each section: set of (type, text)\n",
    "    section_signatures = []\n",
    "    for s in sections:\n",
    "        content = s.get(\"content\") or []\n",
    "        sig = set()\n",
    "        for item in content:\n",
    "            if isinstance(item, dict):\n",
    "                sig.add((item.get(\"type\"), item.get(\"text\")))\n",
    "        section_signatures.append(sig)\n",
    "\n",
    "    # For each h1 section, build the \"other sections\" signature and dedupe its content\n",
    "    for idx, s in enumerate(sections):\n",
    "        if s.get(\"level\") != \"h1\":\n",
    "            continue\n",
    "\n",
    "        # Union of all content in other sections (exclude this h1’s own section)\n",
    "        other_sig = set()\n",
    "        for j, sig in enumerate(section_signatures):\n",
    "            if j != idx:\n",
    "                other_sig |= sig\n",
    "\n",
    "        content = s.get(\"content\") or []\n",
    "        deduped = [\n",
    "            item for item in content\n",
    "            if isinstance(item, dict) and (item.get(\"type\"), item.get(\"text\")) not in other_sig\n",
    "        ]\n",
    "        s[\"content\"] = deduped\n",
    "\n",
    "        # Update the cached signature for this h1 after dedupe (optional, not required further)\n",
    "        section_signatures[idx] = set(\n",
    "            (item.get(\"type\"), item.get(\"text\")) for item in deduped if isinstance(item, dict)\n",
    "        )\n",
    "\n",
    "    return page\n",
    "\n",
    "def process_file(in_fp, out_fp=None):\n",
    "    pages_processed = 0\n",
    "    # If rewriting in place, write to a temp file first\n",
    "    if out_fp is None:\n",
    "        parent = Path(in_fp).parent\n",
    "        tmp_fh, tmp_path = tempfile.mkstemp(prefix=\"dedupe_\", suffix=\".jsonl\", dir=str(parent))\n",
    "        os.close(tmp_fh)\n",
    "        out_fp = tmp_path\n",
    "        inplace = True\n",
    "    else:\n",
    "        inplace = False\n",
    "\n",
    "    with open(in_fp, \"r\", encoding=\"utf-8\") as fin, open(out_fp, \"w\", encoding=\"utf-8\") as fout:\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                page = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            page = process_page(page)\n",
    "            fout.write(json.dumps(page, ensure_ascii=False) + \"\\n\")\n",
    "            pages_processed += 1\n",
    "\n",
    "    if inplace:\n",
    "        # Atomic replace\n",
    "        shutil.move(out_fp, in_fp)\n",
    "\n",
    "    return pages_processed\n",
    "\n",
    "def dedupe_main_section(\n",
    "    scraped_dir=\"../scraped_data\",\n",
    "    test_fp=None,\n",
    "    outfile=None,\n",
    "    glob_pattern=\"*.jsonl\"  # recurse by default\n",
    "):\n",
    "    \"\"\"\n",
    "    For each page record, removes from the main (h1) section any content item (by (type, text))\n",
    "    that also appears in any non-main section.\n",
    "\n",
    "    Behavior:\n",
    "      - If test_fp is provided: process only that file.\n",
    "      - Else: process all JSONL files under scraped_dir matching glob_pattern.\n",
    "      - If outfile is provided: append processed pages to outfile (do not modify inputs).\n",
    "      - If outfile is None: rewrite input files in place (atomic replace).\n",
    "\n",
    "    Returns:\n",
    "      - When processing a single file (test_fp): number of pages processed.\n",
    "      - When processing multiple: total number of files and pages processed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Single-file mode\n",
    "    if test_fp:\n",
    "        if outfile:\n",
    "            # Append mode: process input into outfile (append)\n",
    "            processed = 0\n",
    "            with open(test_fp, \"r\", encoding=\"utf-8\") as fin, open(outfile, \"a\", encoding=\"utf-8\") as fout:\n",
    "                for line in fin:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        page = json.loads(line)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                    page = process_page(page)\n",
    "                    fout.write(json.dumps(page, ensure_ascii=False) + \"\\n\")\n",
    "                    processed += 1\n",
    "            print(f\"Processed {processed} pages from {test_fp} → appended to {outfile}\")\n",
    "            return processed\n",
    "        else:\n",
    "            processed = process_file(test_fp, out_fp=None)\n",
    "            print(f\"Processed {processed} pages (in place): {test_fp}\")\n",
    "            return processed\n",
    "\n",
    "    # Directory mode\n",
    "    base = Path(scraped_dir)\n",
    "    files = sorted(base.glob(glob_pattern))\n",
    "    total_pages = 0\n",
    "    total_files = 0\n",
    "\n",
    "    if outfile:\n",
    "        # Ensure outfile is fresh\n",
    "        Path(outfile).parent.mkdir(parents=True, exist_ok=True)\n",
    "        if Path(outfile).exists():\n",
    "            # Keep appending to existing if desired; otherwise uncomment to truncate:\n",
    "            # open(outfile, \"w\").close()\n",
    "            pass\n",
    "\n",
    "    for fp in files:\n",
    "        if not fp.is_file() or fp.suffix.lower() != \".jsonl\":\n",
    "            continue\n",
    "        total_files += 1\n",
    "        if outfile:\n",
    "            # Append all processed pages from this file into outfile\n",
    "            with open(fp, \"r\", encoding=\"utf-8\") as fin, open(outfile, \"a\", encoding=\"utf-8\") as fout:\n",
    "                for line in fin:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        continue\n",
    "                    try:\n",
    "                        page = json.loads(line)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "                    page = process_page(page)\n",
    "                    fout.write(json.dumps(page, ensure_ascii=False) + \"\\n\")\n",
    "                    total_pages += 1\n",
    "        else:\n",
    "            # Rewrite in place\n",
    "            total_pages += process_file(str(fp), out_fp=None)\n",
    "\n",
    "    print(f\"Processed {total_pages} pages across {total_files} files\" + (f\" → appended to {outfile}\" if outfile else \" (in place)\"))\n",
    "    return total_files, total_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f898e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fp = '/Users/haltriedman/code/wiki-grok-comparison/scraped_data/clive_anderson.jsonl'\n",
    "outfile = 'test_out.jsonl'\n",
    "\n",
    "dedupe_main_section(test_fp=test_fp, outfile=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497124d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dedupe_main_section()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb39d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
