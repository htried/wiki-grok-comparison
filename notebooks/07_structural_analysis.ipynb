{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761b83cd",
   "metadata": {},
   "source": [
    "## Page structure similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKIPEDIA_PATH = \"../grokipedia_wikipedia_articles.ndjson\"\n",
    "GROKIPEDIA_GLOB = \"../scraped_data/batch_*.jsonl\"\n",
    "RESULT_DIR = \"../results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17942bd",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _norm(s: str) -> str:\n",
    "    return \" \".join((s or \"\").strip().lower().split())\n",
    "\n",
    "def _level_to_int(level: str) -> int:\n",
    "    # 'h1' -> 1, 'h2' -> 2, ...\n",
    "    level = (level or \"\").lower().strip()\n",
    "    if level.startswith(\"h\") and level[1:].isdigit():\n",
    "        return int(level[1:])\n",
    "    return 2  # default neutral\n",
    "\n",
    "def extract_outline_from_wikipedia(wiki_obj):\n",
    "    outline = []\n",
    "    for sec in (wiki_obj.get(\"sections\") or []):\n",
    "        name = _norm(sec.get(\"name\"))\n",
    "        if not name:\n",
    "            continue\n",
    "        lvl = 1 if name == \"abstract\" else 2\n",
    "        outline.append((lvl, name))\n",
    "    return outline\n",
    "\n",
    "def extract_outline_from_grokipedia(g_obj: Dict[str, Any]) -> List[Tuple[int, str]]:\n",
    "    \"\"\"\n",
    "    Grokipedia JSON (data.sections): keep (level, title) sequence; handle multiple h1s.\n",
    "    \"\"\"\n",
    "    sections = (((g_obj or {}).get(\"data\") or {}).get(\"sections\") or [])\n",
    "    outline = []\n",
    "    for sec in sections:\n",
    "        lvl = _level_to_int(sec.get(\"level\"))\n",
    "        name = _norm(sec.get(\"title\"))\n",
    "        if name:\n",
    "            outline.append((lvl, name))\n",
    "    return outline\n",
    "\n",
    "def _lcs_len(a: List[str], b: List[str]) -> int:\n",
    "    # classic O(n*m) DP LCS length\n",
    "    n, m = len(a), len(b)\n",
    "    dp = [0]*(m+1)\n",
    "    for i in range(1, n+1):\n",
    "        prev = 0\n",
    "        for j in range(1, m+1):\n",
    "            tmp = dp[j]\n",
    "            if a[i-1] == b[j-1]:\n",
    "                dp[j] = prev + 1\n",
    "            else:\n",
    "                dp[j] = max(dp[j], dp[j-1])\n",
    "            prev = tmp\n",
    "    return dp[m]\n",
    "\n",
    "def _jaccard(a: List[str], b: List[str]) -> float:\n",
    "    A, B = set(a), set(b)\n",
    "    if not A and not B:\n",
    "        return 1.0\n",
    "    return len(A & B) / max(1, len(A | B))\n",
    "\n",
    "def _cosine(u: Dict[int, int], v: Dict[int, int]) -> float:\n",
    "    keys = set(u) | set(v)\n",
    "    num = sum(u.get(k,0)*v.get(k,0) for k in keys)\n",
    "    du = math.sqrt(sum((u.get(k,0))**2 for k in keys))\n",
    "    dv = math.sqrt(sum((v.get(k,0))**2 for k in keys))\n",
    "    if du == 0 or dv == 0:\n",
    "        return 1.0 if du == dv else 0.0\n",
    "    return num / (du*dv)\n",
    "\n",
    "def _depth_hist(outline: List[Tuple[int,str]]) -> Dict[int,int]:\n",
    "    h = {}\n",
    "    for lvl,_ in outline:\n",
    "        h[lvl] = h.get(lvl,0)+1\n",
    "    return h\n",
    "\n",
    "def _as_indent_tree(outline: List[Tuple[int,str]]) -> List[Tuple[int,str]]:\n",
    "    \"\"\"\n",
    "    Normalize to a tree-like preorder list using heading levels (indent structure).\n",
    "    We don’t compute full edit distance; we’ll compare shape via a preorder signature.\n",
    "    \"\"\"\n",
    "    # Ensure non-decreasing by at most +1 to avoid malformed jumps (optional clamp).\n",
    "    norm = []\n",
    "    last = 1\n",
    "    for lvl,name in outline:\n",
    "        lvl = max(1, lvl)\n",
    "        if lvl > last+1:\n",
    "            lvl = last+1\n",
    "        norm.append((lvl,name))\n",
    "        last = lvl\n",
    "    return norm\n",
    "\n",
    "def _tree_signature(t: List[Tuple[int,str]]) -> List[int]:\n",
    "    # Encode just the shape: level sequence deltas\n",
    "    if not t: return []\n",
    "    sig = [t[0][0]]\n",
    "    for i in range(1,len(t)):\n",
    "        sig.append(t[i][0]-t[i-1][0])\n",
    "    return sig\n",
    "\n",
    "def _overlap_ratio(a: List[int], b: List[int]) -> float:\n",
    "    # simple longest-common-prefix-like + LCS hybrid; here use LCS on small ints\n",
    "    # Map ints to strings for LCS (reuse)\n",
    "    sa = list(map(str,a))\n",
    "    sb = list(map(str,b))\n",
    "    l = _lcs_len(sa, sb)\n",
    "    denom = max(1, max(len(sa), len(sb)))\n",
    "    return l/denom\n",
    "\n",
    "def compare_structures(\n",
    "    wiki_outline: List[Tuple[int,str]],\n",
    "    grok_outline: List[Tuple[int,str]]\n",
    ") -> Dict[str, float]:\n",
    "    # Flatten to sequences of titles\n",
    "    w_titles = [t for _,t in wiki_outline]\n",
    "    g_titles = [t for _,t in grok_outline]\n",
    "\n",
    "    lcs = _lcs_len(w_titles, g_titles)\n",
    "    lcs_ratio = lcs / max(1, max(len(w_titles), len(g_titles)))\n",
    "    jacc = _jaccard(w_titles, g_titles)\n",
    "\n",
    "    # Depth / level profile\n",
    "    w_hist = _depth_hist(wiki_outline)\n",
    "    g_hist = _depth_hist(grok_outline)\n",
    "    depth_cos = _cosine(w_hist, g_hist)\n",
    "\n",
    "    # Tree-ish shape similarity from heading indentation\n",
    "    w_tree = _as_indent_tree(wiki_outline)\n",
    "    g_tree = _as_indent_tree(grok_outline)\n",
    "    shape_sim = _overlap_ratio(_tree_signature(w_tree), _tree_signature(g_tree))\n",
    "\n",
    "    # Outline length comparison\n",
    "    w_len = len(wiki_outline)\n",
    "    g_len = len(grok_outline)\n",
    "    length_diff = g_len - w_len\n",
    "    length_ratio = g_len / w_len if w_len > 0 else None\n",
    "\n",
    "    return dict(\n",
    "        lcs_ratio=lcs_ratio,\n",
    "        jaccard=jacc,\n",
    "        depth_cosine=depth_cos,\n",
    "        shape_similarity=shape_sim,\n",
    "        wikipedia_outline_length=w_len,\n",
    "        grokipedia_outline_length=g_len,\n",
    "        outline_length_diff=length_diff,\n",
    "        outline_length_ratio=length_ratio\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db41367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_ndjson(path):\n",
    "    \"\"\"Yields objects (dicts) one at a time from an .ndjson file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "def stream_grokipedia_jsonls(pattern):\n",
    "    \"\"\"Yields objects (dicts) one at a time from all JSONL files matching a pattern.\"\"\"\n",
    "    for fname in sorted(glob.glob(pattern)):\n",
    "        with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    yield json.loads(line)\n",
    "\n",
    "def build_title_pointer_index(path, title_field=\"name\", norm_func=_norm):\n",
    "    \"\"\"\n",
    "    Index file offsets of each normalized title for later recovery.\n",
    "    Returns {normalized_title: file_offset}\n",
    "    \"\"\"\n",
    "    idx = {}\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        pos = 0\n",
    "        for line in f:\n",
    "            # Record position before reading this line\n",
    "            line_start = pos\n",
    "            \n",
    "            if line.strip():\n",
    "                obj = json.loads(line)\n",
    "                title = obj.get(title_field, '')\n",
    "                if title:\n",
    "                    idx[norm_func(title)] = line_start\n",
    "            \n",
    "            # Update position by adding the byte length of this line\n",
    "            pos += len(line.encode('utf-8'))\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def build_grokipedia_file_map(pattern, norm_func=_norm):\n",
    "    \"\"\"\n",
    "    Maps normalized title to (filename, offset within file) for all Grokipedia jsonl files.\n",
    "    Returns {normalized_title: (filename, file_offset)}\n",
    "    \"\"\"\n",
    "    idx = {}\n",
    "    for fname in sorted(glob.glob(pattern)):\n",
    "        with open(fname, \"r\", encoding=\"utf-8\") as f:\n",
    "            pos = 0\n",
    "            for line in f:\n",
    "                # Record position before reading this line\n",
    "                line_start = pos\n",
    "                \n",
    "                if line.strip():\n",
    "                    obj = json.loads(line)\n",
    "                    title = obj.get('data', {}).get('main_title', '')\n",
    "                    if title:\n",
    "                        idx[norm_func(title)] = (fname, line_start)\n",
    "                \n",
    "                # Update position by adding the byte length of this line\n",
    "                pos += len(line.encode('utf-8'))\n",
    "    \n",
    "    return idx\n",
    "\n",
    "def get_object_at_offset(path, offset):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        f.seek(offset)\n",
    "        line = f.readline()\n",
    "        return json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36de5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_structures_on_datasets_memlite(\n",
    "    wikipedia_path=WIKIPEDIA_PATH,\n",
    "    grokipedia_glob=GROKIPEDIA_GLOB\n",
    "):\n",
    "    \"\"\"\n",
    "    Compares structures for matched titles between Wikipedia and Grokipedia datasets,\n",
    "    with efficient file scanning (not loading whole files into memory).\n",
    "    Yields dicts: {'title': ... , **metrics}\n",
    "    \"\"\"\n",
    "    # Step 1: Index Wikipedia titles with file offsets\n",
    "    print(\"Indexing Wikipedia titles...\")\n",
    "    wiki_idx = build_title_pointer_index(wikipedia_path, norm_func=_norm)\n",
    "    # Step 2: Index Grokipedia titles with (filename, offset)\n",
    "    print(\"Indexing Grokipedia titles...\")\n",
    "    grok_idx = build_grokipedia_file_map(grokipedia_glob, norm_func=_norm)\n",
    "\n",
    "    match_titles = set(wiki_idx).intersection(grok_idx)\n",
    "    print(f\"Matched {len(match_titles)} titles.\")\n",
    "\n",
    "    for title in tqdm(sorted(match_titles), desc=\"Comparing structures\"):\n",
    "        wiki_offset = wiki_idx[title]\n",
    "        grok_fname, grok_offset = grok_idx[title]\n",
    "\n",
    "        # Retrieve objects by file offsets—never in memory all at once!\n",
    "        wiki_obj = get_object_at_offset(wikipedia_path, wiki_offset)\n",
    "        grok_obj = get_object_at_offset(grok_fname, grok_offset)\n",
    "\n",
    "        wiki_outline = extract_outline_from_wikipedia(wiki_obj)\n",
    "        grok_outline = extract_outline_from_grokipedia(grok_obj)\n",
    "        metrics = compare_structures(wiki_outline, grok_outline)\n",
    "        metrics['title'] = title\n",
    "        yield metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9130b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 10000\n",
    "current_chunk = []\n",
    "\n",
    "output_path = f\"{RESULT_DIR}/structural_comparison.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "header_written = False\n",
    "\n",
    "for i, result in enumerate(compare_structures_on_datasets_memlite()):\n",
    "    current_chunk.append(result)\n",
    "    if len(current_chunk) == chunk_size:\n",
    "        df_chunk = pd.DataFrame(current_chunk)\n",
    "        df_chunk.to_csv(output_path, mode='a', header=not header_written, index=False)\n",
    "        header_written = True\n",
    "        print(f\"Processed and wrote chunk ending at row {i}\")\n",
    "        current_chunk = []\n",
    "\n",
    "# Process any remaining results\n",
    "if current_chunk:\n",
    "    df_chunk = pd.DataFrame(current_chunk)\n",
    "    df_chunk.to_csv(output_path, mode='a', header=not header_written, index=False)\n",
    "    print(\"Processed and wrote final chunk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3b3970",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{RESULT_DIR}/structural_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19597657",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['outline_length_ratio'] > 1]) / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_DIR = '../results'\n",
    "\n",
    "with open(f\"{RESULT_DIR}/grokipedia_wo_license.txt\", encoding=\"utf-8\") as f:\n",
    "    grokipedia_wo_license_df = pd.DataFrame({\"title\": [line.rstrip('\\n').lower() for line in f]})\n",
    "\n",
    "with open(f\"{RESULT_DIR}/grokipedia_w_license.txt\", encoding=\"utf-8\") as f:\n",
    "    grokipedia_w_license_df = pd.DataFrame({\"title\": [line.rstrip('\\n').lower() for line in f]})\n",
    "\n",
    "df_wo_license = df.merge(grokipedia_wo_license_df, on=\"title\")\n",
    "df_w_license = df.merge(grokipedia_w_license_df, on=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d58972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "wo_ratios = df_wo_license[\"outline_length_ratio\"][df_wo_license[\"outline_length_ratio\"] > 0]\n",
    "w_ratios = df_w_license[\"outline_length_ratio\"][df_w_license[\"outline_length_ratio\"] > 0]\n",
    "\n",
    "# Choose number of bins\n",
    "n_bins = 100\n",
    "\n",
    "# Get combined nonzero ratios\n",
    "all_ratios = pd.concat([wo_ratios, w_ratios])\n",
    "\n",
    "# Compute equal-width bin edges: visually equal-sized buckets in data space\n",
    "min_ratio = all_ratios.min()\n",
    "max_ratio = all_ratios.max()\n",
    "edges = np.logspace(np.log10(min_ratio), np.log10(max_ratio), n_bins + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "n, bins, patches = plt.hist(\n",
    "    [wo_ratios, w_ratios],\n",
    "    bins=edges,\n",
    "    color=[\"tab:orange\", \"tab:blue\"],\n",
    "    label=[\"Without CC License\", \"With CC License\"],\n",
    "    alpha=0.7,\n",
    "    histtype=\"stepfilled\",\n",
    "    log=True,\n",
    ")\n",
    "\n",
    "# Calculate medians\n",
    "wo_median = np.median(wo_ratios)\n",
    "w_median = np.median(w_ratios)\n",
    "\n",
    "# Plot median lines and add them to legend, with parenthetical median values\n",
    "wo_line = plt.axvline(\n",
    "    wo_median, color=\"black\", linestyle=\"--\", linewidth=2, \n",
    "    label=f\"No CC license median = {wo_median:.2g}\"\n",
    ")\n",
    "w_line = plt.axvline(\n",
    "    w_median, color=\"black\", linestyle=\":\", linewidth=2, \n",
    "    label=f\"CC license median = {w_median:.2g}\"\n",
    ")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Outline Length Ratio (log scale)\", fontsize=16)\n",
    "plt.ylabel(\"Count (log scale)\", fontsize=16)\n",
    "plt.title(\"Outline Length Ratio Distribution (With vs Without CC License)\", fontsize=18)\n",
    "\n",
    "# Make sure to show both histogram entries and line entries in the legend\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../graphics/outline_length_ratio_distribution.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fa245e",
   "metadata": {},
   "source": [
    "## Semantic similarity\n",
    "\n",
    "Actual embedding and scoring done on a GPU, see `../scripts/boomhauer/*.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ad7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise = pd.read_parquet(\"../results/embeddings_similarities_pairwise_stats.parquet\")\n",
    "top1 = pd.read_parquet(\"../results/embeddings_similarities_pairwise_top1_alignments.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8d1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of articles where Grokipedia has as many or more chunks than Wikipedia\n",
    "len(pairwise[pairwise['n_w'] <= pairwise['n_g']]) / len(pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f2b4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{RESULT_DIR}/grokipedia_wo_license.txt\", encoding=\"utf-8\") as f:\n",
    "    grokipedia_wo_license_df = pd.DataFrame({\"title\": [line.rstrip('\\n').replace(\" \", \"_\") for line in f]})\n",
    "\n",
    "with open(f\"{RESULT_DIR}/grokipedia_w_license.txt\", encoding=\"utf-8\") as f:\n",
    "    grokipedia_w_license_df = pd.DataFrame({\"title\": [line.rstrip('\\n').replace(\" \", \"_\") for line in f]})\n",
    "\n",
    "top1_w_license = pd.merge(grokipedia_w_license_df, top1, left_on=\"title\", right_on=\"title\")\n",
    "top1_wo_license = pd.merge(grokipedia_wo_license_df, top1, left_on=\"title\", right_on=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1.groupby('title').agg({'similarity': 'mean'}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095dfc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_w_license.groupby('title').agg({'similarity': 'mean'}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c570ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "top1_wo_license.groupby('title').agg({'similarity': 'mean'}).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c59bd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a610608",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(\n",
    "    [psim_w_license[\"similarity\"], psim_wo_license[\"similarity\"]],\n",
    "    bins=500,\n",
    "    color=[\"tab:blue\", \"tab:orange\"],\n",
    "    label=[\"With CC License\", \"Without CC License\"],\n",
    "    alpha=0.7,\n",
    "    histtype=\"stepfilled\",\n",
    ")\n",
    "plt.xlabel(\"Similarity\", fontsize=18)\n",
    "plt.ylabel(\"Count\", fontsize=18)\n",
    "plt.title(\"Average Page Embedding Similarity Distributions:\\n With vs. Without CC License\", fontsize=18)\n",
    "plt.legend(fontsize=16)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../graphics/embedding_similarity_distribution.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c1c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "def map_chunk_id_to_bucket(chunk_id):\n",
    "    if chunk_id == 0:\n",
    "        return \"1\"\n",
    "    elif chunk_id == 1:\n",
    "        return \"2\"\n",
    "    elif chunk_id == 2:\n",
    "        return \"3\"\n",
    "    elif chunk_id == 3:\n",
    "        return \"4\"\n",
    "    elif chunk_id == 4:\n",
    "        return \"5\"\n",
    "    elif chunk_id <= 10:\n",
    "        return \"6-10\"\n",
    "    elif chunk_id <= 20:\n",
    "        return \"11-20\"\n",
    "    elif chunk_id <= 30:\n",
    "        return \"21-30\"\n",
    "    elif chunk_id <= 40:\n",
    "        return \"31-40\"\n",
    "    elif chunk_id <= 50:\n",
    "        return \"41-50\"\n",
    "    elif chunk_id <= 75:\n",
    "        return \"51-75\"\n",
    "    elif chunk_id <= 100:\n",
    "        return \"76-100\"\n",
    "    elif chunk_id <= 125:\n",
    "        return \"101-125\"\n",
    "    elif chunk_id <= 150:\n",
    "        return \"126-150\"\n",
    "    elif chunk_id <= 175:\n",
    "        return \"151-175\"\n",
    "    elif chunk_id <= 200:\n",
    "        return \"176-200\"\n",
    "    elif chunk_id <= 250:\n",
    "        return \"201-250\"\n",
    "    elif chunk_id <= 300:\n",
    "        return \"251-300\"\n",
    "    elif chunk_id <= 350:\n",
    "        return \"301-350\"\n",
    "    elif chunk_id <= 400:\n",
    "        return \"351-400\"\n",
    "    else:\n",
    "        return \">400\"\n",
    "\n",
    "# Order of buckets for plotting, matching the function's logic\n",
    "chunk_bucket_order = [\n",
    "    \"1\", \"2\", \"3\", \"4\", \"5\", \"6-10\", \"11-20\", \"21-30\", \"31-40\", \"41-50\",\n",
    "    \"51-75\", \"76-100\", \"101-125\", \"126-150\", \"151-175\", \"176-200\", \"201-250\", \"251-300\",\n",
    "    \"301-350\", \"351-400\", \">400\"\n",
    "]\n",
    "\n",
    "# Compute bucket labels column\n",
    "top1['chunk_bucket'] = top1['wiki_chunk_id'].apply(map_chunk_id_to_bucket)\n",
    "\n",
    "# Compute group statistics for each bucket\n",
    "grouped = top1.groupby('chunk_bucket')[\"similarity\"]\n",
    "avg_similarity = grouped.mean()\n",
    "count = grouped.count()\n",
    "std = grouped.std()\n",
    "\n",
    "# Calculate 95% confidence intervals\n",
    "alpha = 0.05\n",
    "z_score = stats.norm.ppf(1 - alpha/2)\n",
    "sem = std / np.sqrt(count)\n",
    "ci_halfwidth = z_score * sem\n",
    "\n",
    "ci_lower = avg_similarity - ci_halfwidth\n",
    "ci_upper = avg_similarity + ci_halfwidth\n",
    "\n",
    "# Prepare DataFrame for plotting and reindex for plotting order\n",
    "plot_df = pd.DataFrame({\n",
    "    \"mean\": avg_similarity,\n",
    "    \"ci_lower\": ci_lower,\n",
    "    \"ci_upper\": ci_upper\n",
    "}).reindex(chunk_bucket_order)\n",
    "\n",
    "x = np.arange(len(plot_df.index))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, plot_df[\"mean\"], marker='o', label=\"Mean similarity\")\n",
    "plt.fill_between(\n",
    "    x, \n",
    "    plot_df[\"ci_lower\"], \n",
    "    plot_df[\"ci_upper\"], \n",
    "    color=\"tab:blue\", \n",
    "    alpha=0.2, \n",
    "    label=\"95% Confidence Interval\"\n",
    ")\n",
    "plt.xlabel('Wikipedia Chunk Position', fontsize=18)\n",
    "plt.ylabel('Average Similarity', fontsize=18)\n",
    "plt.title('Average Similarity by Wikipedia Chunk Position\\n(with 95% Confidence Intervals)', fontsize=18)\n",
    "plt.xticks(x, plot_df.index, rotation=45, fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.legend(fontsize=16)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../graphics/average_similarity_by_chunk_position.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c82022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "controversial = set()\n",
    "\n",
    "with open('../results/controversial_pages_in_grokipedia.txt', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        controversial.add(line.rstrip('\\n').replace(\" \", \"_\"))\n",
    "\n",
    "congress = pd.read_csv('../supplemental_data/wikidata_queries/us_members_of_congress.csv')\n",
    "parliament = pd.read_csv('../supplemental_data/wikidata_queries/uk_members_of_parliament.csv')\n",
    "mps = set(congress['personLabel'].str.replace(\" \", \"_\")) | set(parliament['personLabel'].str.replace(\" \", \"_\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b633e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "psim_w_license = top1_w_license[['title', 'similarity']].groupby('title').mean()\n",
    "psim_wo_license = top1_wo_license[['title', 'similarity']].groupby('title').mean()\n",
    "psim_controversial = top1[top1['title'].str.lower().isin(controversial)][['title', 'similarity']].groupby('title').mean()\n",
    "psim_mps = top1[top1['title'].isin(mps)][['title', 'similarity']].groupby('title').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "psim_mps.similarity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45515f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "psim_controversial.sort_values(by='similarity', ascending=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1edd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w license high similarity: Sono_Sachiko, Aubrey_de_Sélincourt, Korba_Super_Thermal_Power_Station\n",
    "# w license medium similarity: (Sittin'_On)_The_Dock_of_the_Bay, Lycorine, Stayman_convention\n",
    "# w license low similarity: Duncan_Jones, MacGyver_in_popular_culture, Ellen_David\n",
    "\n",
    "# wo license high similarity: Seliwanoff's_test, The_Sandlot_2, Mejia_Thermal_Power_Station\n",
    "# wo license medium similarity: Capital_punishment_in_the_Soviet_Union, Grand_Junction,_Colorado, James_Jacobus_Roosevelt\n",
    "# wo license low similarity: Henry_V, Potato_famine, Soylent\n",
    "\n",
    "# controversial high similarity: Christopher_Paul_Neil, Historicity_of_Jesus, Number_of_the_Beast\n",
    "# controversial medium similarity: Dylann_Roof, Criticism_of_Judaism, Presidency_of_George_W._Bush\t\n",
    "# controversial low similarity: Criticism_of_the_United_States_government, Racism_in_the_United_States, Media_bias_in_the_United_States\n",
    "\n",
    "# pol high similarity: David_Scott, Tom_Collins, Chris_Deluzio\n",
    "# pol medium similarity: Chris_Murphy, Bill_Cassidy, Alistair_Strathern\n",
    "# pol low similarity: Chris_Pappas, Adrian_Smith, Nigel_Farage, Keir_Starmer, Pramila_Jayapal, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edba6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
