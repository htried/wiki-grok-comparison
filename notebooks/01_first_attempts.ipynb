{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first pass at downloading chunks from WME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch==2.9.0 --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "# !conda install -c faiss-cpu\n",
    "# !pip install -U sentence-transformers transformers safetensors huggingface-hub\n",
    "# !pip install dataset[faiss]\n",
    "# !pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import gzip\n",
    "import json\n",
    "import mwparserfromhell\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wme_username = os.environ['wme_username']\n",
    "wme_password = os.environ['wme_password']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache all-MiniLM-L6-v2 embedding model and load it\n",
    "model_path = \"/home/htriedman/.cache/huggingface/hub/models--sentence-transformers--all-MiniLM-L6-v2/snapshots/c9745ed1d9f207416be6d2e6f8de32d1f16199bf\"\n",
    "model = SentenceTransformer(model_path, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.post(\n",
    "    'https://auth.enterprise.wikimedia.com/v1/login',\n",
    "    data={\n",
    "        'username': wme_username,\n",
    "        'password': wme_password\n",
    "    }     \n",
    ")\n",
    "tokens = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snapshot_identifier = \"enwiki_namespace_0\"\n",
    "\n",
    "# download a single sample chunk, switch comment to download all ~400 chunks\n",
    "# url = f\"https://api.enterprise.wikimedia.com/v2/snapshots/{snapshot_identifier}/download\"\n",
    "url = f\"https://api.enterprise.wikimedia.com/v2/snapshots/{snapshot_identifier}/chunks/0/download\"\n",
    "destination_directory = \"./extracted_enwiki_chunk0\"\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {tokens['access_token']}\"\n",
    "}\n",
    "\n",
    "chunk = []\n",
    "\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # raise an exception for bad status codes\n",
    "\n",
    "    # check content type\n",
    "    content_type = response.headers.get('Content-Type', '')\n",
    "    if content_type not in ['application/zip', 'binary/octet-stream']:\n",
    "        print(f\"Warning: Expected a zip file, but received {content_type}\")\n",
    "    \n",
    "    compressed_data = response.content\n",
    "    \n",
    "    # decompress, decode, and extract usable string from data\n",
    "    decompressed_data = gzip.decompress(compressed_data)\n",
    "    decoded_string = decompressed_data.decode('utf-8')\n",
    "    decoded_string = decoded_string[decoded_string.find('{'):decoded_string.rfind('}')+1]\n",
    "    for page in decoded_string.split('\\n'):\n",
    "        try:\n",
    "            chunk += [json.loads(page)]\n",
    "        except:\n",
    "            print(f\"{page[:1000]} load failed\")\n",
    "            continue\n",
    "        \n",
    "    print(f\"{len(chunk)} total pages in this chunk\")\n",
    "\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"Error decoding the decompressed data. {e}\")\n",
    "    print(\"The data might be binary or use a different character encoding.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_list(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save_to_disk('data/chunk_0_hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load_from_disk('data/chunk_0_hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_to_embed(article):\n",
    "    name = article['name']\n",
    "    abstract = article['abstract']\n",
    "    \n",
    "    wikitext = mwparserfromhell.parse(article['article_body']['wikitext'])\n",
    "    for t in wikitext.filter_templates():\n",
    "        try:\n",
    "            wikitext.remove(t)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    end = min(500, len(wikitext))\n",
    "    first_chars = str(wikitext[:end])\n",
    "    \n",
    "    article['to_embed'] = f\"{name}\\n\\n{abstract}\\n\\n{first_chars}\"\n",
    "    return article\n",
    "    \n",
    "\n",
    "ds = ds.map(get_to_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def do_embedding(article):\n",
    "    embedding = model.encode(article['to_embed'], convert_to_numpy=True)\n",
    "    article['embedding'] = embedding\n",
    "    return article\n",
    "\n",
    "ds = ds.map(do_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save_to_disk('data/chunk_0_hf_embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.load_from_disk('data/chunk_0_hf_embed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds.add_faiss_index(column='embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 'Cats are so interesting. I especially like tigers.'\n",
    "emb_q = model.encode(q, convert_to_numpy=True)\n",
    "\n",
    "scores, retrieved = ds.get_nearest_examples('embedding', emb_q, k=10)\n",
    "\n",
    "for s, r in zip(scores, retrieved['name']):\n",
    "    print(f\"{r}, {s}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.save_faiss_index('embedding', 'test_idx.faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.load_faiss_index('embedding', 'my_index.faiss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "- parse `{{cite xxx` and `{{citation` templates from source data\n",
    "- paralellize to download multiple chunks at a time\n",
    "- parse grokipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
