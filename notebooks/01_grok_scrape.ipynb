{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef1ea86",
   "metadata": {},
   "source": [
    "# Grokipedia manual scraping file\n",
    "\n",
    "Initial scripts for scraping pages on Grokipedia.\n",
    "\n",
    "First we find discoverable pages, then we scrape.\n",
    "\n",
    "I ended up parallelizing and sharding this process on GCP for higher throughput, but this is a good artifact to document process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93db0726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from aiolimiter import AsyncLimiter\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0abc20",
   "metadata": {},
   "source": [
    "## Networking setup and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proxy_skip = True\n",
    "proxy_skip = False\n",
    "\n",
    "# BrightData Proxy Configuration\n",
    "BRIGHTDATA_USERNAME = os.getenv('BRIGHTDATA_USERNAME')\n",
    "BRIGHTDATA_PASSWORD = os.getenv('BRIGHTDATA_PASSWORD')\n",
    "\n",
    "if proxy_skip:\n",
    "    PROXY_URL = None\n",
    "    logger.warning('Proxy set to skip')\n",
    "elif BRIGHTDATA_USERNAME and BRIGHTDATA_PASSWORD:\n",
    "    PROXY_URL = f'http://{BRIGHTDATA_USERNAME}:{BRIGHTDATA_PASSWORD}@brd.superproxy.io:33335'\n",
    "    logger.info('BrightData proxy configured')\n",
    "else:\n",
    "    PROXY_URL = None\n",
    "    logger.warning('BrightData credentials not found - running without proxy')\n",
    "\n",
    "# Configuration \n",
    "DISCOVERY_START_INDEX = 0\n",
    "DISCOVERY_BATCH_SIZE = 5000\n",
    "DISCOVERY_TIMEOUT = 15 \n",
    "\n",
    "SCRAPING_START_INDEX = 0\n",
    "SCRAPING_BATCH_SIZE = 300\n",
    "SCRAPING_SKIP_ON_ERROR = True\n",
    "SCRAPING_TIMEOUT = 60 \n",
    "\n",
    "MAX_CONCURRENT = 200 \n",
    "RATE_LIMIT = 300 \n",
    "BATCH_DELAY = 0.5 \n",
    "\n",
    "SCRAPED_DATA_DIR = \"../scraped_data\"\n",
    "DISCOVERED_TITLES_DIR = \"../discovered_titles\"\n",
    "\n",
    "# Create output directories\n",
    "Path(DISCOVERED_TITLES_DIR).mkdir(exist_ok=True)\n",
    "Path(SCRAPED_DATA_DIR).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure aiohttp with BrightData proxy if available\n",
    "if PROXY_URL:\n",
    "    logger.info(f\"Using proxy: {PROXY_URL.split('@')[1]}\")  # Don't log credentials\n",
    "    proxy_auth = aiohttp.BasicAuth(BRIGHTDATA_USERNAME, BRIGHTDATA_PASSWORD) if BRIGHTDATA_USERNAME else None\n",
    "    \n",
    "    # Proxy configuration for aiohttp\n",
    "    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT)\n",
    "    proxy_config = {\n",
    "        'http': f'http://brd.superproxy.io:33335',\n",
    "        'https': f'http://brd.superproxy.io:33335'\n",
    "    }\n",
    "else:\n",
    "    proxy_config = None\n",
    "    logger.info(\"Running without proxy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d5f95d",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56683ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to parse grokipedia HTML\n",
    "def parse_grokipedia_html(html_content, url, title=None):\n",
    "    \"\"\"Parse grokipedia HTML and extract structured data\"\"\"\n",
    "    if title is None:\n",
    "        title = url.split('/page/')[-1]\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    data = {\n",
    "        'title': title,\n",
    "        'url': url,\n",
    "        'main_title': None,\n",
    "        'sections': [],\n",
    "        'paragraphs': [],\n",
    "        'tables': [],\n",
    "        'references': [],\n",
    "    }\n",
    "    \n",
    "    # Find article container\n",
    "    article = soup.find('div', class_='mx-auto max-w-[850px]')\n",
    "    if not article:\n",
    "        return data\n",
    "    \n",
    "    # Extract main title (h1)\n",
    "    h1 = article.find('h1')\n",
    "    if h1:\n",
    "        data['main_title'] = h1.get_text(strip=True)\n",
    "    \n",
    "    # Extract sections with proper content\n",
    "    for heading in article.find_all(['h1', 'h2', 'h3'], id=True):\n",
    "        section_data = {\n",
    "            'level': heading.name,\n",
    "            'id': heading.get('id'),\n",
    "            'title': heading.get_text(strip=True),\n",
    "            'content': []\n",
    "        }\n",
    "        \n",
    "        # Walk through siblings after heading\n",
    "        current = heading.next_sibling\n",
    "        while current:\n",
    "            if hasattr(current, 'name') and current.name in ['h1', 'h2', 'h3']:\n",
    "                if current.name <= heading.name:\n",
    "                    break\n",
    "            \n",
    "            if hasattr(current, 'name'):\n",
    "                if current.name == 'span' and 'mb-4' in (current.get('class') or []):\n",
    "                    text = current.get_text(strip=True)\n",
    "                    if text:\n",
    "                        # Join sentences with proper spacing\n",
    "                        section_data['content'].append({'type': 'paragraph', 'text': ' '.join(text.split())})\n",
    "                elif current.name == 'ul':\n",
    "                    items = [li.get_text(strip=True) for li in current.find_all('li')]\n",
    "                    if items:\n",
    "                        section_data['content'].append({'type': 'list', 'items': items})\n",
    "                elif current.name == 'ol':\n",
    "                    items = [li.get_text(strip=True) for li in current.find_all('li')]\n",
    "                    if items:\n",
    "                        section_data['content'].append({'type': 'ordered_list', 'items': items})\n",
    "            \n",
    "            current = current.next_sibling\n",
    "        \n",
    "        data['sections'].append(section_data)\n",
    "    \n",
    "    # Extract paragraphs with proper spacing\n",
    "    for span in article.find_all('span', class_='mb-4'):\n",
    "        text = span.get_text(strip=True)\n",
    "        # Normalize whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        if text and text not in data['paragraphs']:\n",
    "            data['paragraphs'].append(text)\n",
    "    \n",
    "    # Extract tables\n",
    "    for table in article.find_all('table'):\n",
    "        table_data = []\n",
    "        headers = []\n",
    "        \n",
    "        if table.find('thead'):\n",
    "            for th in table.find('thead').find_all('th'):\n",
    "                headers.append(th.get_text(strip=True))\n",
    "        \n",
    "        if table.find('tbody'):\n",
    "            for tr in table.find('tbody').find_all('tr'):\n",
    "                row = []\n",
    "                for td in tr.find_all('td'):\n",
    "                    row.append(td.get_text(strip=True))\n",
    "                if row:\n",
    "                    table_data.append(row)\n",
    "        \n",
    "        if headers or table_data:\n",
    "            data['tables'].append({'headers': headers, 'rows': table_data})\n",
    "    \n",
    "    # Extract references WITH links\n",
    "    references_section = soup.find('div', id='references')\n",
    "    if references_section:\n",
    "        for li in references_section.find_all('li'):\n",
    "            ref_text = li.get_text(strip=True)\n",
    "            ref_link = None\n",
    "            \n",
    "            link = li.find('a')\n",
    "            if link and link.get('href'):\n",
    "                ref_link = {'href': link.get('href'), 'text': link.get_text(strip=True)}\n",
    "            \n",
    "            if ref_text:\n",
    "                data['references'].append({'text': ref_text, 'link': ref_link})\n",
    "    \n",
    "    # Remove references from paragraphs\n",
    "    data['paragraphs'] = [p for p in data['paragraphs'] \n",
    "                          if not any(ref['text'].split()[0:3] == p.split()[0:3] for ref in data['references'])]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5582f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def discover_page_exists(session, limiter, title):\n",
    "    \"\"\"Check if a grokipedia page exists using HEAD request\"\"\"\n",
    "    url = f\"https://grokipedia.com/page/{quote(title)}\"\n",
    "    try:\n",
    "        async with limiter:\n",
    "            async with session.head(\n",
    "                url,\n",
    "                timeout=DISCOVERY_TIMEOUT,\n",
    "                headers={'Accept-Encoding': 'gzip, deflate'},\n",
    "                proxy=PROXY_URL,\n",
    "            ) as response:\n",
    "                status = response.status\n",
    "                if status == 200:\n",
    "                    return {'title': title, 'url': url, 'status': 'exists', 'checked_at': datetime.now().isoformat()}\n",
    "                elif status == 404:\n",
    "                    return {'title': title, 'url': url, 'status': 'not_found', 'checked_at': datetime.now().isoformat()}\n",
    "                elif status == 429:\n",
    "                    await asyncio.sleep(5)\n",
    "                    return {'title': title, 'url': url, 'status': 'rate_limited', 'checked_at': datetime.now().isoformat()}\n",
    "                else:\n",
    "                    return {'title': title, 'url': url, 'status': f'error_{status}', 'checked_at': datetime.now().isoformat()}\n",
    "    except Exception as e:\n",
    "        return {'title': title, 'url': url, 'status': 'error', 'error': str(e), 'checked_at': datetime.now().isoformat()}\n",
    "\n",
    "async def discovery_phase(titles, start_index=0, batch_size=10000):\n",
    "    \"\"\"Run discovery phase to find which pages exist\"\"\"\n",
    "    limiter = AsyncLimiter(max_rate=RATE_LIMIT, time_period=DISCOVERY_TIMEOUT)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        discovered_count = 0\n",
    "        not_found_count = 0\n",
    "        error_count = 0\n",
    "        results = []\n",
    "\n",
    "        pbar = tqdm(total=len(titles), desc=\"Discovery\", initial=start_index)\n",
    "\n",
    "        for i in range(start_index, len(titles), MAX_CONCURRENT):\n",
    "            batch = titles[i:i + MAX_CONCURRENT]\n",
    "\n",
    "            tasks = [discover_page_exists(session, limiter, title) for title in batch]\n",
    "            batch_results = await asyncio.gather(*tasks)\n",
    "\n",
    "            for result in batch_results:\n",
    "                if result['status'] == 'exists':\n",
    "                    discovered_count += 1\n",
    "                elif result['status'] == 'not_found':\n",
    "                    not_found_count += 1\n",
    "                else:\n",
    "                    error_count += 1\n",
    "                results.append(result)\n",
    "\n",
    "            pbar.update(len(batch))\n",
    "            pbar.set_postfix({'found': discovered_count, 'not_found': not_found_count, 'errors': error_count})\n",
    "\n",
    "            if i % MAX_CONCURRENT < MAX_CONCURRENT - 1:\n",
    "                await asyncio.sleep(BATCH_DELAY)\n",
    "\n",
    "            if len(results) >= batch_size:\n",
    "                batch_num = (i // batch_size) + 1\n",
    "                batch_start = (batch_num - 1) * batch_size\n",
    "                batch_end = batch_start + len(results)\n",
    "                with open(f'discovered_titles/batch_{batch_start}_{batch_end}.jsonl', 'w') as f:\n",
    "                    for result in results:\n",
    "                        f.write(json.dumps(result) + '\\n')\n",
    "                checkpoint = {\n",
    "                    'last_processed_index': i,\n",
    "                    'discovered_count': discovered_count,\n",
    "                    'not_found_count': not_found_count,\n",
    "                    'error_count': error_count,\n",
    "                    'total_processed': i + len(batch)\n",
    "                }\n",
    "                with open('discovery_checkpoint.json', 'w') as f:\n",
    "                    json.dump(checkpoint, f)\n",
    "                results = []\n",
    "\n",
    "        if results:\n",
    "            batch_num = (len(titles) // batch_size)\n",
    "            batch_start = batch_num * batch_size\n",
    "            batch_end = batch_start + len(results)\n",
    "            with open(f'discovered_titles/batch_{batch_start}_{batch_end}.jsonl', 'w') as f:\n",
    "                for result in results:\n",
    "                    f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        stats = {\n",
    "            'total_checked': len(titles),\n",
    "            'discovered': discovered_count,\n",
    "            'not_found': not_found_count,\n",
    "            'errors': error_count,\n",
    "            'completed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        with open('discovery_stats.json', 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "\n",
    "        logger.info(f\"Discovery complete: Found {discovered_count} existing pages out of {len(titles)} checked\")\n",
    "        return discovered_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f16e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_page(session, limiter, url, skip_on_error=True):\n",
    "    \"\"\"Scrape a single grokipedia page\"\"\"\n",
    "    try:\n",
    "        title = url.split('/page/')[-1]\n",
    "        async with limiter:\n",
    "            async with session.get(\n",
    "                url,\n",
    "                timeout=60,\n",
    "                headers={'Accept-Encoding': 'gzip, deflate'},\n",
    "                proxy=PROXY_URL,\n",
    "            ) as response:\n",
    "                if response.status == 200:\n",
    "                    html = await response.text()\n",
    "                    data = parse_grokipedia_html(html, url)\n",
    "                    return {'success': True, 'data': data}\n",
    "                elif response.status == 404:\n",
    "                    return {'success': False, 'error': 'not_found', 'title': title}\n",
    "                else:\n",
    "                    return {'success': False, 'error': f'status_{response.status}', 'title': title}\n",
    "    except asyncio.TimeoutError:\n",
    "        if skip_on_error:\n",
    "            return {'success': False, 'error': 'timeout', 'title': title}\n",
    "        else:\n",
    "            for delay in [2, 4, 8]:\n",
    "                await asyncio.sleep(delay)\n",
    "                try:\n",
    "                    async with limiter:\n",
    "                        async with session.get(\n",
    "                            url,\n",
    "                            timeout=45,\n",
    "                            headers={'Accept-Encoding': 'gzip, deflate'},\n",
    "                            proxy=PROXY_URL,\n",
    "                        ) as response:\n",
    "                            if response.status == 200:\n",
    "                                html = await response.text()\n",
    "                                data = parse_grokipedia_html(html, url, title)\n",
    "                                return {'success': True, 'data': data}\n",
    "                except:\n",
    "                    continue\n",
    "            return {'success': False, 'error': 'timeout_retries_exhausted', 'title': title}\n",
    "    except Exception as e:\n",
    "        return {'success': False, 'error': str(e), 'title': title}\n",
    "\n",
    "async def scraping_phase(urls, start_index=0, batch_size=1000, skip_on_error=True):\n",
    "    \"\"\"Run scraping phase to extract data from discovered pages\"\"\"\n",
    "    limiter = AsyncLimiter(max_rate=RATE_LIMIT, time_period=60)  # Fixed: use 60 second window\n",
    "    connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT, force_close=True, enable_cleanup_closed=True)\n",
    "    \n",
    "    # Ensure output directory exists\n",
    "    Path(SCRAPED_DATA_DIR).mkdir(exist_ok=True)\n",
    "    \n",
    "    async with aiohttp.ClientSession(connector=connector, timeout=aiohttp.ClientTimeout(total=SCRAPING_TIMEOUT)) as session:\n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        scraped_data = []\n",
    "        failed_pages = []\n",
    "        batch_count = 0  # Track batch number\n",
    "        last_save_index = 0  # Track what's been saved\n",
    "        \n",
    "        # Create progress bar\n",
    "        pbar = tqdm(total=len(urls), desc=\"Scraping\", initial=start_index)\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(start_index, len(urls), MAX_CONCURRENT):\n",
    "            batch = urls[i:i + MAX_CONCURRENT]\n",
    "            \n",
    "            # Create tasks for concurrent requests\n",
    "            tasks = [scrape_page(session, limiter, url, skip_on_error) for url in batch]\n",
    "            \n",
    "            # Use gather with return_exceptions to handle individual failures\n",
    "            try:\n",
    "                batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Batch gather failed: {e}\")\n",
    "                batch_results = [{'success': False, 'error': str(e), 'title': 'batch_error'} for _ in batch]\n",
    "            \n",
    "            # Process results\n",
    "            for j, result in enumerate(batch_results):\n",
    "                # Handle exceptions\n",
    "                if isinstance(result, Exception):\n",
    "                    fail_count += 1\n",
    "                    failed_pages.append({\n",
    "                        'title': batch[j] if j < len(batch) else 'unknown',\n",
    "                        'url': batch[j] if j < len(batch) else 'unknown',\n",
    "                        'error': str(result),\n",
    "                        'failed_at': datetime.now().isoformat()\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                if result['success']:\n",
    "                    success_count += 1\n",
    "                    scraped_data.append({\n",
    "                        'title': result['data']['title'],\n",
    "                        'url': result['data']['url'],\n",
    "                        'data': result['data'],\n",
    "                        'scraped_at': datetime.now().isoformat()\n",
    "                    })\n",
    "                else:\n",
    "                    fail_count += 1\n",
    "                    failed_pages.append({\n",
    "                        'title': result.get('title', 'unknown'),\n",
    "                        'url': result.get('url', 'unknown'),\n",
    "                        'error': result.get('error', 'unknown'),\n",
    "                        'failed_at': datetime.now().isoformat()\n",
    "                    })\n",
    "            \n",
    "            pbar.update(len(batch))\n",
    "            pbar.set_postfix({\n",
    "                'success': success_count,\n",
    "                'failed': fail_count,\n",
    "                'fail_rate': f'{fail_count/(success_count+fail_count)*100:.1f}%' if (success_count+fail_count) > 0 else '0%'\n",
    "            })\n",
    "            \n",
    "            # Save batch when we hit the batch_size limit OR every 100 successful items\n",
    "            should_save = False\n",
    "            save_reason = \"\"\n",
    "            \n",
    "            if len(scraped_data) >= batch_size:\n",
    "                should_save = True\n",
    "                save_reason = \"batch_size\"\n",
    "            elif len(scraped_data) >= 100 and (i - last_save_index) >= 5000:  # Save every 5000 processed pages\n",
    "                should_save = True\n",
    "                save_reason = \"progress\"\n",
    "            \n",
    "            if should_save and scraped_data:\n",
    "                batch_start = start_index + (batch_count * batch_size)\n",
    "                batch_end = batch_start + len(scraped_data)\n",
    "                \n",
    "                batch_file = Path(f'{SCRAPED_DATA_DIR}/batch_{batch_start}_{batch_end}.jsonl')\n",
    "                \n",
    "                with open(batch_file, 'w') as f:\n",
    "                    for item in scraped_data:\n",
    "                        f.write(json.dumps(item) + '\\n')\n",
    "                \n",
    "                # Save checkpoint\n",
    "                checkpoint = {\n",
    "                    'last_processed_index': i,\n",
    "                    'success_count': success_count,\n",
    "                    'fail_count': fail_count,\n",
    "                    'total_processed': i + len(batch),\n",
    "                    'last_save_index': batch_end,\n",
    "                    'batch_count': batch_count,\n",
    "                    'save_reason': save_reason\n",
    "                }\n",
    "                \n",
    "                with open('scraping_checkpoint.json', 'w') as f:\n",
    "                    json.dump(checkpoint, f)\n",
    "                \n",
    "                logger.info(f\"Saved batch: {batch_start} to {batch_end} ({len(scraped_data)} items) - Reason: {save_reason}\")\n",
    "                last_save_index = i\n",
    "                batch_count += 1\n",
    "                scraped_data = []\n",
    "            \n",
    "            # Save failed pages periodically too\n",
    "            if len(failed_pages) >= 1000:\n",
    "                failed_file = Path('scraping_failed_partial.jsonl')\n",
    "                with open(failed_file, 'a') as f:\n",
    "                    for item in failed_pages:\n",
    "                        f.write(json.dumps(item) + '\\n')\n",
    "                logger.info(f\"Saved {len(failed_pages)} failed pages to partial file\")\n",
    "                failed_pages = []\n",
    "            \n",
    "            # Adaptive delay - increase if failure rate is high\n",
    "            failure_rate = fail_count / (success_count + fail_count) if (success_count + fail_count) > 0 else 0\n",
    "            if failure_rate > 0.3:  # More than 30% failure rate\n",
    "                delay = 1.0\n",
    "                logger.warning(f\"High failure rate ({failure_rate*100:.1f}%), increasing delay to {delay}s\")\n",
    "            else:\n",
    "                delay = 0.1\n",
    "            \n",
    "            await asyncio.sleep(delay)\n",
    "        \n",
    "        # Save any remaining results\n",
    "        if scraped_data:\n",
    "            batch_start = start_index + (batch_count * batch_size)\n",
    "            batch_end = batch_start + len(scraped_data)\n",
    "            \n",
    "            batch_file = Path(f'{SCRAPED_DATA_DIR}/batch_{batch_start}_{batch_end}.jsonl')\n",
    "            \n",
    "            with open(batch_file, 'w') as f:\n",
    "                for item in scraped_data:\n",
    "                    f.write(json.dumps(item) + '\\n')\n",
    "            \n",
    "            logger.info(f\"Saved final batch: {batch_start} to {batch_end} ({len(scraped_data)} items)\")\n",
    "        \n",
    "        # Save all failed pages\n",
    "        if failed_pages:\n",
    "            with open('scraping_failed.jsonl', 'w') as f:\n",
    "                for item in failed_pages:\n",
    "                    f.write(json.dumps(item) + '\\n')\n",
    "            logger.info(f\"Saved {len(failed_pages)} failed pages\")\n",
    "        \n",
    "        pbar.close()\n",
    "        \n",
    "        # Save final stats\n",
    "        stats = {\n",
    "            'total_scraped': len(urls),\n",
    "            'success': success_count,\n",
    "            'failed': fail_count,\n",
    "            'completed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open('scraping_stats.json', 'w') as f:\n",
    "            json.dump(stats, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Scraping complete: Successfully scraped {success_count} pages out of {len(urls)} attempted\")\n",
    "        return success_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0d2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def load_titles():\n",
    "    \"\"\"Load and sort enwiki titles from pandas DataFrame\"\"\"\n",
    "    df = pd.read_pickle('../enwiki_titles_20251027.pkl')\n",
    "    titles = df['page_title'].tolist()\n",
    "    titles_sorted = sorted(titles)\n",
    "    logger.info(f\"Loaded {len(titles_sorted)} titles from DataFrame\")\n",
    "    return titles_sorted\n",
    "\n",
    "def load_discovered_titles():\n",
    "    \"\"\"Load all discovered titles from batch files\"\"\"\n",
    "    discovered = []\n",
    "    for file in sorted(Path('../discovered_titles').glob('batch_*.jsonl')):\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                result = json.loads(line)\n",
    "                if result['status'] == 'exists':\n",
    "                    discovered.append(result['title'])\n",
    "    logger.info(f\"Loaded {len(discovered)} discovered titles\")\n",
    "    return discovered\n",
    "\n",
    "def load_scraped_data():\n",
    "    \"\"\"Load all scraped data from batch files\"\"\"\n",
    "    scraped = []\n",
    "    for file in sorted(Path('{SCRAPED_DATA_DIR}').glob('batch_*.jsonl')):\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f:\n",
    "                scraped.append(json.loads(line))\n",
    "    logger.info(f\"Loaded {len(scraped)} scraped pages\")\n",
    "    return scraped\n",
    "\n",
    "def retry_failed_pages():\n",
    "    \"\"\"Retry scraping failed pages\"\"\"\n",
    "    failed = []\n",
    "    if Path('scraping_failed.jsonl').exists():\n",
    "        with open('scraping_failed.jsonl', 'r') as f:\n",
    "            for line in f:\n",
    "                failed.append(json.loads(line))\n",
    "    \n",
    "    if not failed:\n",
    "        logger.info(\"No failed pages to retry\")\n",
    "        return []\n",
    "    \n",
    "    titles = [item['title'] for item in failed]\n",
    "    logger.info(f\"Retrying {len(titles)} failed pages\")\n",
    "    return titles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f415a7fa",
   "metadata": {},
   "source": [
    "## Discovery\n",
    "\n",
    "Initially I was going to skim through all the existing enwiki pages and see if they had corresponding Grokipedia pages, but I ended up finding a Huggingface repo of all the Grokipedia URLs (https://huggingface.co/datasets/stefan-it/grokipedia-urls). Keeping this for posterity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11a10d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and sort all enwiki titles\n",
    "titles = load_titles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b70520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run discovery phase\n",
    "# This will check all 7M titles to find which ~885k exist on grokipedia\n",
    "discovered_count = await discovery_phase(\n",
    "    titles,\n",
    "    start_index=DISCOVERY_START_INDEX,\n",
    "    batch_size=DISCOVERY_BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"\\nDiscovery Summary:\")\n",
    "print(f\"Checked {len(titles)} titles\")\n",
    "print(f\"Found {discovered_count} existing pages on grokipedia\")\n",
    "print(f\"Success rate: {discovered_count/len(titles)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a55e57",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8e703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load titles from HF\n",
    "# discovered_titles = load_discovered_titles()\n",
    "# df_urls = pd.read_json(\"hf://datasets/stefan-it/grokipedia-urls/urls.jsonl\", lines=True)\n",
    "# urls = df_urls['url'].tolist()\n",
    "\n",
    "# Load URLs from file (for cleanup / retrying failures)\n",
    "urls = []\n",
    "with open('../urls.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        urls.append(line.strip())\n",
    "\n",
    "print(f\"Loaded {len(urls)} URLs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba659aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [\n",
    "'0_341',\n",
    "'300_640',\n",
    "'600_934',\n",
    "'900_902',\n",
    "'0_352',\n",
    "'300_651',\n",
    "'600_944',\n",
    "'900_1255',\n",
    "'1200_1543',\n",
    "'1500_1760',\n",
    "'0_185'\n",
    "]\n",
    "\n",
    "success = [f'/Users/haltriedman/code/wiki-grok-comparison/scraped_data/batch_{i}.jsonl' for i in indices]\n",
    "\n",
    "success_urls = []\n",
    "for fp in success:\n",
    "    with open(fp, 'r') as f:\n",
    "        for line in f:\n",
    "            page = json.loads(line)\n",
    "            if 'url' in page:\n",
    "                success_urls.append(page['url'])\n",
    "\n",
    "\n",
    "i = 0\n",
    "urls = []\n",
    "with open('../urls.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        url = line.strip()\n",
    "        end = url.replace('https://grokipedia.com/page/', '')\n",
    "        url_encoded_old = quote(end, safe=\":#[]@!$&'()*+,;=%\")\n",
    "        url_encoded = quote(end, safe=\":#[]@!$&'()*+;=%\")\n",
    "        url_encoded_old = f\"https://grokipedia.com/page/{url_encoded_old}\"\n",
    "        url_encoded = f\"https://grokipedia.com/page/{url_encoded}\"\n",
    "        if url not in success_urls and url_encoded_old not in success_urls and url_encoded not in success_urls:\n",
    "            if '%2f' in url_encoded:\n",
    "                i += 1\n",
    "                url_encoded = url_encoded.replace('%2f', '%252f')\n",
    "            urls.append(url_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69326e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec9464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run scraping phase\n",
    "# This will extract structured data from the discovered pages\n",
    "scraped_count = await scraping_phase(\n",
    "    urls,\n",
    "    # start_index=SCRAPING_START_INDEX,\n",
    "    start_index=0,\n",
    "    batch_size=SCRAPING_BATCH_SIZE,\n",
    "    skip_on_error=SCRAPING_SKIP_ON_ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e71aa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
