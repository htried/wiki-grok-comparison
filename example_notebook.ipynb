{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Dataset and Embeddings - Example Usage\n",
    "\n",
    "This notebook demonstrates how to use the Wikipedia dataset utilities and embedding functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, make sure you have installed the required dependencies:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the utilities\n",
    "from wikipedia_dataset import (\n",
    "    download_wikipedia_dataset,\n",
    "    parse_page_content,\n",
    "    parse_multiple_pages,\n",
    "    get_page_by_title\n",
    ")\n",
    "from embeddings import (\n",
    "    load_embedding_model,\n",
    "    embed_text,\n",
    "    embed_page_content,\n",
    "    compare_embeddings,\n",
    "    compare_multiple_embeddings,\n",
    "    find_most_similar\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Wikipedia Dataset\n",
    "\n",
    "⚠️ **Note**: This step downloads a large dataset and may take significant time on first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the English Wikipedia dataset\n",
    "dataset = download_wikipedia_dataset(\"en\")\n",
    "print(f\"Downloaded dataset with {len(dataset)} articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse Wikipedia Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a single page\n",
    "title, lead = parse_page_content(dataset[0])\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Lead content (first 200 chars): {lead[:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse multiple pages\n",
    "pages = parse_multiple_pages(dataset, num_pages=10)\n",
    "for i, (title, lead) in enumerate(pages):\n",
    "    print(f\"{i+1}. {title} - {len(lead)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding model (this may take a moment on first run)\n",
    "model = load_embedding_model()\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a single text\n",
    "text = \"Python is a high-level programming language.\"\n",
    "embedding = embed_text(text, model)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"First 5 values: {embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed multiple texts at once\n",
    "texts = [\n",
    "    \"Python is a programming language\",\n",
    "    \"Java is a programming language\",\n",
    "    \"The cat sat on the mat\"\n",
    "]\n",
    "embeddings = embed_text(texts, model)\n",
    "print(f\"Embedded {len(texts)} texts\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed a Wikipedia page (title + lead content)\n",
    "title, lead = parse_page_content(dataset[0])\n",
    "page_embedding = embed_page_content(title, lead, model)\n",
    "print(f\"Page: {title}\")\n",
    "print(f\"Embedding shape: {page_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two embeddings\n",
    "text1 = \"Python programming language\"\n",
    "text2 = \"Programming in Python\"\n",
    "text3 = \"Cooking pasta recipes\"\n",
    "\n",
    "emb1 = embed_text(text1, model)\n",
    "emb2 = embed_text(text2, model)\n",
    "emb3 = embed_text(text3, model)\n",
    "\n",
    "sim_1_2 = compare_embeddings(emb1, emb2)\n",
    "sim_1_3 = compare_embeddings(emb1, emb3)\n",
    "sim_2_3 = compare_embeddings(emb2, emb3)\n",
    "\n",
    "print(f\"Text 1: {text1}\")\n",
    "print(f\"Text 2: {text2}\")\n",
    "print(f\"Text 3: {text3}\\n\")\n",
    "\n",
    "print(f\"Similarity (1 vs 2): {sim_1_2:.4f}\")\n",
    "print(f\"Similarity (1 vs 3): {sim_1_3:.4f}\")\n",
    "print(f\"Similarity (2 vs 3): {sim_2_3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Multiple Embeddings (Pairwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pairwise similarities\n",
    "import numpy as np\n",
    "\n",
    "texts = [\"Python\", \"Java\", \"JavaScript\", \"Cooking\"]\n",
    "embeddings = embed_text(texts, model)\n",
    "similarities = compare_multiple_embeddings(embeddings)\n",
    "\n",
    "print(\"Pairwise similarities:\")\n",
    "print(\"\\t\" + \"\\t\".join(texts))\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"{text}\\t\", end=\"\")\n",
    "    for j in range(len(texts)):\n",
    "        print(f\"{similarities[i,j]:.3f}\\t\", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Find Most Similar Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse and embed multiple Wikipedia pages\n",
    "pages = parse_multiple_pages(dataset, num_pages=50)\n",
    "page_embeddings = []\n",
    "page_titles = []\n",
    "\n",
    "for title, lead in pages:\n",
    "    if lead:  # Only embed if there's content\n",
    "        embedding = embed_page_content(title, lead, model)\n",
    "        page_embeddings.append(embedding)\n",
    "        page_titles.append(title)\n",
    "\n",
    "page_embeddings_array = np.array(page_embeddings)\n",
    "print(f\"Embedded {len(page_embeddings)} pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find pages most similar to a query\n",
    "query_text = \"Computer science and programming\"\n",
    "query_embedding = embed_text(query_text, model)\n",
    "\n",
    "top_matches = find_most_similar(query_embedding, page_embeddings_array, top_k=5)\n",
    "\n",
    "print(f\"Query: {query_text}\\n\")\n",
    "print(\"Most similar pages:\")\n",
    "for idx, score in top_matches:\n",
    "    print(f\"{score:.4f} - {page_titles[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare Wikipedia Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get specific pages by title (if they exist in the dataset)\n",
    "# Example: Compare embeddings of different programming language articles\n",
    "\n",
    "# Parse first 1000 pages and find specific topics\n",
    "pages = parse_multiple_pages(dataset, num_pages=1000)\n",
    "\n",
    "# Look for pages about programming languages\n",
    "target_keywords = [\"Python\", \"Java\", \"JavaScript\", \"C++\"]\n",
    "found_pages = {}\n",
    "\n",
    "for title, lead in pages:\n",
    "    for keyword in target_keywords:\n",
    "        if keyword.lower() in title.lower() and keyword not in found_pages:\n",
    "            found_pages[keyword] = (title, lead)\n",
    "            break\n",
    "\n",
    "# Embed and compare found pages\n",
    "if len(found_pages) >= 2:\n",
    "    print(\"Found pages:\")\n",
    "    page_list = list(found_pages.items())\n",
    "    for keyword, (title, _) in page_list:\n",
    "        print(f\"  {keyword}: {title}\")\n",
    "    \n",
    "    print(\"\\nComparing embeddings:\")\n",
    "    for i in range(len(page_list)):\n",
    "        for j in range(i+1, len(page_list)):\n",
    "            kw1, (t1, l1) = page_list[i]\n",
    "            kw2, (t2, l2) = page_list[j]\n",
    "            emb1 = embed_page_content(t1, l1, model)\n",
    "            emb2 = embed_page_content(t2, l2, model)\n",
    "            similarity = compare_embeddings(emb1, emb2)\n",
    "            print(f\"  {kw1} vs {kw2}: {similarity:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough pages found in the first 1000 entries.\")\n",
    "    print(\"Try increasing the num_pages parameter or use different keywords.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
